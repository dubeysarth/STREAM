{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import networkx as nx\n",
    "\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import tqdm\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "cfg = configparser.ConfigParser()\n",
    "cfg.optionxform = str\n",
    "cfg.read('/home/sarth/rootdir/assets/global.ini')\n",
    "cfg = {s: dict(cfg.items(s)) for s in cfg.sections()}\n",
    "PATHS = cfg['PATHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME = '30min_CWatM_CAMELS-US'\n",
    "SAVE_PATH = os.path.join(PATHS['devp_datasets'], DIRNAME)\n",
    "resolution = 0.50\n",
    "lon_360_180 = lambda x: (x + 180) % 360 - 180 # convert 0-360 to -180-180\n",
    "lon_180_360 = lambda x: x % 360 # convert -180-180 to 0-360\n",
    "region_bounds = {\n",
    "    'minx': -130,\n",
    "    'miny': 20,\n",
    "    'maxx': -65,\n",
    "    'maxy': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Watershed Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_lon</th>\n",
       "      <th>gauge_lat</th>\n",
       "      <th>area_geospa_fabric</th>\n",
       "      <th>snapped_lon</th>\n",
       "      <th>snapped_lat</th>\n",
       "      <th>snapped_uparea</th>\n",
       "      <th>snapped_iou</th>\n",
       "      <th>area_percent_difference</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_edges</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gauge_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>06452000</th>\n",
       "      <td>10</td>\n",
       "      <td>-99.55649</td>\n",
       "      <td>43.74833</td>\n",
       "      <td>25817.78</td>\n",
       "      <td>-99.75</td>\n",
       "      <td>43.75</td>\n",
       "      <td>26974.6170</td>\n",
       "      <td>0.644380</td>\n",
       "      <td>4.480780</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13340000</th>\n",
       "      <td>17</td>\n",
       "      <td>-116.25750</td>\n",
       "      <td>46.47833</td>\n",
       "      <td>14270.76</td>\n",
       "      <td>-115.75</td>\n",
       "      <td>46.25</td>\n",
       "      <td>12915.0530</td>\n",
       "      <td>0.666042</td>\n",
       "      <td>9.499894</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06447000</th>\n",
       "      <td>10</td>\n",
       "      <td>-101.52487</td>\n",
       "      <td>43.75250</td>\n",
       "      <td>12869.46</td>\n",
       "      <td>-101.75</td>\n",
       "      <td>43.75</td>\n",
       "      <td>13514.7300</td>\n",
       "      <td>0.571147</td>\n",
       "      <td>5.013967</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06360500</th>\n",
       "      <td>10</td>\n",
       "      <td>-100.84292</td>\n",
       "      <td>45.25582</td>\n",
       "      <td>12601.47</td>\n",
       "      <td>-101.25</td>\n",
       "      <td>45.25</td>\n",
       "      <td>13086.8330</td>\n",
       "      <td>0.774024</td>\n",
       "      <td>3.851640</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06354000</th>\n",
       "      <td>10</td>\n",
       "      <td>-100.93444</td>\n",
       "      <td>46.37611</td>\n",
       "      <td>10626.74</td>\n",
       "      <td>-101.25</td>\n",
       "      <td>46.25</td>\n",
       "      <td>8571.6370</td>\n",
       "      <td>0.628915</td>\n",
       "      <td>19.338984</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14303200</th>\n",
       "      <td>17</td>\n",
       "      <td>-123.54650</td>\n",
       "      <td>45.32428</td>\n",
       "      <td>8.07</td>\n",
       "      <td>-123.75</td>\n",
       "      <td>45.25</td>\n",
       "      <td>2181.1390</td>\n",
       "      <td>0.003707</td>\n",
       "      <td>26927.744000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10336740</th>\n",
       "      <td>16</td>\n",
       "      <td>-119.93546</td>\n",
       "      <td>39.06658</td>\n",
       "      <td>7.94</td>\n",
       "      <td>-119.75</td>\n",
       "      <td>39.25</td>\n",
       "      <td>2395.8298</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>30074.180000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01466500</th>\n",
       "      <td>02</td>\n",
       "      <td>-74.50528</td>\n",
       "      <td>39.88500</td>\n",
       "      <td>6.25</td>\n",
       "      <td>-74.25</td>\n",
       "      <td>39.75</td>\n",
       "      <td>2378.9302</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>37962.883000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01594950</th>\n",
       "      <td>02</td>\n",
       "      <td>-79.39031</td>\n",
       "      <td>39.27669</td>\n",
       "      <td>6.10</td>\n",
       "      <td>-79.25</td>\n",
       "      <td>39.25</td>\n",
       "      <td>2395.8298</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>39175.900000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06614800</th>\n",
       "      <td>10</td>\n",
       "      <td>-105.86501</td>\n",
       "      <td>40.49609</td>\n",
       "      <td>4.10</td>\n",
       "      <td>-105.75</td>\n",
       "      <td>40.25</td>\n",
       "      <td>2361.8464</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>57506.010000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>671 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         huc_02  gauge_lon  gauge_lat  area_geospa_fabric  snapped_lon  \\\n",
       "gauge_id                                                                 \n",
       "06452000     10  -99.55649   43.74833            25817.78       -99.75   \n",
       "13340000     17 -116.25750   46.47833            14270.76      -115.75   \n",
       "06447000     10 -101.52487   43.75250            12869.46      -101.75   \n",
       "06360500     10 -100.84292   45.25582            12601.47      -101.25   \n",
       "06354000     10 -100.93444   46.37611            10626.74      -101.25   \n",
       "...         ...        ...        ...                 ...          ...   \n",
       "14303200     17 -123.54650   45.32428                8.07      -123.75   \n",
       "10336740     16 -119.93546   39.06658                7.94      -119.75   \n",
       "01466500     02  -74.50528   39.88500                6.25       -74.25   \n",
       "01594950     02  -79.39031   39.27669                6.10       -79.25   \n",
       "06614800     10 -105.86501   40.49609                4.10      -105.75   \n",
       "\n",
       "          snapped_lat  snapped_uparea  snapped_iou  area_percent_difference  \\\n",
       "gauge_id                                                                      \n",
       "06452000        43.75      26974.6170     0.644380                 4.480780   \n",
       "13340000        46.25      12915.0530     0.666042                 9.499894   \n",
       "06447000        43.75      13514.7300     0.571147                 5.013967   \n",
       "06360500        45.25      13086.8330     0.774024                 3.851640   \n",
       "06354000        46.25       8571.6370     0.628915                19.338984   \n",
       "...               ...             ...          ...                      ...   \n",
       "14303200        45.25       2181.1390     0.003707             26927.744000   \n",
       "10336740        39.25       2395.8298     0.001653             30074.180000   \n",
       "01466500        39.75       2378.9302     0.002212             37962.883000   \n",
       "01594950        39.25       2395.8298     0.002549             39175.900000   \n",
       "06614800        40.25       2361.8464     0.001744             57506.010000   \n",
       "\n",
       "          num_nodes  num_edges  \n",
       "gauge_id                        \n",
       "06452000       12.0       11.0  \n",
       "13340000        6.0        5.0  \n",
       "06447000        6.0        5.0  \n",
       "06360500        6.0        5.0  \n",
       "06354000        4.0        3.0  \n",
       "...             ...        ...  \n",
       "14303200        1.0        0.0  \n",
       "10336740        2.0        1.0  \n",
       "01466500        1.0        0.0  \n",
       "01594950        1.0        0.0  \n",
       "06614800        1.0        0.0  \n",
       "\n",
       "[671 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_attributes_graph = pd.read_csv(os.path.join(SAVE_PATH, 'graph_attributes.csv'), index_col=0)\n",
    "camels_attributes_graph.index = camels_attributes_graph.index.map(lambda x: str(x).zfill(8))\n",
    "camels_attributes_graph['huc_02'] = camels_attributes_graph['huc_02'].map(lambda x: str(x).zfill(2))\n",
    "camels_attributes_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 11)\n",
      "(10, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "huc_02\n",
       "09    1\n",
       "10    5\n",
       "11    2\n",
       "15    1\n",
       "17    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_graph = camels_attributes_graph.copy()\n",
    "camels_graph = camels_graph[camels_graph['area_percent_difference'] < 10]\n",
    "print(camels_graph.shape)\n",
    "camels_graph = camels_graph[camels_graph['num_nodes'] > 1]\n",
    "print(camels_graph.shape)\n",
    "# Print the number of graphs per 'huc_02' (sorted in values of huc_02)\n",
    "camels_graph.sort_values(ascending=True, by = 'huc_02').groupby('huc_02').size()\n",
    "# camels_graph['huc_02'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       10.000000\n",
       "mean      9668.537000\n",
       "std       6872.941039\n",
       "min       4344.130000\n",
       "25%       4918.577500\n",
       "50%       6060.605000\n",
       "75%      12802.462500\n",
       "max      25817.780000\n",
       "Name: area_geospa_fabric, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_graph['area_geospa_fabric'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_lon</th>\n",
       "      <th>gauge_lat</th>\n",
       "      <th>area_geospa_fabric</th>\n",
       "      <th>snapped_lon</th>\n",
       "      <th>snapped_lat</th>\n",
       "      <th>snapped_uparea</th>\n",
       "      <th>snapped_iou</th>\n",
       "      <th>area_percent_difference</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_edges</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gauge_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>06452000</th>\n",
       "      <td>10</td>\n",
       "      <td>-99.55649</td>\n",
       "      <td>43.74833</td>\n",
       "      <td>25817.78</td>\n",
       "      <td>-99.75</td>\n",
       "      <td>43.75</td>\n",
       "      <td>26974.6170</td>\n",
       "      <td>0.644380</td>\n",
       "      <td>4.480780</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13340000</th>\n",
       "      <td>17</td>\n",
       "      <td>-116.25750</td>\n",
       "      <td>46.47833</td>\n",
       "      <td>14270.76</td>\n",
       "      <td>-115.75</td>\n",
       "      <td>46.25</td>\n",
       "      <td>12915.0530</td>\n",
       "      <td>0.666042</td>\n",
       "      <td>9.499894</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06447000</th>\n",
       "      <td>10</td>\n",
       "      <td>-101.52487</td>\n",
       "      <td>43.75250</td>\n",
       "      <td>12869.46</td>\n",
       "      <td>-101.75</td>\n",
       "      <td>43.75</td>\n",
       "      <td>13514.7300</td>\n",
       "      <td>0.571147</td>\n",
       "      <td>5.013967</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06360500</th>\n",
       "      <td>10</td>\n",
       "      <td>-100.84292</td>\n",
       "      <td>45.25582</td>\n",
       "      <td>12601.47</td>\n",
       "      <td>-101.25</td>\n",
       "      <td>45.25</td>\n",
       "      <td>13086.8330</td>\n",
       "      <td>0.774024</td>\n",
       "      <td>3.851640</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06191500</th>\n",
       "      <td>10</td>\n",
       "      <td>-110.79438</td>\n",
       "      <td>45.11188</td>\n",
       "      <td>6808.28</td>\n",
       "      <td>-110.75</td>\n",
       "      <td>45.25</td>\n",
       "      <td>6599.8230</td>\n",
       "      <td>0.404993</td>\n",
       "      <td>3.061809</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07068000</th>\n",
       "      <td>11</td>\n",
       "      <td>-90.84762</td>\n",
       "      <td>36.62200</td>\n",
       "      <td>5312.93</td>\n",
       "      <td>-91.25</td>\n",
       "      <td>37.25</td>\n",
       "      <td>4923.1274</td>\n",
       "      <td>0.592192</td>\n",
       "      <td>7.336869</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07226500</th>\n",
       "      <td>11</td>\n",
       "      <td>-103.52579</td>\n",
       "      <td>35.43838</td>\n",
       "      <td>5245.84</td>\n",
       "      <td>-103.75</td>\n",
       "      <td>35.75</td>\n",
       "      <td>5002.1724</td>\n",
       "      <td>0.505891</td>\n",
       "      <td>4.644966</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>09430500</th>\n",
       "      <td>15</td>\n",
       "      <td>-108.53727</td>\n",
       "      <td>33.06118</td>\n",
       "      <td>4809.49</td>\n",
       "      <td>-108.25</td>\n",
       "      <td>33.25</td>\n",
       "      <td>5153.1113</td>\n",
       "      <td>0.474348</td>\n",
       "      <td>7.144647</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06353000</th>\n",
       "      <td>10</td>\n",
       "      <td>-101.33374</td>\n",
       "      <td>46.09167</td>\n",
       "      <td>4605.23</td>\n",
       "      <td>-102.25</td>\n",
       "      <td>46.25</td>\n",
       "      <td>4285.8184</td>\n",
       "      <td>0.341339</td>\n",
       "      <td>6.935845</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05131500</th>\n",
       "      <td>09</td>\n",
       "      <td>-93.54933</td>\n",
       "      <td>48.39578</td>\n",
       "      <td>4344.13</td>\n",
       "      <td>-93.25</td>\n",
       "      <td>47.75</td>\n",
       "      <td>4168.6170</td>\n",
       "      <td>0.485655</td>\n",
       "      <td>4.040227</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         huc_02  gauge_lon  gauge_lat  area_geospa_fabric  snapped_lon  \\\n",
       "gauge_id                                                                 \n",
       "06452000     10  -99.55649   43.74833            25817.78       -99.75   \n",
       "13340000     17 -116.25750   46.47833            14270.76      -115.75   \n",
       "06447000     10 -101.52487   43.75250            12869.46      -101.75   \n",
       "06360500     10 -100.84292   45.25582            12601.47      -101.25   \n",
       "06191500     10 -110.79438   45.11188             6808.28      -110.75   \n",
       "07068000     11  -90.84762   36.62200             5312.93       -91.25   \n",
       "07226500     11 -103.52579   35.43838             5245.84      -103.75   \n",
       "09430500     15 -108.53727   33.06118             4809.49      -108.25   \n",
       "06353000     10 -101.33374   46.09167             4605.23      -102.25   \n",
       "05131500     09  -93.54933   48.39578             4344.13       -93.25   \n",
       "\n",
       "          snapped_lat  snapped_uparea  snapped_iou  area_percent_difference  \\\n",
       "gauge_id                                                                      \n",
       "06452000        43.75      26974.6170     0.644380                 4.480780   \n",
       "13340000        46.25      12915.0530     0.666042                 9.499894   \n",
       "06447000        43.75      13514.7300     0.571147                 5.013967   \n",
       "06360500        45.25      13086.8330     0.774024                 3.851640   \n",
       "06191500        45.25       6599.8230     0.404993                 3.061809   \n",
       "07068000        37.25       4923.1274     0.592192                 7.336869   \n",
       "07226500        35.75       5002.1724     0.505891                 4.644966   \n",
       "09430500        33.25       5153.1113     0.474348                 7.144647   \n",
       "06353000        46.25       4285.8184     0.341339                 6.935845   \n",
       "05131500        47.75       4168.6170     0.485655                 4.040227   \n",
       "\n",
       "          num_nodes  num_edges  \n",
       "gauge_id                        \n",
       "06452000       12.0       11.0  \n",
       "13340000        6.0        5.0  \n",
       "06447000        6.0        5.0  \n",
       "06360500        6.0        5.0  \n",
       "06191500        3.0        2.0  \n",
       "07068000        2.0        1.0  \n",
       "07226500        2.0        1.0  \n",
       "09430500        2.0        1.0  \n",
       "06353000        2.0        1.0  \n",
       "05131500        2.0        1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del camels_attributes_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Node Features as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(SAVE_PATH, \"graph_features\"), exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd = xr.open_dataset(os.path.join(PATHS['gis_ldd'], 'CWatM_30min', 'ldd.nc'))\n",
    "ldd = ldd['ldd']\n",
    "ldd = ldd.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "\n",
    "lons = ldd['lon'].values\n",
    "lats = ldd['lat'].values\n",
    "\n",
    "ds_grid = xr.Dataset({\n",
    "    'lat': (['lat'], lats),\n",
    "    'lon': (['lon'], lons),\n",
    "})\n",
    "\n",
    "# Round the lat lon values to 3 decimal places in ds_grid\n",
    "ds_grid['lat'] = ds_grid['lat'].round(3)\n",
    "ds_grid['lon'] = ds_grid['lon'].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERA5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "2m_temperature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 44.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2891.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowfall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3690.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_net_solar_radiation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:00<00:00, 3182.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_net_thermal_radiation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3087.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_pressure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3884.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_precipitation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3803.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m_dewpoint_temperature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3809.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m_u_component_of_wind\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3597.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m_v_component_of_wind\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3750.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forecast_albedo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2941.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential_evaporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3428.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runoff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3456.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow_albedo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3613.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow_depth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3604.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowmelt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3191.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_surface_runoff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3303.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_runoff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2703.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_column_water\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3526.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3310.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3808.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3122.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3766.10it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = [\n",
    "    '2m_temperature', \n",
    "    'evaporation', \n",
    "    'snowfall', \n",
    "    'surface_net_solar_radiation', \n",
    "    'surface_net_thermal_radiation', \n",
    "    'surface_pressure', \n",
    "    'total_precipitation',\n",
    "    '2m_dewpoint_temperature',\n",
    "    '10m_u_component_of_wind',\n",
    "    '10m_v_component_of_wind',\n",
    "    'forecast_albedo',\n",
    "    'potential_evaporation',\n",
    "    'runoff',\n",
    "    'snow_albedo',\n",
    "    'snow_depth',\n",
    "    'snowmelt',\n",
    "    'sub_surface_runoff',\n",
    "    'surface_runoff',\n",
    "    'total_column_water',\n",
    "    'volumetric_soil_water_layer_1',\n",
    "    'volumetric_soil_water_layer_2',\n",
    "    'volumetric_soil_water_layer_3',\n",
    "    'volumetric_soil_water_layer_4'\n",
    "]\n",
    "\n",
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m_temperature\n",
      "Time: 5.8555 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 21.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaporation\n",
      "Time: 7.4110 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowfall\n",
      "Time: 6.6407 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_net_solar_radiation\n",
      "Time: 6.5198 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_net_thermal_radiation\n",
      "Time: 5.5614 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_pressure\n",
      "Time: 6.6378 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_precipitation\n",
      "Time: 5.9092 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m_dewpoint_temperature\n",
      "Time: 4.6285 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 8168.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m_u_component_of_wind\n",
      "Time: 4.6888 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9547.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m_v_component_of_wind\n",
      "Time: 4.6944 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 8442.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forecast_albedo\n",
      "Time: 4.5303 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9400.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential_evaporation\n",
      "Time: 4.4328 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 8758.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runoff\n",
      "Time: 4.4816 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9619.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow_albedo\n",
      "Time: 4.5534 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 8360.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow_depth\n",
      "Time: 4.4701 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10207.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowmelt\n",
      "Time: 4.5027 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 8859.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_surface_runoff\n",
      "Time: 4.4199 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7767.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_runoff\n",
      "Time: 4.4533 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6375.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_column_water\n",
      "Time: 4.5690 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 8623.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_1\n",
      "Time: 4.4949 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6144.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_2\n",
      "Time: 4.4934 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 8024.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_3\n",
      "Time: 4.4980 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7835.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_4\n",
      "Time: 4.5548 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11140.25it/s]\n"
     ]
    }
   ],
   "source": [
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['RawData'], 'ERA5', var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds['lon'] = [lon_360_180(lon) for lon in ds['lon'].values]\n",
    "    ds = ds.sortby('lon')\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    _, index = np.unique(ds['time'], return_index = True)\n",
    "    ds = ds.isel(time = index)\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_era5_to_cwatm_30min.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder_era5_to_cwatm_30min.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_era5_to_cwatm_30min.nc'))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'Time: {((end_time - start_time) / 60):.4f} mins')\n",
    "    \n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds_regrided.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_soil_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 25.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_high_vegetation_cover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 76.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_low_vegetation_cover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 121.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_type_of_high_vegetation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 123.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_type_of_low_vegetation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 171.07it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = [\n",
    "    'static_soil_type', \n",
    "    'static_high_vegetation_cover', \n",
    "    'static_low_vegetation_cover', \n",
    "    'static_type_of_high_vegetation', \n",
    "    'static_type_of_low_vegetation'\n",
    "    ]\n",
    "ds_filenames = [\n",
    "    'soil_type_static.nc',\n",
    "    'high_vegetation_cover_static.nc',\n",
    "    'low_vegetation_cover_static.nc',\n",
    "    'type_of_high_vegetation_static.nc',\n",
    "    'type_of_low_vegetation_static.nc'\n",
    "]\n",
    "\n",
    "for var_name, ds_filename in zip(var_names, ds_filenames):\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(PATHS['RawData'], 'ERA5', ds_filename))\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.isel(time = 0)\n",
    "    ds = ds.drop('time')\n",
    "    ds = ds.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
    "    ds['lon'] = [lon_360_180(lon) for lon in ds['lon'].values]\n",
    "    ds = ds.sortby('lon')\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[0, node_idx] = int(ds_window_loc.values)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'ERA5'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "        process(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HWSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_CLAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 282.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_GRAVEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 281.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_SAND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 342.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_SILT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 342.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_CLAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 314.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_GRAVEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 358.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_SAND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 354.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_SILT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 358.98it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['S_CLAY', 'S_GRAVEL', 'S_SAND', 'S_SILT', 'T_CLAY', 'T_GRAVEL', 'T_SAND', 'T_SILT']\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(PATHS['HWSD'], f'{var_name}.nc4'))\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['miny'], region_bounds['maxy']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds = ds / 100\n",
    "    ds.load()\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(\n",
    "                lat = slice(lat-resolution/2, lat+resolution/2),\n",
    "                lon = slice(lon-resolution/2, lon+resolution/2)\n",
    "            ).values.mean()\n",
    "            data.loc[0, node_idx] = ds_window_loc\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'HWSD'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'HWSD', f\"{var_name}.csv\"))\n",
    "\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "        process(idx, row)\n",
    "    \n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "Ep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 13617.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMroot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1390.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMsurf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2895.22it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "\n",
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep\n",
      "Time: 0.8309 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7249.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMroot\n",
      "Time: 0.8188 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9393.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMsurf\n",
      "Time: 0.8206 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6557.70it/s]\n"
     ]
    }
   ],
   "source": [
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_gleam_to_cwatm_30min.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder_gleam_to_cwatm_30min.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_gleam_to_cwatm_30min.nc'))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'Time: {((end_time - start_time) / 60):.4f} mins')\n",
    "    \n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds_regrided.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep (Time: 0.4826 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 52.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 1\n",
      "------\n",
      "0 10 06191500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 10 06191500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "SMroot (Time: 0.4898 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 54.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 1\n",
      "------\n",
      "0 10 06191500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 10 06191500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "SMsurf (Time: 0.4872 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 51.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 1\n",
      "------\n",
      "0 10 06191500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 10 06191500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "for var_name in var_names:\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_gleam_to_cwatm_30min.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder_gleam_to_cwatm_30min.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_gleam_to_cwatm_30min.nc'))\n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'{var_name} (Time: {((end_time - start_time) / 60):.4f} mins)')\n",
    "\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "                node_lat = float(round(nodes_coords.loc[node_idx, 'lat'], 3))\n",
    "                node_lon = float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "                multiplier = 1.5\n",
    "                ds_slice = ds_regrided.sel(\n",
    "                    lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                    lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                    )\n",
    "                slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "                slice_df['lat'] = slice_df['lat'].round(3)\n",
    "                slice_df['lon'] = slice_df['lon'].round(3)\n",
    "                slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "                slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "                num_nan_nodes = slice_df.isnull().any(axis=0).sum()\n",
    "                num_nonnan_nodes = len(slice_df.columns) - num_nan_nodes\n",
    "                nodes_coords.loc[node_idx, 'nonNaNneighbours'] = num_nonnan_nodes\n",
    "        nodes_coords_sorted = nodes_coords.sort_values(by = 'nonNaNneighbours', ascending = False)\n",
    "        nodes_coords_sorted = nodes_coords_sorted[nodes_coords_sorted['isNaN']]\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted.shape[0]}\")\n",
    "        \n",
    "        for node_idx in tqdm.tqdm(nodes_coords_sorted.index):\n",
    "            node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "            multiplier = 1.5\n",
    "            ds_slice = ds_regrided.sel(\n",
    "                lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                )\n",
    "            slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "            slice_df['lat'] = slice_df['lat'].round(3)\n",
    "            slice_df['lon'] = slice_df['lon'].round(3)\n",
    "            slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "            slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "            slice_df.columns = list(map(str, slice_df.columns))\n",
    "            num_nonnan_nodes = len(slice_df.columns) - slice_df.isnull().any(axis=0).sum()\n",
    "            # print(node_idx, (node_lat, node_lon), num_nonnan_nodes)\n",
    "            if num_nonnan_nodes == 9:\n",
    "                replacement_values = slice_df.loc[:, f\"({node_lat}, {node_lon})\"]\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "            elif num_nonnan_nodes > 0:\n",
    "                replacement_values = np.nanmean(slice_df, axis = 1)\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                ds_regrided.loc[dict(lat = node_lat, lon = node_lon)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted['isNaN'].sum()}\")\n",
    "        print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solar Insolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solar_insolation(lat, lon, start_date, end_date):\n",
    "    # Constants\n",
    "    Sc = 1361  # Solar constant (W/m^2)\n",
    "    \n",
    "    # Convert dates to datetime objects\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Generate date range\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "    \n",
    "    # Function to calculate solar declination\n",
    "    def solar_declination(n):\n",
    "        return 23.45 * np.sin(np.radians((360 / 365) * (n - 81)))\n",
    "\n",
    "    # Function to calculate cos(theta_z) for solar zenith angle\n",
    "    def cos_theta_z(lat, decl, hour_angle):\n",
    "        lat_rad = np.radians(lat)\n",
    "        decl_rad = np.radians(decl)\n",
    "        return (np.sin(lat_rad) * np.sin(decl_rad) + \n",
    "                np.cos(lat_rad) * np.cos(decl_rad) * np.cos(np.radians(hour_angle)))\n",
    "    \n",
    "    # Function to calculate the hour angle\n",
    "    def hour_angle(lon, date):\n",
    "        # Assuming solar noon (local solar time = 12 hours)\n",
    "        return 0  # hour angle at solar noon\n",
    "    \n",
    "    # Calculate solar insolation for each day\n",
    "    insolation_values = []\n",
    "    for date in dates:\n",
    "        day_of_year = date.day_of_year\n",
    "        declination = solar_declination(day_of_year)\n",
    "        h = hour_angle(lon, date)\n",
    "        cos_zenith_angle = cos_theta_z(lat, declination, h)\n",
    "        \n",
    "        # Insolation formula\n",
    "        insolation = Sc * (1 + 0.033 * np.cos(np.radians(360 * day_of_year / 365))) * cos_zenith_angle\n",
    "        \n",
    "        # Make sure insolation is non-negative\n",
    "        insolation = max(insolation, 0)\n",
    "        insolation_values.append(insolation)\n",
    "    \n",
    "    # Create pandas Series\n",
    "    insolation_series = pd.Series(insolation_values, index=dates, name='Solar Insolation (kW/m²)')\n",
    "    insolation_series = insolation_series / 1000  # Convert to kW/m²\n",
    "    \n",
    "    return insolation_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6254.55it/s]\n"
     ]
    }
   ],
   "source": [
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = dates)\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        ds_window_loc = solar_insolation(lat, lon, '1980-01-01', '2020-12-31')\n",
    "        data.loc[:, node_idx] = ds_window_loc.values\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'solar_insolation.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_time_encoding(start_date, end_date):\n",
    "    # (a) Create a date_range and remove leap days\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    dates = dates[~((dates.month == 2) & (dates.day == 29))]  # Remove February 29 (leap days)\n",
    "    \n",
    "    # (b) Create a dataframe with 'month', 'weekofyear', 'dayofyear' columns\n",
    "    df = pd.DataFrame(index=dates)\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    \n",
    "    # (c) Define lambda transformations for sine encoding\n",
    "    # For day of year (range 1-365), week of year (range 1-52), and month (range 1-12)\n",
    "    sine_transform = lambda x, max_val: np.sin(2 * np.pi * x / max_val)\n",
    "    \n",
    "    # (d) Apply sine transformation and add transformed columns\n",
    "    df['sine_month'] = df['month'].apply(sine_transform, max_val=12)\n",
    "    df['sine_weekofyear'] = df['weekofyear'].apply(sine_transform, max_val=52)\n",
    "    df['sine_dayofyear'] = df['dayofyear'].apply(sine_transform, max_val=365)\n",
    "    \n",
    "    # return df[['sine_month', 'sine_weekofyear', 'sine_dayofyear']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.25it/s]\n"
     ]
    }
   ],
   "source": [
    "df_encoded = sine_time_encoding('1980-01-01', '2020-12-31')\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    df_encoded.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'time_encodings.csv'))\n",
    "\n",
    "# with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    # _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "    process(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daymet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "prcp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 44.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2653.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2813.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3394.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3508.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3340.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dayl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3459.51it/s]\n"
     ]
    }
   ],
   "source": [
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "var_names = ['prcp', 'srad', 'swe', 'tmax', 'tmin', 'vp', 'dayl']\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'Daymet'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'Daymet', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates = ['1980-12-31', '1984-12-31', '1988-12-31', '1992-12-31',\n",
    "               '1996-12-31', '2000-12-31', '2004-12-31', '2008-12-31',\n",
    "               '2012-12-31', '2016-12-31', '2020-12-31']\n",
    "missing_dates = [date + 'T12:00:00' for date in missing_dates]\n",
    "missing_dates = np.array(missing_dates, dtype = 'datetime64')\n",
    "ds_missing_dates = xr.DataArray(\n",
    "    np.nan*np.zeros((len(missing_dates), len(lats), len(lons))),\n",
    "    coords = [missing_dates, lats, lons],\n",
    "    dims = ['time', 'lat', 'lon']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prcp\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 4.04 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 5248.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 3.98 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6091.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 4.09 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7449.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 4.00 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6554.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 3.89 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 4898.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 4.03 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6153.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 4.00 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 4065.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 4.13 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7201.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 0.92 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3048.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srad\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 5.90 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 6.36 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 6.59 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 6.44 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 5.90 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 6.11 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 6.13 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 6.27 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 1.21 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2943.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swe\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 3.94 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 5519.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 3.96 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 5168.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 4.02 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6540.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 3.97 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6561.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 3.82 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7355.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 3.94 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6770.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 3.88 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7180.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 4.02 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7613.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 0.90 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3291.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmax\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 4.85 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6604.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 4.91 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7124.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 5.02 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 4.95 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 5913.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 4.86 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7508.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 4.98 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 8366.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 4.96 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6493.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 5.09 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 1.17 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2820.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmin\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 4.80 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9004.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 4.85 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7898.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 4.91 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7605.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 4.81 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2174.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 4.75 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 5671.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 4.87 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7503.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 4.87 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 8509.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 5.03 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 1.17 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 4404.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vp\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 6.45 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 7.00 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 7.15 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 7.02 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 6.84 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 7.02 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 7.15 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 7.41 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 1.28 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2708.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dayl\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 2.71 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6098.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 2.75 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7246.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 2.84 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6247.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 2.77 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 7401.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 2.70 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6800.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 2.78 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6721.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 2.70 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 2155.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 2.93 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6183.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 0.85 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3007.75it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['prcp', 'srad', 'swe', 'tmax', 'tmin', 'vp', 'dayl']\n",
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['Daymet'], var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds = ds[var_name]\n",
    "    ds = ds.rename({'x': 'lon', 'y': 'lat'})\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_daymet_to_cwatm_30min.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder_daymet_to_cwatm_30min.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_daymet_to_cwatm_30min.nc'))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    # ds_regrided['time'] = ds_regrided['time'].dt.floor('D')\n",
    "    # ds_regrided['time'] = [np.datetime64(str(date).split('T')[0]) for date in ds_regrided['time'].values]\n",
    "\n",
    "    # Concatenate missing dates\n",
    "    ds_regrided = xr.concat([ds_regrided, ds_missing_dates], dim = 'time')\n",
    "    ds_regrided = ds_regrided.sortby('time')\n",
    "    ds.close()\n",
    "    # Print length of time\n",
    "    print(f\"timesteps: {len(ds_regrided['time'])}\")\n",
    "\n",
    "    for start_year in range(1980, 2020+1, 5):\n",
    "        start_date = f\"{start_year}-01-01\"\n",
    "        end_date = f\"{min(start_year+4,2020)}-12-31\"\n",
    "        ds_window = ds_regrided.sel(time = slice(start_date, end_date)).copy()\n",
    "        start_time = time.time()\n",
    "        ds_window.load()\n",
    "        end_time = time.time()\n",
    "        print(start_date, end_date, f\"Time: {(end_time - start_time)/60:.2f} mins\")\n",
    "    \n",
    "        def process(idx, row):\n",
    "            huc, gauge_id = row['huc_02'], row.name\n",
    "            nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "            data = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'Daymet', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "            for node_idx, node_row in nodes_coords.iterrows():\n",
    "                lat, lon = node_row['lat'], node_row['lon']\n",
    "                ds_window_loc = ds_window.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "                data.loc[start_date:end_date, str(node_idx)] = ds_window_loc.values\n",
    "            data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'Daymet', f\"{var_name}.csv\"))\n",
    "            return None\n",
    "        \n",
    "        with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "            _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "        ds_window.close()\n",
    "        del ds_window\n",
    "        gc.collect()\n",
    "\n",
    "    ds_regrided.close()\n",
    "    del ds, ds_regrided\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:10,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['prcp', 'srad', 'swe', 'tmax', 'tmin', 'vp', 'dayl']\n",
    "for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    for var_name in var_names:\n",
    "        data = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'Daymet', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        # Fill the NaN values with a window of 15 days centered around the missing value\n",
    "        for col in data.columns:\n",
    "            data[col] = data[col].fillna(data[col].rolling(15, min_periods = 1, center = True).mean())\n",
    "        data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'Daymet', f\"{var_name}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terrain Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import rioxarray\n",
    "\n",
    "def coords_to_polygon(lon, lat, resolution):\n",
    "    half_res = resolution / 2\n",
    "    return Polygon([\n",
    "        (round(lon - half_res,3), round(lat - half_res,3)),\n",
    "        (round(lon - half_res,3), round(lat + half_res,3)),\n",
    "        (round(lon + half_res,3), round(lat + half_res,3)),\n",
    "        (round(lon + half_res,3), round(lat - half_res,3))\n",
    "    ])\n",
    "def tile_filename_to_coords(filename):\n",
    "    # format: n/s{dd}e/w{ddd}_elv.tif\n",
    "    # n/e: positive, s/w: negative\n",
    "    n_s, lat, e_w, lon = filename[0], int(filename[1:3]), filename[3], int(filename[4:7])\n",
    "    lat = lat if n_s == 'n' else -lat\n",
    "    lon = lon if e_w == 'e' else -lon\n",
    "    return (lon, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:25<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_percentage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_riserun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_degrees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_radians\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curvature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planform_curvature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile_curvature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:19<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "var_names = ['elv', 'slope_percentage', 'slope_riserun', 'slope_degrees', 'slope_radians', 'aspect', 'curvature', 'planform_curvature', 'profile_curvature', 'upa', 'wth']\n",
    "# valid_tiles = ['n30w150', 'n30w120', 'n30w090']\n",
    "\n",
    "issues = []\n",
    "for var_name in itertools.islice(var_names,0,None,1):\n",
    "    print(var_name)\n",
    "    tiles_paths = sorted(glob.glob(os.path.join(PATHS['MERIT-Hydro'], var_name, '**', '*.tif'), recursive=True))\n",
    "    # tiles_paths = [tile for tile in tiles_paths if os.path.basename(os.path.dirname(tile)).split('_')[-1] in valid_tiles]\n",
    "    tiles_filenames = [os.path.basename(tile) for tile in tiles_paths]\n",
    "    tiles_names = [tile.split('_')[0] for tile in tiles_filenames]\n",
    "    tiles_lower_left_corner = [tile_filename_to_coords(tile) for tile in tiles_filenames]\n",
    "    tiles_polygons = [Polygon([(lon, lat), (lon + 5, lat), (lon + 5, lat + 5), (lon, lat + 5)]) for lon, lat in tiles_lower_left_corner]\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = ['mean', 'std', '25%', '50%', '75%'])\n",
    "        cell_polygons = [coords_to_polygon(row['lon'], row['lat'], resolution) for _, row in nodes_coords.iterrows()]\n",
    "        catmt_polygon = cell_polygons[0]\n",
    "        for polygon in cell_polygons[1:]:\n",
    "            catmt_polygon = catmt_polygon.union(polygon)\n",
    "        intersected_tiles = []\n",
    "        for tile_polygon, tile_path in zip(tiles_polygons, tiles_paths):\n",
    "            if tile_polygon.intersects(catmt_polygon):\n",
    "                intersected_tiles.append(tile_path)\n",
    "        ds = rioxarray.open_rasterio(intersected_tiles[0])\n",
    "        for tile in intersected_tiles[1:]:\n",
    "            ds = ds.combine_first(rioxarray.open_rasterio(tile))\n",
    "        ds = ds.sel(band=1)\n",
    "        # Sort the x and y coordinates to be ascending\n",
    "        ds = ds.sortby('x', ascending=True)\n",
    "        ds = ds.sortby('y', ascending=True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            # ds_node = ds.rio.clip_box(lon - resolution/2, lat - resolution/2, lon + resolution/2, lat + resolution/2)\n",
    "            ds_node = ds.sel(x = slice(lon - resolution/2, lon + resolution/2), y = slice(lat - resolution/2, lat + resolution/2))\n",
    "            ds_node = ds_node.where(ds_node != ds.rio.nodata)\n",
    "            ds_node_values = ds_node.values.flatten()\n",
    "            mean = np.nanmean(ds_node_values)\n",
    "            std = np.nanstd(ds_node_values)\n",
    "            q25 = np.nanquantile(ds_node_values, 0.25)\n",
    "            q50 = np.nanquantile(ds_node_values, 0.50)\n",
    "            q75 = np.nanquantile(ds_node_values, 0.75)\n",
    "            data.loc['mean', node_idx] = mean\n",
    "            data.loc['std', node_idx] = std\n",
    "            data.loc['25%', node_idx] = q25\n",
    "            data.loc['50%', node_idx] = q50\n",
    "            data.loc['75%', node_idx] = q75\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'MERIT-Hydro'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'MERIT-Hydro', f\"{var_name}.csv\"))\n",
    "\n",
    "        ds.close()\n",
    "        del ds\n",
    "        gc.collect()\n",
    "\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "        try:\n",
    "            process(idx, row)\n",
    "        except Exception as e:\n",
    "            issues.append(f\"{var_name}-{row['huc_02']}-{row.name}\")\n",
    "            print(f\"Error: {var_name}-{row['huc_02']}-{row.name}. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_name</th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [var_name, huc_02, gauge_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df = [entry.split('-') for entry in issues]\n",
    "issues_df = pd.DataFrame(issues_df, columns = ['var_name', 'huc_02', 'gauge_id'])\n",
    "issues_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_name</th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [var_name, huc_02, gauge_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df[issues_df['var_name'] == 'elv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 129.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    # lon: -180 to 180; lat: -60 to 90\n",
    "    lon_transform = lambda x: np.sin(2 * np.pi * (x+180) / 360)\n",
    "    lat_transform = lambda x: (x - (-60))/(90 - (-60))\n",
    "\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = ['lon_transformed', 'lat_transformed'])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        data.loc['lon_transformed', node_idx] = lon_transform(lon)\n",
    "        data.loc['lat_transformed', node_idx] = lat_transform(lat)\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'spatial_encodings.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uparea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 1497.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "897"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uparea = xr.open_dataset(os.path.join(PATHS['gis_ldd'], 'CWatM_30min/upstream_area_km2.nc'))\n",
    "ds_varname = list(uparea.data_vars)[0]\n",
    "uparea = uparea[ds_varname]\n",
    "uparea = uparea.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "uparea.load()\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        data.loc[0, node_idx] = uparea.sel(lat = lat, lon = lon, method = 'nearest').values.item()\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'uparea.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "uparea.close()\n",
    "del uparea\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 6824.45it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    uparea = row['area_geospa_fabric']\n",
    "\n",
    "    usgs_filepath = os.path.join(PATHS['USGS'], 'CAMELS-US', huc, 'csv', f'{gauge_id}.csv')\n",
    "    usgs_data = pd.read_csv(usgs_filepath, index_col = 0, parse_dates = True)\n",
    "    usgs_data.columns = ['Q_ft3s']\n",
    "    usgs_data['Q_mm'] = ((usgs_data['Q_ft3s'] / (3.28084**3)) / (uparea * 1e6)) * (3600*24*1000)\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    usgs_data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'USGS.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloFAS Parameter Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanbnkf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3291.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanflpn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 4145.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9915.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanlength\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11241.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11072.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9560.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanbw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 3739.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracforest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 8950.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracirrigated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10116.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracrice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9058.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracsealed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 9474.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracwater\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11187.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracother\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11428.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"Catchment_morphology_and_river_network\" (14 surface fields)\n",
    "# - chanbnkf_Global_03min.nc (channel bankfull depth, m);\n",
    "# - chanflpn_Global_03min.nc (width of the floodplain, m);\n",
    "# - changrad_Global_03min.nc (channel longitudinal gradient, m/m);\n",
    "# - chanlength_Global_03min.nc (channel length within a pixel, m);\n",
    "# - chanman_Global_03min.nc (channel Manning's roughness coefficient, m^(1/3)s^(-1));\n",
    "# - chans_Global_03min.nc (channel side slope, m/m);\n",
    "# - chanbw_Global_03min.nc (channel bottom width, m):\n",
    "\n",
    "# \"Land_use\" (7 surface fields)\n",
    "# - fracforest_Global_03min.nc (fraction of forest for each grid-cell, -);\n",
    "# - fracirrigated_Global_03min.nc (fraction of irrigated crops [except rice] for each grid-cell, -);\n",
    "# - fracrice_Global_03min.nc (fraction of rice crops for each grid-cell, -);\n",
    "# - fracsealed_Global_03min.nc (fraction of urban area for each grid-cell, -);\n",
    "# - fracwater_Global_03min.nc (fraction of inland water for each grid-cell, -);\n",
    "# - fracother_Global_03min.nc (fraction of other land cover for each grid-cell, -);\n",
    "Parameter_Maps = os.path.join(PATHS['GloFAS'], 'LISFLOOD_Parameter_Maps')\n",
    "\n",
    "var_names = ['chanbnkf', 'chanflpn', 'changrad', 'chanlength', 'chanman', 'chans', 'chanbw']\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Catchments_morphology_and_river_network', f\"{var_name}_Global_03min.nc\"))['Band1']\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds.load()\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            # ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            ds_window_loc = ds.sel(\n",
    "                lat = slice(lat + 0.5*resolution, lat - 0.5*resolution),\n",
    "                lon = slice(lon - 0.5*resolution, lon + 0.5*resolution)\n",
    "            ).mean()\n",
    "            data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "        os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()\n",
    "\n",
    "var_names = ['fracforest', 'fracirrigated', 'fracrice', 'fracsealed', 'fracwater', 'fracother']\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Land_use', f\"{var_name}_Global_03min.nc\"))['Band1']\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds.load()\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            # ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            ds_window_loc = ds.sel(\n",
    "                lat = slice(lat + 0.5*resolution, lat - 0.5*resolution),\n",
    "                lon = slice(lon - 0.5*resolution, lon + 0.5*resolution)\n",
    "            ).mean()\n",
    "            data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "        os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
