{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import networkx as nx\n",
    "\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import tqdm\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "cfg = configparser.ConfigParser()\n",
    "cfg.optionxform = str\n",
    "cfg.read('/home/sarth/rootdir/assets/global.ini')\n",
    "cfg = {s: dict(cfg.items(s)) for s in cfg.sections()}\n",
    "PATHS = cfg['PATHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME = '15min_MERIT-Plus_CAMELS-IND'\n",
    "SAVE_PATH = os.path.join(PATHS['devp_datasets'], DIRNAME)\n",
    "resolution = 0.25\n",
    "lon_360_180 = lambda x: (x + 180) % 360 - 180 # convert 0-360 to -180-180\n",
    "lon_180_360 = lambda x: x % 360 # convert -180-180 to 0-360\n",
    "region_bounds = {\n",
    "    'minx': 66,\n",
    "    'miny': 5,\n",
    "    'maxx': 100,\n",
    "    'maxy': 30\n",
    "}\n",
    "# minx, miny, maxx, maxy = 66, 5, 100, 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Watershed Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>huc_02</th>\n",
       "      <th>ghi_lon</th>\n",
       "      <th>ghi_lat</th>\n",
       "      <th>ghi_area</th>\n",
       "      <th>cwc_lon</th>\n",
       "      <th>cwc_lat</th>\n",
       "      <th>cwc_area</th>\n",
       "      <th>cwc_site_name</th>\n",
       "      <th>ghi_stn_id</th>\n",
       "      <th>river_basin</th>\n",
       "      <th>cwc_river</th>\n",
       "      <th>flow_availability</th>\n",
       "      <th>snapped_lon</th>\n",
       "      <th>snapped_lat</th>\n",
       "      <th>snapped_uparea</th>\n",
       "      <th>snapped_iou</th>\n",
       "      <th>area_percent_difference</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_edges</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gauge_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14015</th>\n",
       "      <td>14</td>\n",
       "      <td>73.11033</td>\n",
       "      <td>18.73540</td>\n",
       "      <td>125.7</td>\n",
       "      <td>73.1108</td>\n",
       "      <td>18.7367</td>\n",
       "      <td>125.0</td>\n",
       "      <td>Pen</td>\n",
       "      <td>wfrn_penxx</td>\n",
       "      <td>WFRN</td>\n",
       "      <td>Bhogeswari</td>\n",
       "      <td>31.36</td>\n",
       "      <td>73.375</td>\n",
       "      <td>18.625</td>\n",
       "      <td>733.7</td>\n",
       "      <td>0.010124</td>\n",
       "      <td>483.691350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14009</th>\n",
       "      <td>14</td>\n",
       "      <td>73.28122</td>\n",
       "      <td>18.23122</td>\n",
       "      <td>292.3</td>\n",
       "      <td>73.2833</td>\n",
       "      <td>18.2317</td>\n",
       "      <td>259.0</td>\n",
       "      <td>Mangaon (Seasonal)</td>\n",
       "      <td>wfrn_manga</td>\n",
       "      <td>WFRN</td>\n",
       "      <td>Savitri/Kal</td>\n",
       "      <td>67.40</td>\n",
       "      <td>73.375</td>\n",
       "      <td>18.375</td>\n",
       "      <td>734.8</td>\n",
       "      <td>0.342040</td>\n",
       "      <td>151.385570</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15006</th>\n",
       "      <td>15</td>\n",
       "      <td>74.88124</td>\n",
       "      <td>13.51876</td>\n",
       "      <td>299.6</td>\n",
       "      <td>74.8800</td>\n",
       "      <td>13.5214</td>\n",
       "      <td>253.0</td>\n",
       "      <td>Avershe</td>\n",
       "      <td>wfrs_avers</td>\n",
       "      <td>WFRS</td>\n",
       "      <td>Seetha</td>\n",
       "      <td>39.00</td>\n",
       "      <td>75.125</td>\n",
       "      <td>13.375</td>\n",
       "      <td>753.2</td>\n",
       "      <td>0.163620</td>\n",
       "      <td>151.401870</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05025</th>\n",
       "      <td>05</td>\n",
       "      <td>78.05617</td>\n",
       "      <td>11.93959</td>\n",
       "      <td>356.0</td>\n",
       "      <td>78.0572</td>\n",
       "      <td>11.9383</td>\n",
       "      <td>362.0</td>\n",
       "      <td>Thoppur</td>\n",
       "      <td>cauv_thopp</td>\n",
       "      <td>Cauvery</td>\n",
       "      <td>Cauvery/Thoppaiyar</td>\n",
       "      <td>31.52</td>\n",
       "      <td>78.125</td>\n",
       "      <td>11.875</td>\n",
       "      <td>757.7</td>\n",
       "      <td>0.275466</td>\n",
       "      <td>112.837074</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15032</th>\n",
       "      <td>15</td>\n",
       "      <td>74.98123</td>\n",
       "      <td>13.29791</td>\n",
       "      <td>356.9</td>\n",
       "      <td>74.9806</td>\n",
       "      <td>13.2942</td>\n",
       "      <td>327.0</td>\n",
       "      <td>Yennehole</td>\n",
       "      <td>wfrs_yenne</td>\n",
       "      <td>WFRS</td>\n",
       "      <td>Swarna</td>\n",
       "      <td>70.67</td>\n",
       "      <td>75.125</td>\n",
       "      <td>13.125</td>\n",
       "      <td>754.0</td>\n",
       "      <td>0.208032</td>\n",
       "      <td>111.263670</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04062</th>\n",
       "      <td>04</td>\n",
       "      <td>80.06876</td>\n",
       "      <td>16.79375</td>\n",
       "      <td>240055.4</td>\n",
       "      <td>80.0692</td>\n",
       "      <td>16.7942</td>\n",
       "      <td>235544.0</td>\n",
       "      <td>Wadenapally</td>\n",
       "      <td>kris_waden</td>\n",
       "      <td>Krishna</td>\n",
       "      <td>Krishna</td>\n",
       "      <td>95.43</td>\n",
       "      <td>79.875</td>\n",
       "      <td>16.625</td>\n",
       "      <td>245756.9</td>\n",
       "      <td>0.915327</td>\n",
       "      <td>2.375077</td>\n",
       "      <td>331.0</td>\n",
       "      <td>330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04060</th>\n",
       "      <td>04</td>\n",
       "      <td>80.61874</td>\n",
       "      <td>16.49796</td>\n",
       "      <td>257260.0</td>\n",
       "      <td>80.6250</td>\n",
       "      <td>16.5011</td>\n",
       "      <td>251360.0</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>kris_vijay</td>\n",
       "      <td>Krishna</td>\n",
       "      <td>Krishna</td>\n",
       "      <td>84.90</td>\n",
       "      <td>80.375</td>\n",
       "      <td>16.625</td>\n",
       "      <td>261282.5</td>\n",
       "      <td>0.913691</td>\n",
       "      <td>1.563593</td>\n",
       "      <td>352.0</td>\n",
       "      <td>351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03073</th>\n",
       "      <td>03</td>\n",
       "      <td>80.39375</td>\n",
       "      <td>18.58542</td>\n",
       "      <td>267340.4</td>\n",
       "      <td>80.3958</td>\n",
       "      <td>18.5872</td>\n",
       "      <td>268200.0</td>\n",
       "      <td>Perur</td>\n",
       "      <td>goda_perur</td>\n",
       "      <td>Godavari</td>\n",
       "      <td>Godavari</td>\n",
       "      <td>94.32</td>\n",
       "      <td>80.375</td>\n",
       "      <td>18.625</td>\n",
       "      <td>267352.8</td>\n",
       "      <td>0.918200</td>\n",
       "      <td>0.004641</td>\n",
       "      <td>367.0</td>\n",
       "      <td>366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03043</th>\n",
       "      <td>03</td>\n",
       "      <td>81.38540</td>\n",
       "      <td>17.48542</td>\n",
       "      <td>304628.7</td>\n",
       "      <td>81.3875</td>\n",
       "      <td>17.4875</td>\n",
       "      <td>305460.0</td>\n",
       "      <td>Koida</td>\n",
       "      <td>goda_koida</td>\n",
       "      <td>Godavari</td>\n",
       "      <td>Godavari</td>\n",
       "      <td>64.48</td>\n",
       "      <td>81.375</td>\n",
       "      <td>17.625</td>\n",
       "      <td>305596.3</td>\n",
       "      <td>0.925894</td>\n",
       "      <td>0.317641</td>\n",
       "      <td>419.0</td>\n",
       "      <td>418.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03075</th>\n",
       "      <td>03</td>\n",
       "      <td>81.66043</td>\n",
       "      <td>17.25215</td>\n",
       "      <td>306683.6</td>\n",
       "      <td>81.6564</td>\n",
       "      <td>17.2519</td>\n",
       "      <td>307800.0</td>\n",
       "      <td>Polavaram</td>\n",
       "      <td>goda_polav</td>\n",
       "      <td>Godavari</td>\n",
       "      <td>Godavari</td>\n",
       "      <td>94.03</td>\n",
       "      <td>81.625</td>\n",
       "      <td>17.375</td>\n",
       "      <td>308550.9</td>\n",
       "      <td>0.924652</td>\n",
       "      <td>0.608873</td>\n",
       "      <td>423.0</td>\n",
       "      <td>422.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         huc_02   ghi_lon   ghi_lat  ghi_area  cwc_lon  cwc_lat  cwc_area  \\\n",
       "gauge_id                                                                    \n",
       "14015        14  73.11033  18.73540     125.7  73.1108  18.7367     125.0   \n",
       "14009        14  73.28122  18.23122     292.3  73.2833  18.2317     259.0   \n",
       "15006        15  74.88124  13.51876     299.6  74.8800  13.5214     253.0   \n",
       "05025        05  78.05617  11.93959     356.0  78.0572  11.9383     362.0   \n",
       "15032        15  74.98123  13.29791     356.9  74.9806  13.2942     327.0   \n",
       "...         ...       ...       ...       ...      ...      ...       ...   \n",
       "04062        04  80.06876  16.79375  240055.4  80.0692  16.7942  235544.0   \n",
       "04060        04  80.61874  16.49796  257260.0  80.6250  16.5011  251360.0   \n",
       "03073        03  80.39375  18.58542  267340.4  80.3958  18.5872  268200.0   \n",
       "03043        03  81.38540  17.48542  304628.7  81.3875  17.4875  305460.0   \n",
       "03075        03  81.66043  17.25215  306683.6  81.6564  17.2519  307800.0   \n",
       "\n",
       "               cwc_site_name  ghi_stn_id river_basin           cwc_river  \\\n",
       "gauge_id                                                                   \n",
       "14015                    Pen  wfrn_penxx        WFRN          Bhogeswari   \n",
       "14009     Mangaon (Seasonal)  wfrn_manga        WFRN         Savitri/Kal   \n",
       "15006                Avershe  wfrs_avers        WFRS              Seetha   \n",
       "05025                Thoppur  cauv_thopp     Cauvery  Cauvery/Thoppaiyar   \n",
       "15032              Yennehole  wfrs_yenne        WFRS              Swarna   \n",
       "...                      ...         ...         ...                 ...   \n",
       "04062            Wadenapally  kris_waden     Krishna             Krishna   \n",
       "04060             Vijayawada  kris_vijay     Krishna             Krishna   \n",
       "03073                  Perur  goda_perur    Godavari            Godavari   \n",
       "03043                  Koida  goda_koida    Godavari            Godavari   \n",
       "03075              Polavaram  goda_polav    Godavari            Godavari   \n",
       "\n",
       "          flow_availability  snapped_lon  snapped_lat  snapped_uparea  \\\n",
       "gauge_id                                                                \n",
       "14015                 31.36       73.375       18.625           733.7   \n",
       "14009                 67.40       73.375       18.375           734.8   \n",
       "15006                 39.00       75.125       13.375           753.2   \n",
       "05025                 31.52       78.125       11.875           757.7   \n",
       "15032                 70.67       75.125       13.125           754.0   \n",
       "...                     ...          ...          ...             ...   \n",
       "04062                 95.43       79.875       16.625        245756.9   \n",
       "04060                 84.90       80.375       16.625        261282.5   \n",
       "03073                 94.32       80.375       18.625        267352.8   \n",
       "03043                 64.48       81.375       17.625        305596.3   \n",
       "03075                 94.03       81.625       17.375        308550.9   \n",
       "\n",
       "          snapped_iou  area_percent_difference  num_nodes  num_edges  \n",
       "gauge_id                                                              \n",
       "14015        0.010124               483.691350        1.0        0.0  \n",
       "14009        0.342040               151.385570        1.0        0.0  \n",
       "15006        0.163620               151.401870        1.0        0.0  \n",
       "05025        0.275466               112.837074        1.0        0.0  \n",
       "15032        0.208032               111.263670        1.0        0.0  \n",
       "...               ...                      ...        ...        ...  \n",
       "04062        0.915327                 2.375077      331.0      330.0  \n",
       "04060        0.913691                 1.563593      352.0      351.0  \n",
       "03073        0.918200                 0.004641      367.0      366.0  \n",
       "03043        0.925894                 0.317641      419.0      418.0  \n",
       "03075        0.924652                 0.608873      423.0      422.0  \n",
       "\n",
       "[242 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_attributes_graph = pd.read_csv(os.path.join(SAVE_PATH, 'graph_attributes.csv'), index_col=0)\n",
    "camels_attributes_graph.index = camels_attributes_graph.index.map(lambda x: str(x).zfill(5))\n",
    "camels_attributes_graph['huc_02'] = camels_attributes_graph['huc_02'].map(lambda x: str(x).zfill(2))\n",
    "camels_attributes_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 19)\n",
      "(200, 19)\n",
      "(88, 19)\n",
      "(78, 19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "huc_02\n",
       "03    20\n",
       "04    12\n",
       "05     4\n",
       "06     1\n",
       "07     4\n",
       "08    10\n",
       "09     2\n",
       "10     3\n",
       "11     2\n",
       "12     2\n",
       "13     2\n",
       "14     2\n",
       "15     4\n",
       "16     4\n",
       "17     6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_graph = camels_attributes_graph.copy()\n",
    "print(camels_graph.shape)\n",
    "camels_graph = camels_graph[camels_graph['ghi_area'] <= 30000]\n",
    "print(camels_graph.shape)\n",
    "camels_graph = camels_graph[camels_graph['area_percent_difference'] < 10]\n",
    "print(camels_graph.shape)\n",
    "camels_graph = camels_graph[camels_graph['num_nodes'] > 1]\n",
    "print(camels_graph.shape)\n",
    "# Print the number of graphs per 'huc_02' (sorted in values of huc_02)\n",
    "camels_graph.sort_values(ascending=True, by = 'huc_02').groupby('huc_02').size()\n",
    "# camels_graph['huc_02'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       78.000000\n",
       "mean      9168.983333\n",
       "std       6669.060364\n",
       "min       1482.200000\n",
       "25%       3688.175000\n",
       "50%       7156.800000\n",
       "75%      12445.250000\n",
       "max      29822.500000\n",
       "Name: ghi_area, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_graph['ghi_area'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    78.000000\n",
       "mean     12.551282\n",
       "std       9.219752\n",
       "min       2.000000\n",
       "25%       5.000000\n",
       "50%      10.000000\n",
       "75%      17.750000\n",
       "max      39.000000\n",
       "Name: num_nodes, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_graph['num_nodes'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del camels_attributes_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Node Features as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(SAVE_PATH, \"graph_features\"), exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd = xr.open_dataset(os.path.join(PATHS['gis_ldd'], 'MERIT-Plus_15min', 'ldd.nc'))\n",
    "ldd = ldd['ldd']\n",
    "ldd = ldd.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "\n",
    "lons = ldd['lon'].values\n",
    "lats = ldd['lat'].values\n",
    "\n",
    "ds_grid = xr.Dataset({\n",
    "    'lat': (['lat'], lats),\n",
    "    'lon': (['lon'], lons),\n",
    "})\n",
    "\n",
    "# Round the lat lon values to 3 decimal places in ds_grid\n",
    "ds_grid['lat'] = ds_grid['lat'].round(3)\n",
    "ds_grid['lon'] = ds_grid['lon'].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERA5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_names = [\n",
    "#     '2m_temperature', \n",
    "#     'evaporation', \n",
    "#     'snowfall', \n",
    "#     'surface_net_solar_radiation', \n",
    "#     'surface_net_thermal_radiation', \n",
    "#     'surface_pressure', \n",
    "#     'total_precipitation',\n",
    "#     '2m_dewpoint_temperature',\n",
    "#     '10m_u_component_of_wind',\n",
    "#     '10m_v_component_of_wind',\n",
    "#     'forecast_albedo',\n",
    "#     'potential_evaporation',\n",
    "#     'runoff',\n",
    "#     'snow_albedo',\n",
    "#     'snow_depth',\n",
    "#     'snowmelt',\n",
    "#     'sub_surface_runoff',\n",
    "#     'surface_runoff',\n",
    "#     'total_column_water',\n",
    "#     'volumetric_soil_water_layer_1',\n",
    "#     'volumetric_soil_water_layer_2',\n",
    "#     'volumetric_soil_water_layer_3',\n",
    "#     'volumetric_soil_water_layer_4'\n",
    "# ]\n",
    "# dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "# dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "# print(f\"Number of dates: {len(dates)}\")\n",
    "# def process(idx, row, var_name):\n",
    "#     huc, gauge_id = row['huc_02'], row.name\n",
    "#     nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "#     data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "#     os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "#     os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5'), exist_ok = True)\n",
    "#     data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"))\n",
    "# for var_name in var_names:\n",
    "#     print(var_name)\n",
    "#     with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "#         _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "#     print(var_name)\n",
    "#     ds = xr.open_mfdataset(os.path.join(PATHS['RawData'], 'ERA5', var_name, f\"*.nc\"), combine='by_coords')\n",
    "#     ds_var_name = list(ds.data_vars)[0]\n",
    "#     ds = ds[ds_var_name]\n",
    "#     ds = ds.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
    "#     ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "#     ds['lon'] = [lon_360_180(lon) for lon in ds['lon'].values]\n",
    "#     ds = ds.sortby('lon')\n",
    "#     ds = ds.sel(\n",
    "#         lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "#         lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "#     )\n",
    "#     _, index = np.unique(ds['time'], return_index = True)\n",
    "#     ds = ds.isel(time = index)\n",
    "\n",
    "#     if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_era5_to_merit-plus_15min_IND.nc')):\n",
    "#         regridder = xe.Regridder(\n",
    "#             ds, \n",
    "#             ds_grid, \n",
    "#             'bilinear', \n",
    "#             reuse_weights=True, \n",
    "#             filename = os.path.join(PATHS['Assets'], 'regridder_era5_to_merit-plus_15min_IND.nc')\n",
    "#         )\n",
    "#     else:\n",
    "#         regridder = xe.Regridder(\n",
    "#             ds, \n",
    "#             ds_grid, \n",
    "#             'bilinear', \n",
    "#             reuse_weights=False\n",
    "#         )\n",
    "#         regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_era5_to_merit-plus_15min_IND.nc'))\n",
    "    \n",
    "#     ds_regrided = regridder(ds)\n",
    "#     ds.close()\n",
    "#     start_time = time.time()\n",
    "#     ds_regrided.load()\n",
    "#     end_time = time.time()\n",
    "#     print(f'Time: {((end_time - start_time) / 60):.4f} mins')\n",
    "    \n",
    "#     def process(idx, row):\n",
    "#         huc, gauge_id = row['huc_02'], row.name\n",
    "#         nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "#         data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "#         for node_idx, node_row in nodes_coords.iterrows():\n",
    "#             lat, lon = node_row['lat'], node_row['lon']\n",
    "#             ds_window_loc = ds_regrided.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "#             data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "#         data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "#     with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "#         _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "#     ds.close()\n",
    "#     del ds\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_soil_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 52.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_high_vegetation_cover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 76.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_low_vegetation_cover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 77.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_type_of_high_vegetation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 77.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_type_of_low_vegetation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:01<00:00, 74.55it/s] \n"
     ]
    }
   ],
   "source": [
    "var_names = [\n",
    "    'static_soil_type', \n",
    "    'static_high_vegetation_cover', \n",
    "    'static_low_vegetation_cover', \n",
    "    'static_type_of_high_vegetation', \n",
    "    'static_type_of_low_vegetation'\n",
    "    ]\n",
    "ds_filenames = [\n",
    "    'soil_type_static.nc',\n",
    "    'high_vegetation_cover_static.nc',\n",
    "    'low_vegetation_cover_static.nc',\n",
    "    'type_of_high_vegetation_static.nc',\n",
    "    'type_of_low_vegetation_static.nc'\n",
    "]\n",
    "\n",
    "for var_name, ds_filename in zip(var_names, ds_filenames):\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(PATHS['RawData'], 'ERA5', ds_filename))\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.isel(time = 0)\n",
    "    ds = ds.drop('time')\n",
    "    ds = ds.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
    "    ds['lon'] = [lon_360_180(lon) for lon in ds['lon'].values]\n",
    "    ds = ds.sortby('lon')\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[0, node_idx] = int(ds_window_loc.values)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'ERA5'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "        process(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HWSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_CLAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 166.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_GRAVEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 176.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_SAND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 177.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_SILT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 174.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_CLAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 175.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_GRAVEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 175.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_SAND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 174.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_SILT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 173.98it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['S_CLAY', 'S_GRAVEL', 'S_SAND', 'S_SILT', 'T_CLAY', 'T_GRAVEL', 'T_SAND', 'T_SILT']\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(PATHS['HWSD'], f'{var_name}.nc4'))\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['miny'], region_bounds['maxy']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds = ds / 100\n",
    "    ds.load()\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(\n",
    "                lat = slice(lat-resolution/2, lat+resolution/2),\n",
    "                lon = slice(lon-resolution/2, lon+resolution/2)\n",
    "            ).values.mean()\n",
    "            data.loc[0, node_idx] = ds_window_loc\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'HWSD'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'HWSD', f\"{var_name}.csv\"))\n",
    "\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "        process(idx, row)\n",
    "    \n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "Ep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 564.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMroot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 535.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMsurf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 574.40it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "\n",
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep\n",
      "Time: 1.1553 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:03<00:00, 24.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMroot\n",
      "Time: 1.0796 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:03<00:00, 23.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMsurf\n",
      "Time: 1.0914 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:03<00:00, 22.87it/s]\n"
     ]
    }
   ],
   "source": [
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_gleam_to_merit-plus_15min_IND.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder_gleam_to_merit-plus_15min_IND.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_gleam_to_merit-plus_15min_IND.nc'))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'Time: {((end_time - start_time) / 60):.4f} mins')\n",
    "    \n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds_regrided.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep (Time: 0.5297 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [00:02, 27.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n",
      "SMroot (Time: 0.5279 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [00:02, 27.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n",
      "SMsurf (Time: 0.5348 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [00:02, 28.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "for var_name in var_names:\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_gleam_to_merit-plus_15min_IND.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder_gleam_to_merit-plus_15min_IND.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_gleam_to_merit-plus_15min_IND.nc'))\n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'{var_name} (Time: {((end_time - start_time) / 60):.4f} mins)')\n",
    "\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "                node_lat = float(round(nodes_coords.loc[node_idx, 'lat'], 3))\n",
    "                node_lon = float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "                multiplier = 1.5\n",
    "                ds_slice = ds_regrided.sel(\n",
    "                    lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                    lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                    )\n",
    "                slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "                slice_df['lat'] = slice_df['lat'].round(3)\n",
    "                slice_df['lon'] = slice_df['lon'].round(3)\n",
    "                slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "                slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "                num_nan_nodes = slice_df.isnull().any(axis=0).sum()\n",
    "                num_nonnan_nodes = len(slice_df.columns) - num_nan_nodes\n",
    "                nodes_coords.loc[node_idx, 'nonNaNneighbours'] = num_nonnan_nodes\n",
    "        nodes_coords_sorted = nodes_coords.sort_values(by = 'nonNaNneighbours', ascending = False)\n",
    "        nodes_coords_sorted = nodes_coords_sorted[nodes_coords_sorted['isNaN']]\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted.shape[0]}\")\n",
    "        \n",
    "        for node_idx in tqdm.tqdm(nodes_coords_sorted.index):\n",
    "            node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "            multiplier = 1.5\n",
    "            ds_slice = ds_regrided.sel(\n",
    "                lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                )\n",
    "            slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "            slice_df['lat'] = slice_df['lat'].round(3)\n",
    "            slice_df['lon'] = slice_df['lon'].round(3)\n",
    "            slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "            slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "            slice_df.columns = list(map(str, slice_df.columns))\n",
    "            num_nonnan_nodes = len(slice_df.columns) - slice_df.isnull().any(axis=0).sum()\n",
    "            # print(node_idx, (node_lat, node_lon), num_nonnan_nodes)\n",
    "            if num_nonnan_nodes == 9:\n",
    "                replacement_values = slice_df.loc[:, f\"({node_lat}, {node_lon})\"]\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "            elif num_nonnan_nodes > 0:\n",
    "                replacement_values = np.nanmean(slice_df, axis = 1)\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                ds_regrided.loc[dict(lat = node_lat, lon = node_lon)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted['isNaN'].sum()}\")\n",
    "        print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "for var_name in var_names:\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "for var_name in var_names:\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "\n",
    "        \n",
    "        for node_idx in tqdm.tqdm(nodes_coords[nodes_coords['isNaN']].index):\n",
    "            nodes_coords['distances'] = None\n",
    "            node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "            for node_idx2 in nodes_coords[nodes_coords['isNaN'] == False].index:\n",
    "                if node_idx != node_idx2:\n",
    "                    node_lat2, node_lon2 = float(round(nodes_coords.loc[node_idx2, 'lat'], 3)), float(round(nodes_coords.loc[node_idx2, 'lon'], 3))\n",
    "                    distance = np.sqrt((node_lat - node_lat2)**2 + (node_lon - node_lon2)**2)\n",
    "                    nodes_coords.loc[node_idx2, 'distances'] = distance\n",
    "            min_distance = nodes_coords.loc[nodes_coords['distances'].idxmin(), 'distances']\n",
    "            # Replace with mean of nodes having distance equal to min_distance\n",
    "            replacement_nodes = nodes_coords[nodes_coords['distances'] == min_distance].index\n",
    "            replacement_nodes = list(map(str, replacement_nodes))\n",
    "            replacement_values = data.loc[:, replacement_nodes].mean(axis = 1)\n",
    "            data.loc[:, str(node_idx)] = replacement_values\n",
    "            nodes_coords.loc[node_idx, 'isNaN'] = False\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "        print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solar Insolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solar_insolation(lat, lon, start_date, end_date):\n",
    "    # Constants\n",
    "    Sc = 1361  # Solar constant (W/m^2)\n",
    "    \n",
    "    # Convert dates to datetime objects\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Generate date range\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "    \n",
    "    # Function to calculate solar declination\n",
    "    def solar_declination(n):\n",
    "        return 23.45 * np.sin(np.radians((360 / 365) * (n - 81)))\n",
    "\n",
    "    # Function to calculate cos(theta_z) for solar zenith angle\n",
    "    def cos_theta_z(lat, decl, hour_angle):\n",
    "        lat_rad = np.radians(lat)\n",
    "        decl_rad = np.radians(decl)\n",
    "        return (np.sin(lat_rad) * np.sin(decl_rad) + \n",
    "                np.cos(lat_rad) * np.cos(decl_rad) * np.cos(np.radians(hour_angle)))\n",
    "    \n",
    "    # Function to calculate the hour angle\n",
    "    def hour_angle(lon, date):\n",
    "        # Assuming solar noon (local solar time = 12 hours)\n",
    "        return 0  # hour angle at solar noon\n",
    "    \n",
    "    # Calculate solar insolation for each day\n",
    "    insolation_values = []\n",
    "    for date in dates:\n",
    "        day_of_year = date.day_of_year\n",
    "        declination = solar_declination(day_of_year)\n",
    "        h = hour_angle(lon, date)\n",
    "        cos_zenith_angle = cos_theta_z(lat, declination, h)\n",
    "        \n",
    "        # Insolation formula\n",
    "        insolation = Sc * (1 + 0.033 * np.cos(np.radians(360 * day_of_year / 365))) * cos_zenith_angle\n",
    "        \n",
    "        # Make sure insolation is non-negative\n",
    "        insolation = max(insolation, 0)\n",
    "        insolation_values.append(insolation)\n",
    "    \n",
    "    # Create pandas Series\n",
    "    insolation_series = pd.Series(insolation_values, index=dates, name='Solar Insolation (kW/m²)')\n",
    "    insolation_series = insolation_series / 1000  # Convert to kW/m²\n",
    "    \n",
    "    return insolation_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/78 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:12<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = dates)\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        ds_window_loc = solar_insolation(lat, lon, '1980-01-01', '2020-12-31')\n",
    "        data.loc[:, node_idx] = ds_window_loc.values\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'solar_insolation.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_time_encoding(start_date, end_date):\n",
    "    # (a) Create a date_range and remove leap days\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    dates = dates[~((dates.month == 2) & (dates.day == 29))]  # Remove February 29 (leap days)\n",
    "    \n",
    "    # (b) Create a dataframe with 'month', 'weekofyear', 'dayofyear' columns\n",
    "    df = pd.DataFrame(index=dates)\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    \n",
    "    # (c) Define lambda transformations for sine encoding\n",
    "    # For day of year (range 1-365), week of year (range 1-52), and month (range 1-12)\n",
    "    sine_transform = lambda x, max_val: np.sin(2 * np.pi * x / max_val)\n",
    "    \n",
    "    # (d) Apply sine transformation and add transformed columns\n",
    "    df['sine_month'] = df['month'].apply(sine_transform, max_val=12)\n",
    "    df['sine_weekofyear'] = df['weekofyear'].apply(sine_transform, max_val=52)\n",
    "    df['sine_dayofyear'] = df['dayofyear'].apply(sine_transform, max_val=365)\n",
    "    \n",
    "    # return df[['sine_month', 'sine_weekofyear', 'sine_dayofyear']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:07<00:00, 10.82it/s]\n"
     ]
    }
   ],
   "source": [
    "df_encoded = sine_time_encoding('1980-01-01', '2020-12-31')\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    df_encoded.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'time_encodings.csv'))\n",
    "\n",
    "# with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    # _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "    process(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terrain Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import rioxarray\n",
    "\n",
    "def coords_to_polygon(lon, lat, resolution):\n",
    "    half_res = resolution / 2\n",
    "    return Polygon([\n",
    "        (round(lon - half_res,3), round(lat - half_res,3)),\n",
    "        (round(lon - half_res,3), round(lat + half_res,3)),\n",
    "        (round(lon + half_res,3), round(lat + half_res,3)),\n",
    "        (round(lon + half_res,3), round(lat - half_res,3))\n",
    "    ])\n",
    "def tile_filename_to_coords(filename):\n",
    "    # format: n/s{dd}e/w{ddd}_elv.tif\n",
    "    # n/e: positive, s/w: negative\n",
    "    n_s, lat, e_w, lon = filename[0], int(filename[1:3]), filename[3], int(filename[4:7])\n",
    "    lat = lat if n_s == 'n' else -lat\n",
    "    lon = lon if e_w == 'e' else -lon\n",
    "    return (lon, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/78 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:55<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_percentage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:10<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_riserun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:06<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_degrees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:13<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_radians\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:06<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:11<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curvature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:21<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planform_curvature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:16<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile_curvature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:16<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:47<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [01:25<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "var_names = ['elv', 'slope_percentage', 'slope_riserun', 'slope_degrees', 'slope_radians', 'aspect', 'curvature', 'planform_curvature', 'profile_curvature', 'upa', 'wth']\n",
    "# valid_tiles = ['n30w150', 'n30w120', 'n30w090']\n",
    "\n",
    "issues = []\n",
    "for var_name in itertools.islice(var_names,0,None,1):\n",
    "    print(var_name)\n",
    "    tiles_paths = sorted(glob.glob(os.path.join(PATHS['MERIT-Hydro'], var_name, '**', '*.tif'), recursive=True))\n",
    "    # tiles_paths = [tile for tile in tiles_paths if os.path.basename(os.path.dirname(tile)).split('_')[-1] in valid_tiles]\n",
    "    tiles_filenames = [os.path.basename(tile) for tile in tiles_paths]\n",
    "    tiles_names = [tile.split('_')[0] for tile in tiles_filenames]\n",
    "    tiles_lower_left_corner = [tile_filename_to_coords(tile) for tile in tiles_filenames]\n",
    "    tiles_polygons = [Polygon([(lon, lat), (lon + 5, lat), (lon + 5, lat + 5), (lon, lat + 5)]) for lon, lat in tiles_lower_left_corner]\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = ['mean', 'std', '25%', '50%', '75%'])\n",
    "        cell_polygons = [coords_to_polygon(row['lon'], row['lat'], resolution) for _, row in nodes_coords.iterrows()]\n",
    "        catmt_polygon = cell_polygons[0]\n",
    "        for polygon in cell_polygons[1:]:\n",
    "            catmt_polygon = catmt_polygon.union(polygon)\n",
    "        intersected_tiles = []\n",
    "        for tile_polygon, tile_path in zip(tiles_polygons, tiles_paths):\n",
    "            if tile_polygon.intersects(catmt_polygon):\n",
    "                intersected_tiles.append(tile_path)\n",
    "        ds = rioxarray.open_rasterio(intersected_tiles[0])\n",
    "        for tile in intersected_tiles[1:]:\n",
    "            ds = ds.combine_first(rioxarray.open_rasterio(tile))\n",
    "        ds = ds.sel(band=1)\n",
    "        # Sort the x and y coordinates to be ascending\n",
    "        ds = ds.sortby('x', ascending=True)\n",
    "        ds = ds.sortby('y', ascending=True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            # ds_node = ds.rio.clip_box(lon - resolution/2, lat - resolution/2, lon + resolution/2, lat + resolution/2)\n",
    "            ds_node = ds.sel(x = slice(lon - resolution/2, lon + resolution/2), y = slice(lat - resolution/2, lat + resolution/2))\n",
    "            ds_node = ds_node.where(ds_node != ds.rio.nodata)\n",
    "            ds_node_values = ds_node.values.flatten()\n",
    "            mean = np.nanmean(ds_node_values)\n",
    "            std = np.nanstd(ds_node_values)\n",
    "            q25 = np.nanquantile(ds_node_values, 0.25)\n",
    "            q50 = np.nanquantile(ds_node_values, 0.50)\n",
    "            q75 = np.nanquantile(ds_node_values, 0.75)\n",
    "            data.loc['mean', node_idx] = mean\n",
    "            data.loc['std', node_idx] = std\n",
    "            data.loc['25%', node_idx] = q25\n",
    "            data.loc['50%', node_idx] = q50\n",
    "            data.loc['75%', node_idx] = q75\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'MERIT-Hydro'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'MERIT-Hydro', f\"{var_name}.csv\"))\n",
    "\n",
    "        ds.close()\n",
    "        del ds\n",
    "        gc.collect()\n",
    "\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "        try:\n",
    "            process(idx, row)\n",
    "        except Exception as e:\n",
    "            issues.append(f\"{var_name}-{row['huc_02']}-{row.name}\")\n",
    "            print(f\"Error: {var_name}-{row['huc_02']}-{row.name}. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_name</th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [var_name, huc_02, gauge_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df = [entry.split('-') for entry in issues]\n",
    "issues_df = pd.DataFrame(issues_df, columns = ['var_name', 'huc_02', 'gauge_id'])\n",
    "issues_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_name</th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [var_name, huc_02, gauge_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df[issues_df['var_name'] == 'elv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 147.56it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    # lon: -180 to 180; lat: -60 to 90\n",
    "    lon_transform = lambda x: np.sin(2 * np.pi * (x+180) / 360)\n",
    "    lat_transform = lambda x: (x - (-60))/(90 - (-60))\n",
    "\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = ['lon_transformed', 'lat_transformed'])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        data.loc['lon_transformed', node_idx] = lon_transform(lon)\n",
    "        data.loc['lat_transformed', node_idx] = lat_transform(lat)\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'spatial_encodings.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uparea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 609.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uparea = xr.open_dataset(os.path.join(PATHS['gis_ldd'], 'GloFAS_03min/upstream_area_km2.nc'))\n",
    "ds_varname = list(uparea.data_vars)[0]\n",
    "uparea = uparea[ds_varname]\n",
    "uparea = uparea.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "uparea.load()\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        data.loc[0, node_idx] = uparea.sel(lat = lat, lon = lon, method = 'nearest').values.item()\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'uparea.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "uparea.close()\n",
    "del uparea\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndiaWRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:02<00:00, 27.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    uparea = row['ghi_area']\n",
    "\n",
    "    streamflow = pd.read_csv(os.path.join(PATHS['CAMELS'], 'CAMELS-IND', 'CAMELS_IND_Catchments_Streamflow_Sufficient', 'streamflow_timeseries', 'streamflow_observed.csv'))\n",
    "    streamflow['date'] = pd.to_datetime(streamflow[['year', 'month', 'day']])\n",
    "    streamflow = streamflow.set_index('date')\n",
    "    streamflow = streamflow.drop(columns=['year', 'month', 'day'])\n",
    "    streamflow = streamflow[~((streamflow.index.month == 2) & (streamflow.index.day == 29))]\n",
    "    streamflow.columns = streamflow.columns.astype(str)\n",
    "    streamflow.columns = streamflow.columns.str.zfill(5)\n",
    "    streamflow = streamflow[[gauge_id]]\n",
    "    streamflow.columns = ['Q_m3s']\n",
    "    streamflow['Q_mm'] = (streamflow['Q_m3s'] / (uparea * 1e6)) * (3600*24*1000)\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    streamflow.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'IndiaWRIS.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloFAS Parameter Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanbnkf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 646.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanflpn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 732.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 879.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanlength\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 696.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 625.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 680.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanbw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 786.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracforest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 835.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracirrigated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 678.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracrice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 765.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracsealed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 675.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracwater\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 794.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracother\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 844.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"Catchment_morphology_and_river_network\" (14 surface fields)\n",
    "# - chanbnkf_Global_03min.nc (channel bankfull depth, m);\n",
    "# - chanflpn_Global_03min.nc (width of the floodplain, m);\n",
    "# - changrad_Global_03min.nc (channel longitudinal gradient, m/m);\n",
    "# - chanlength_Global_03min.nc (channel length within a pixel, m);\n",
    "# - chanman_Global_03min.nc (channel Manning's roughness coefficient, m^(1/3)s^(-1));\n",
    "# - chans_Global_03min.nc (channel side slope, m/m);\n",
    "# - chanbw_Global_03min.nc (channel bottom width, m):\n",
    "\n",
    "# \"Land_use\" (7 surface fields)\n",
    "# - fracforest_Global_03min.nc (fraction of forest for each grid-cell, -);\n",
    "# - fracirrigated_Global_03min.nc (fraction of irrigated crops [except rice] for each grid-cell, -);\n",
    "# - fracrice_Global_03min.nc (fraction of rice crops for each grid-cell, -);\n",
    "# - fracsealed_Global_03min.nc (fraction of urban area for each grid-cell, -);\n",
    "# - fracwater_Global_03min.nc (fraction of inland water for each grid-cell, -);\n",
    "# - fracother_Global_03min.nc (fraction of other land cover for each grid-cell, -);\n",
    "Parameter_Maps = os.path.join(PATHS['GloFAS'], 'LISFLOOD_Parameter_Maps')\n",
    "\n",
    "var_names = ['chanbnkf', 'chanflpn', 'changrad', 'chanlength', 'chanman', 'chans', 'chanbw']\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Catchments_morphology_and_river_network', f\"{var_name}_Global_03min.nc\"))['Band1']\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds.load()\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "        os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()\n",
    "\n",
    "var_names = ['fracforest', 'fracirrigated', 'fracrice', 'fracsealed', 'fracwater', 'fracother']\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Land_use', f\"{var_name}_Global_03min.nc\"))['Band1']\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds.load()\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "        os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:00<00:00, 832.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parameter_Maps = os.path.join(PATHS['GloFAS'], 'LISFLOOD_Parameter_Maps')\n",
    "ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Main', 'pixarea_Global_03min.nc'))['Band1'] / 1e6\n",
    "ds = ds.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "ds.load()\n",
    "var_name = 'cellarea_km2'\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "        data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "ds.close()\n",
    "del ds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloFAS Discharge in mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloFAS Runoff in m3s and mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
