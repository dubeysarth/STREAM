{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import networkx as nx\n",
    "\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import tqdm\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "cfg = configparser.ConfigParser()\n",
    "cfg.optionxform = str\n",
    "cfg.read('/home/sarth/rootdir/assets/global.ini')\n",
    "cfg = {s: dict(cfg.items(s)) for s in cfg.sections()}\n",
    "PATHS = cfg['PATHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME = '30min_CWatM_India'\n",
    "basin_name = 'mahanadi'\n",
    "SAVE_PATH = os.path.join(PATHS['devp_datasets'], DIRNAME)\n",
    "resolution = 0.50\n",
    "lon_360_180 = lambda x: (x + 180) % 360 - 180 # convert 0-360 to -180-180\n",
    "lon_180_360 = lambda x: x % 360 # convert -180-180 to 0-360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = gpd.read_file(os.path.join(PATHS['gis_shapefiles'], 'asia', basin_name, 'shapefile.shp'), crs = 'epsg:4326')\n",
    "minx, miny, maxx, maxy = basins.total_bounds\n",
    "buffer = 0.5\n",
    "region_bounds = {\n",
    "    'minx': minx - buffer,\n",
    "    'miny': miny - buffer,\n",
    "    'maxx': maxx + buffer,\n",
    "    'maxy': maxy + buffer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Watershed Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>River Point Name</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>NaNs</th>\n",
       "      <th>Days with Observations</th>\n",
       "      <th>Zero of Gauge</th>\n",
       "      <th>MERIT_Longitude</th>\n",
       "      <th>MERIT_Latitude</th>\n",
       "      <th>Gauge_MERIT_Distance</th>\n",
       "      <th>uparea (in m2)</th>\n",
       "      <th>uparea (in km2)</th>\n",
       "      <th>snapped_lon</th>\n",
       "      <th>snapped_lat</th>\n",
       "      <th>snapped_uparea</th>\n",
       "      <th>snapped_iou</th>\n",
       "      <th>area_percent_difference</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_edges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>andhiyar_khore</td>\n",
       "      <td>21.833889</td>\n",
       "      <td>81.597500</td>\n",
       "      <td>5.30</td>\n",
       "      <td>14184</td>\n",
       "      <td>252.0</td>\n",
       "      <td>81.597500</td>\n",
       "      <td>21.834167</td>\n",
       "      <td>30.887</td>\n",
       "      <td>2.166368e+09</td>\n",
       "      <td>2166.368402</td>\n",
       "      <td>81.25</td>\n",
       "      <td>22.25</td>\n",
       "      <td>2853.6025</td>\n",
       "      <td>0.512368</td>\n",
       "      <td>31.722864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bamnidhi</td>\n",
       "      <td>21.908486</td>\n",
       "      <td>82.713640</td>\n",
       "      <td>4.45</td>\n",
       "      <td>14311</td>\n",
       "      <td>223.0</td>\n",
       "      <td>82.710000</td>\n",
       "      <td>21.910000</td>\n",
       "      <td>411.522</td>\n",
       "      <td>9.811223e+09</td>\n",
       "      <td>9811.223484</td>\n",
       "      <td>82.75</td>\n",
       "      <td>22.25</td>\n",
       "      <td>11373.9510</td>\n",
       "      <td>0.639430</td>\n",
       "      <td>15.927957</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baronda</td>\n",
       "      <td>20.910000</td>\n",
       "      <td>81.888056</td>\n",
       "      <td>6.64</td>\n",
       "      <td>13983</td>\n",
       "      <td>283.0</td>\n",
       "      <td>81.888333</td>\n",
       "      <td>20.910000</td>\n",
       "      <td>28.853</td>\n",
       "      <td>3.193768e+09</td>\n",
       "      <td>3193.768107</td>\n",
       "      <td>82.25</td>\n",
       "      <td>20.25</td>\n",
       "      <td>2891.6934</td>\n",
       "      <td>0.362836</td>\n",
       "      <td>9.458254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basantpur</td>\n",
       "      <td>21.738500</td>\n",
       "      <td>82.785942</td>\n",
       "      <td>4.64</td>\n",
       "      <td>14282</td>\n",
       "      <td>206.0</td>\n",
       "      <td>82.778333</td>\n",
       "      <td>21.722500</td>\n",
       "      <td>1944.963</td>\n",
       "      <td>5.898489e+10</td>\n",
       "      <td>58984.885999</td>\n",
       "      <td>82.75</td>\n",
       "      <td>21.75</td>\n",
       "      <td>65959.0100</td>\n",
       "      <td>0.798565</td>\n",
       "      <td>11.823573</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ghatora</td>\n",
       "      <td>22.048592</td>\n",
       "      <td>82.221956</td>\n",
       "      <td>5.35</td>\n",
       "      <td>14176</td>\n",
       "      <td>246.0</td>\n",
       "      <td>82.222500</td>\n",
       "      <td>22.047500</td>\n",
       "      <td>133.739</td>\n",
       "      <td>3.037212e+09</td>\n",
       "      <td>3037.211720</td>\n",
       "      <td>82.25</td>\n",
       "      <td>22.25</td>\n",
       "      <td>2853.6025</td>\n",
       "      <td>0.377117</td>\n",
       "      <td>6.045319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jondhra</td>\n",
       "      <td>21.712492</td>\n",
       "      <td>82.333106</td>\n",
       "      <td>5.66</td>\n",
       "      <td>14130</td>\n",
       "      <td>219.0</td>\n",
       "      <td>82.332500</td>\n",
       "      <td>21.715000</td>\n",
       "      <td>285.855</td>\n",
       "      <td>2.979901e+10</td>\n",
       "      <td>29799.010658</td>\n",
       "      <td>81.75</td>\n",
       "      <td>21.75</td>\n",
       "      <td>22964.7300</td>\n",
       "      <td>0.615867</td>\n",
       "      <td>22.934586</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kantamal</td>\n",
       "      <td>20.658333</td>\n",
       "      <td>83.732069</td>\n",
       "      <td>5.28</td>\n",
       "      <td>14186</td>\n",
       "      <td>118.0</td>\n",
       "      <td>83.730000</td>\n",
       "      <td>20.660833</td>\n",
       "      <td>351.621</td>\n",
       "      <td>2.026583e+10</td>\n",
       "      <td>20265.829359</td>\n",
       "      <td>83.25</td>\n",
       "      <td>20.25</td>\n",
       "      <td>14485.4110</td>\n",
       "      <td>0.501618</td>\n",
       "      <td>28.522980</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kesinga</td>\n",
       "      <td>20.285831</td>\n",
       "      <td>83.221333</td>\n",
       "      <td>5.58</td>\n",
       "      <td>14142</td>\n",
       "      <td>166.0</td>\n",
       "      <td>83.249167</td>\n",
       "      <td>20.262500</td>\n",
       "      <td>3893.397</td>\n",
       "      <td>1.197787e+10</td>\n",
       "      <td>11977.870300</td>\n",
       "      <td>83.25</td>\n",
       "      <td>20.25</td>\n",
       "      <td>14485.4110</td>\n",
       "      <td>0.570078</td>\n",
       "      <td>20.934782</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kotni</td>\n",
       "      <td>21.240000</td>\n",
       "      <td>81.250278</td>\n",
       "      <td>4.57</td>\n",
       "      <td>14293</td>\n",
       "      <td>268.0</td>\n",
       "      <td>81.250833</td>\n",
       "      <td>21.236667</td>\n",
       "      <td>375.095</td>\n",
       "      <td>7.036693e+09</td>\n",
       "      <td>7036.693190</td>\n",
       "      <td>81.25</td>\n",
       "      <td>21.25</td>\n",
       "      <td>11511.1490</td>\n",
       "      <td>0.550663</td>\n",
       "      <td>63.587482</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kurubhata</td>\n",
       "      <td>21.981278</td>\n",
       "      <td>83.210600</td>\n",
       "      <td>12.18</td>\n",
       "      <td>13153</td>\n",
       "      <td>215.0</td>\n",
       "      <td>83.211667</td>\n",
       "      <td>21.980833</td>\n",
       "      <td>120.579</td>\n",
       "      <td>4.752530e+09</td>\n",
       "      <td>4752.530476</td>\n",
       "      <td>83.25</td>\n",
       "      <td>22.25</td>\n",
       "      <td>5697.1436</td>\n",
       "      <td>0.599300</td>\n",
       "      <td>19.876007</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>manendragarh</td>\n",
       "      <td>23.205361</td>\n",
       "      <td>82.216401</td>\n",
       "      <td>28.55</td>\n",
       "      <td>10701</td>\n",
       "      <td>411.0</td>\n",
       "      <td>82.216667</td>\n",
       "      <td>23.204167</td>\n",
       "      <td>135.561</td>\n",
       "      <td>1.023368e+09</td>\n",
       "      <td>1023.367874</td>\n",
       "      <td>82.25</td>\n",
       "      <td>23.25</td>\n",
       "      <td>2833.2659</td>\n",
       "      <td>0.294179</td>\n",
       "      <td>176.857030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pathardihi</td>\n",
       "      <td>21.340803</td>\n",
       "      <td>81.594989</td>\n",
       "      <td>29.03</td>\n",
       "      <td>10630</td>\n",
       "      <td>271.0</td>\n",
       "      <td>81.594167</td>\n",
       "      <td>21.341667</td>\n",
       "      <td>128.372</td>\n",
       "      <td>2.490334e+09</td>\n",
       "      <td>2490.333890</td>\n",
       "      <td>81.75</td>\n",
       "      <td>21.25</td>\n",
       "      <td>2873.0798</td>\n",
       "      <td>0.169220</td>\n",
       "      <td>15.369257</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rajim</td>\n",
       "      <td>20.975078</td>\n",
       "      <td>81.881039</td>\n",
       "      <td>14.19</td>\n",
       "      <td>12852</td>\n",
       "      <td>275.0</td>\n",
       "      <td>81.878333</td>\n",
       "      <td>20.977500</td>\n",
       "      <td>389.168</td>\n",
       "      <td>8.412393e+09</td>\n",
       "      <td>8412.392615</td>\n",
       "      <td>81.75</td>\n",
       "      <td>20.75</td>\n",
       "      <td>8665.8820</td>\n",
       "      <td>0.491331</td>\n",
       "      <td>3.013284</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rampur</td>\n",
       "      <td>21.647192</td>\n",
       "      <td>82.516164</td>\n",
       "      <td>12.31</td>\n",
       "      <td>13134</td>\n",
       "      <td>219.0</td>\n",
       "      <td>82.515000</td>\n",
       "      <td>21.648333</td>\n",
       "      <td>174.887</td>\n",
       "      <td>3.405829e+09</td>\n",
       "      <td>3405.829451</td>\n",
       "      <td>82.75</td>\n",
       "      <td>21.25</td>\n",
       "      <td>2873.0798</td>\n",
       "      <td>0.284886</td>\n",
       "      <td>15.642283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>salebhata</td>\n",
       "      <td>20.983333</td>\n",
       "      <td>83.539444</td>\n",
       "      <td>5.19</td>\n",
       "      <td>14200</td>\n",
       "      <td>130.0</td>\n",
       "      <td>83.540833</td>\n",
       "      <td>20.981667</td>\n",
       "      <td>234.815</td>\n",
       "      <td>4.598711e+09</td>\n",
       "      <td>4598.710788</td>\n",
       "      <td>83.25</td>\n",
       "      <td>20.75</td>\n",
       "      <td>8638.0690</td>\n",
       "      <td>0.320302</td>\n",
       "      <td>87.836754</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>seorinarayan</td>\n",
       "      <td>21.718535</td>\n",
       "      <td>82.597543</td>\n",
       "      <td>18.80</td>\n",
       "      <td>12161</td>\n",
       "      <td>209.5</td>\n",
       "      <td>82.600000</td>\n",
       "      <td>21.715833</td>\n",
       "      <td>393.276</td>\n",
       "      <td>4.792836e+10</td>\n",
       "      <td>47928.363884</td>\n",
       "      <td>82.25</td>\n",
       "      <td>21.75</td>\n",
       "      <td>48848.5350</td>\n",
       "      <td>0.811889</td>\n",
       "      <td>1.919890</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>simga</td>\n",
       "      <td>21.633081</td>\n",
       "      <td>81.684869</td>\n",
       "      <td>4.84</td>\n",
       "      <td>14252</td>\n",
       "      <td>244.0</td>\n",
       "      <td>81.684167</td>\n",
       "      <td>21.632500</td>\n",
       "      <td>97.181</td>\n",
       "      <td>1.688621e+10</td>\n",
       "      <td>16886.210359</td>\n",
       "      <td>81.25</td>\n",
       "      <td>21.75</td>\n",
       "      <td>14374.5990</td>\n",
       "      <td>0.662833</td>\n",
       "      <td>14.873748</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sundergarh</td>\n",
       "      <td>22.109722</td>\n",
       "      <td>84.030000</td>\n",
       "      <td>5.34</td>\n",
       "      <td>14178</td>\n",
       "      <td>214.0</td>\n",
       "      <td>84.020833</td>\n",
       "      <td>22.102500</td>\n",
       "      <td>1239.653</td>\n",
       "      <td>5.917222e+09</td>\n",
       "      <td>5917.221758</td>\n",
       "      <td>83.75</td>\n",
       "      <td>22.25</td>\n",
       "      <td>8550.7460</td>\n",
       "      <td>0.518569</td>\n",
       "      <td>44.506096</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   River Point Name   Latitude  Longitude   NaNs  Days with Observations  \\\n",
       "0    andhiyar_khore  21.833889  81.597500   5.30                   14184   \n",
       "1          bamnidhi  21.908486  82.713640   4.45                   14311   \n",
       "2           baronda  20.910000  81.888056   6.64                   13983   \n",
       "3         basantpur  21.738500  82.785942   4.64                   14282   \n",
       "4           ghatora  22.048592  82.221956   5.35                   14176   \n",
       "5           jondhra  21.712492  82.333106   5.66                   14130   \n",
       "6          kantamal  20.658333  83.732069   5.28                   14186   \n",
       "7           kesinga  20.285831  83.221333   5.58                   14142   \n",
       "8             kotni  21.240000  81.250278   4.57                   14293   \n",
       "9         kurubhata  21.981278  83.210600  12.18                   13153   \n",
       "10     manendragarh  23.205361  82.216401  28.55                   10701   \n",
       "11       pathardihi  21.340803  81.594989  29.03                   10630   \n",
       "12            rajim  20.975078  81.881039  14.19                   12852   \n",
       "13           rampur  21.647192  82.516164  12.31                   13134   \n",
       "14        salebhata  20.983333  83.539444   5.19                   14200   \n",
       "15     seorinarayan  21.718535  82.597543  18.80                   12161   \n",
       "16            simga  21.633081  81.684869   4.84                   14252   \n",
       "17       sundergarh  22.109722  84.030000   5.34                   14178   \n",
       "\n",
       "    Zero of Gauge  MERIT_Longitude  MERIT_Latitude  Gauge_MERIT_Distance  \\\n",
       "0           252.0        81.597500       21.834167                30.887   \n",
       "1           223.0        82.710000       21.910000               411.522   \n",
       "2           283.0        81.888333       20.910000                28.853   \n",
       "3           206.0        82.778333       21.722500              1944.963   \n",
       "4           246.0        82.222500       22.047500               133.739   \n",
       "5           219.0        82.332500       21.715000               285.855   \n",
       "6           118.0        83.730000       20.660833               351.621   \n",
       "7           166.0        83.249167       20.262500              3893.397   \n",
       "8           268.0        81.250833       21.236667               375.095   \n",
       "9           215.0        83.211667       21.980833               120.579   \n",
       "10          411.0        82.216667       23.204167               135.561   \n",
       "11          271.0        81.594167       21.341667               128.372   \n",
       "12          275.0        81.878333       20.977500               389.168   \n",
       "13          219.0        82.515000       21.648333               174.887   \n",
       "14          130.0        83.540833       20.981667               234.815   \n",
       "15          209.5        82.600000       21.715833               393.276   \n",
       "16          244.0        81.684167       21.632500                97.181   \n",
       "17          214.0        84.020833       22.102500              1239.653   \n",
       "\n",
       "    uparea (in m2)  uparea (in km2)  snapped_lon  snapped_lat  snapped_uparea  \\\n",
       "0     2.166368e+09      2166.368402        81.25        22.25       2853.6025   \n",
       "1     9.811223e+09      9811.223484        82.75        22.25      11373.9510   \n",
       "2     3.193768e+09      3193.768107        82.25        20.25       2891.6934   \n",
       "3     5.898489e+10     58984.885999        82.75        21.75      65959.0100   \n",
       "4     3.037212e+09      3037.211720        82.25        22.25       2853.6025   \n",
       "5     2.979901e+10     29799.010658        81.75        21.75      22964.7300   \n",
       "6     2.026583e+10     20265.829359        83.25        20.25      14485.4110   \n",
       "7     1.197787e+10     11977.870300        83.25        20.25      14485.4110   \n",
       "8     7.036693e+09      7036.693190        81.25        21.25      11511.1490   \n",
       "9     4.752530e+09      4752.530476        83.25        22.25       5697.1436   \n",
       "10    1.023368e+09      1023.367874        82.25        23.25       2833.2659   \n",
       "11    2.490334e+09      2490.333890        81.75        21.25       2873.0798   \n",
       "12    8.412393e+09      8412.392615        81.75        20.75       8665.8820   \n",
       "13    3.405829e+09      3405.829451        82.75        21.25       2873.0798   \n",
       "14    4.598711e+09      4598.710788        83.25        20.75       8638.0690   \n",
       "15    4.792836e+10     47928.363884        82.25        21.75      48848.5350   \n",
       "16    1.688621e+10     16886.210359        81.25        21.75      14374.5990   \n",
       "17    5.917222e+09      5917.221758        83.75        22.25       8550.7460   \n",
       "\n",
       "    snapped_iou  area_percent_difference  num_nodes  num_edges  \n",
       "0      0.512368                31.722864        1.0        0.0  \n",
       "1      0.639430                15.927957        4.0        3.0  \n",
       "2      0.362836                 9.458254        1.0        0.0  \n",
       "3      0.798565                11.823573       23.0       22.0  \n",
       "4      0.377117                 6.045319        1.0        0.0  \n",
       "5      0.615867                22.934586        8.0        7.0  \n",
       "6      0.501618                28.522980        5.0        4.0  \n",
       "7      0.570078                20.934782        5.0        4.0  \n",
       "8      0.550663                63.587482        4.0        3.0  \n",
       "9      0.599300                19.876007        2.0        1.0  \n",
       "10     0.294179               176.857030        1.0        0.0  \n",
       "11     0.169220                15.369257        1.0        0.0  \n",
       "12     0.491331                 3.013284        3.0        2.0  \n",
       "13     0.284886                15.642283        1.0        0.0  \n",
       "14     0.320302                87.836754        3.0        2.0  \n",
       "15     0.811889                 1.919890       17.0       16.0  \n",
       "16     0.662833                14.873748        5.0        4.0  \n",
       "17     0.518569                44.506096        3.0        2.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indiawris_attributes_graph = pd.read_csv(os.path.join(SAVE_PATH, f'graph_attributes_{basin_name}.csv'), index_col = 0)\n",
    "indiawris_attributes_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 18)\n",
      "(2, 18)\n"
     ]
    }
   ],
   "source": [
    "indiawris_graph = indiawris_attributes_graph.copy()\n",
    "indiawris_graph = indiawris_graph[indiawris_graph['area_percent_difference'] < 15]\n",
    "print(indiawris_graph.shape)\n",
    "indiawris_graph = indiawris_graph[indiawris_graph['num_nodes'] > 5]\n",
    "print(indiawris_graph.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        2.000000\n",
       "mean     53456.624942\n",
       "std       7818.141764\n",
       "min      47928.363884\n",
       "25%      50692.494413\n",
       "50%      53456.624942\n",
       "75%      56220.755470\n",
       "max      58984.885999\n",
       "Name: uparea (in km2), dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indiawris_graph['uparea (in km2)'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>River Point Name</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>NaNs</th>\n",
       "      <th>Days with Observations</th>\n",
       "      <th>Zero of Gauge</th>\n",
       "      <th>MERIT_Longitude</th>\n",
       "      <th>MERIT_Latitude</th>\n",
       "      <th>Gauge_MERIT_Distance</th>\n",
       "      <th>uparea (in m2)</th>\n",
       "      <th>uparea (in km2)</th>\n",
       "      <th>snapped_lon</th>\n",
       "      <th>snapped_lat</th>\n",
       "      <th>snapped_uparea</th>\n",
       "      <th>snapped_iou</th>\n",
       "      <th>area_percent_difference</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_edges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>basantpur</td>\n",
       "      <td>21.738500</td>\n",
       "      <td>82.785942</td>\n",
       "      <td>4.64</td>\n",
       "      <td>14282</td>\n",
       "      <td>206.0</td>\n",
       "      <td>82.778333</td>\n",
       "      <td>21.722500</td>\n",
       "      <td>1944.963</td>\n",
       "      <td>5.898489e+10</td>\n",
       "      <td>58984.885999</td>\n",
       "      <td>82.75</td>\n",
       "      <td>21.75</td>\n",
       "      <td>65959.010</td>\n",
       "      <td>0.798565</td>\n",
       "      <td>11.823573</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seorinarayan</td>\n",
       "      <td>21.718535</td>\n",
       "      <td>82.597543</td>\n",
       "      <td>18.80</td>\n",
       "      <td>12161</td>\n",
       "      <td>209.5</td>\n",
       "      <td>82.600000</td>\n",
       "      <td>21.715833</td>\n",
       "      <td>393.276</td>\n",
       "      <td>4.792836e+10</td>\n",
       "      <td>47928.363884</td>\n",
       "      <td>82.25</td>\n",
       "      <td>21.75</td>\n",
       "      <td>48848.535</td>\n",
       "      <td>0.811889</td>\n",
       "      <td>1.919890</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  River Point Name   Latitude  Longitude   NaNs  Days with Observations  \\\n",
       "0        basantpur  21.738500  82.785942   4.64                   14282   \n",
       "1     seorinarayan  21.718535  82.597543  18.80                   12161   \n",
       "\n",
       "   Zero of Gauge  MERIT_Longitude  MERIT_Latitude  Gauge_MERIT_Distance  \\\n",
       "0          206.0        82.778333       21.722500              1944.963   \n",
       "1          209.5        82.600000       21.715833               393.276   \n",
       "\n",
       "   uparea (in m2)  uparea (in km2)  snapped_lon  snapped_lat  snapped_uparea  \\\n",
       "0    5.898489e+10     58984.885999        82.75        21.75       65959.010   \n",
       "1    4.792836e+10     47928.363884        82.25        21.75       48848.535   \n",
       "\n",
       "   snapped_iou  area_percent_difference  num_nodes  num_edges  \n",
       "0     0.798565                11.823573       23.0       22.0  \n",
       "1     0.811889                 1.919890       17.0       16.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indiawris_graph = indiawris_graph.reset_index(drop = True)\n",
    "indiawris_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del indiawris_attributes_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Node Features as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(SAVE_PATH, \"graph_features\"), exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd = xr.open_dataset(os.path.join(PATHS['gis_ldd'], 'CWatM_30min', 'ldd.nc'))\n",
    "ldd = ldd['ldd']\n",
    "ldd = ldd.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "\n",
    "lons = ldd['lon'].values\n",
    "lats = ldd['lat'].values\n",
    "\n",
    "ds_grid = xr.Dataset({\n",
    "    'lat': (['lat'], lats),\n",
    "    'lon': (['lon'], lons),\n",
    "})\n",
    "\n",
    "# Round the lat lon values to 3 decimal places in ds_grid\n",
    "ds_grid['lat'] = ds_grid['lat'].round(3)\n",
    "ds_grid['lon'] = ds_grid['lon'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regridder_files = {\n",
    "    'ERA5': f'regridder_era5_to_cwatm_30min_India_{basin_name}.nc',\n",
    "    'GLEAM': f'regridder_gleam_to_cwatm_30min_India_{basin_name}.nc',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERA5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "evaporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1796.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowfall\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2654.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_net_solar_radiation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1832.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_net_thermal_radiation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1527.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_pressure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1775.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m_dewpoint_temperature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1833.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m_u_component_of_wind\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1709.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m_v_component_of_wind\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1792.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forecast_albedo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1859.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential_evaporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1590.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow_albedo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1510.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow_depth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1557.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowmelt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1612.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_column_water\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1844.06it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = [\n",
    "    # 'sub_surface_runoff',\n",
    "    # 'surface_runoff',\n",
    "    # 'total_precipitation',\n",
    "    # '2m_temperature',\n",
    "    # 'volumetric_soil_water_layer_1',\n",
    "    # 'volumetric_soil_water_layer_2',\n",
    "    # 'volumetric_soil_water_layer_3',\n",
    "    # 'volumetric_soil_water_layer_4',\n",
    "    # 'runoff',\n",
    "    'evaporation', \n",
    "    'snowfall', \n",
    "    'surface_net_solar_radiation', \n",
    "    'surface_net_thermal_radiation', \n",
    "    'surface_pressure', \n",
    "    '2m_dewpoint_temperature',\n",
    "    '10m_u_component_of_wind',\n",
    "    '10m_v_component_of_wind',\n",
    "    'forecast_albedo',\n",
    "    'potential_evaporation',\n",
    "    'snow_albedo',\n",
    "    'snow_depth',\n",
    "    'snowmelt',\n",
    "    'total_column_water',\n",
    "]\n",
    "\n",
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = basin_name, row['River Point Name']\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaporation\n",
      "Time: 6.5852 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1091.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowfall\n",
      "Time: 5.0478 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1093.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_net_solar_radiation\n",
      "Time: 5.3348 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1028.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_net_thermal_radiation\n",
      "Time: 4.8022 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 839.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_pressure\n",
      "Time: 6.4313 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1061.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m_dewpoint_temperature\n",
      "Time: 4.6667 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 956.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m_u_component_of_wind\n",
      "Time: 4.7914 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 760.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m_v_component_of_wind\n",
      "Time: 4.5801 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 666.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forecast_albedo\n",
      "Time: 4.6053 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 920.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "potential_evaporation\n",
      "Time: 4.5425 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 830.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow_albedo\n",
      "Time: 4.6815 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 3003.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snow_depth\n",
      "Time: 4.5222 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1407.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snowmelt\n",
      "Time: 4.6015 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1423.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_column_water\n",
      "Time: 4.5999 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 591.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['RawData'], 'ERA5', var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds['lon'] = [lon_360_180(lon) for lon in ds['lon'].values]\n",
    "    ds = ds.sortby('lon')\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    _, index = np.unique(ds['time'], return_index = True)\n",
    "    ds = ds.isel(time = index)\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], regridder_files['ERA5'])):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], regridder_files['ERA5'])\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], regridder_files['ERA5']))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'Time: {((end_time - start_time) / 60):.4f} mins')\n",
    "    \n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = basin_name, row['River Point Name']\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds_regrided.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_soil_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 12.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_high_vegetation_cover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_low_vegetation_cover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_type_of_high_vegetation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 24.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_type_of_low_vegetation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 35.82it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = [\n",
    "    'static_soil_type', \n",
    "    'static_high_vegetation_cover', \n",
    "    'static_low_vegetation_cover', \n",
    "    'static_type_of_high_vegetation', \n",
    "    'static_type_of_low_vegetation'\n",
    "    ]\n",
    "ds_filenames = [\n",
    "    'soil_type_static.nc',\n",
    "    'high_vegetation_cover_static.nc',\n",
    "    'low_vegetation_cover_static.nc',\n",
    "    'type_of_high_vegetation_static.nc',\n",
    "    'type_of_low_vegetation_static.nc'\n",
    "]\n",
    "\n",
    "for var_name, ds_filename in zip(var_names, ds_filenames):\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(PATHS['RawData'], 'ERA5', ds_filename))\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.isel(time = 0)\n",
    "    ds = ds.drop('time')\n",
    "    ds = ds.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
    "    ds['lon'] = [lon_360_180(lon) for lon in ds['lon'].values]\n",
    "    ds = ds.sortby('lon')\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = basin_name, row['River Point Name']\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[0, node_idx] = int(ds_window_loc.values)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'ERA5'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "    for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)):\n",
    "        process(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HWSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_CLAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 15.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_GRAVEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [00:00<00:00, 90.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_SAND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 110.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_SILT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 106.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_CLAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 107.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_GRAVEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 113.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_SAND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 108.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_SILT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 104.10it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['S_CLAY', 'S_GRAVEL', 'S_SAND', 'S_SILT', 'T_CLAY', 'T_GRAVEL', 'T_SAND', 'T_SILT']\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(PATHS['HWSD'], f'{var_name}.nc4'))\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['miny'], region_bounds['maxy']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds = ds / 100\n",
    "    ds.load()\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = basin_name, row['River Point Name']\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(\n",
    "                lat = slice(lat-resolution/2, lat+resolution/2),\n",
    "                lon = slice(lon-resolution/2, lon+resolution/2)\n",
    "            ).values.mean()\n",
    "            data.loc[0, node_idx] = ds_window_loc\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'HWSD'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'HWSD', f\"{var_name}.csv\"))\n",
    "\n",
    "    for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)):\n",
    "        process(idx, row)\n",
    "    \n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "Ep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4826.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMroot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2142.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMsurf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2494.38it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "\n",
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = basin_name, row['River Point Name']\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep\n",
      "Time: 0.3355 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1780.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMroot\n",
      "Time: 0.3308 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1084.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMsurf\n",
      "Time: 0.3247 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 1236.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], regridder_files['GLEAM'])):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], regridder_files['GLEAM'])\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], regridder_files['GLEAM']))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'Time: {((end_time - start_time) / 60):.4f} mins')\n",
    "    \n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = basin_name, row['River Point Name']\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds_regrided.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep (Time: 0.3222 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 13.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMroot (Time: 0.3133 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 13.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMsurf (Time: 0.3121 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 12.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "for var_name in var_names:\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], regridder_files['GLEAM'])):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], regridder_files['GLEAM'])\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], regridder_files['GLEAM']))\n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'{var_name} (Time: {((end_time - start_time) / 60):.4f} mins)')\n",
    "\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(indiawris_graph.iterrows()):\n",
    "        huc, gauge_id = basin_name, row['River Point Name']\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "                node_lat = float(round(nodes_coords.loc[node_idx, 'lat'], 3))\n",
    "                node_lon = float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "                multiplier = 1.5\n",
    "                ds_slice = ds_regrided.sel(\n",
    "                    lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                    lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                    )\n",
    "                slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "                slice_df['lat'] = slice_df['lat'].round(3)\n",
    "                slice_df['lon'] = slice_df['lon'].round(3)\n",
    "                slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "                slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "                num_nan_nodes = slice_df.isnull().any(axis=0).sum()\n",
    "                num_nonnan_nodes = len(slice_df.columns) - num_nan_nodes\n",
    "                nodes_coords.loc[node_idx, 'nonNaNneighbours'] = num_nonnan_nodes\n",
    "        nodes_coords_sorted = nodes_coords.sort_values(by = 'nonNaNneighbours', ascending = False)\n",
    "        nodes_coords_sorted = nodes_coords_sorted[nodes_coords_sorted['isNaN']]\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted.shape[0]}\")\n",
    "        \n",
    "        for node_idx in tqdm.tqdm(nodes_coords_sorted.index):\n",
    "            node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "            multiplier = 1.5\n",
    "            ds_slice = ds_regrided.sel(\n",
    "                lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                )\n",
    "            slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "            slice_df['lat'] = slice_df['lat'].round(3)\n",
    "            slice_df['lon'] = slice_df['lon'].round(3)\n",
    "            slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "            slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "            slice_df.columns = list(map(str, slice_df.columns))\n",
    "            num_nonnan_nodes = len(slice_df.columns) - slice_df.isnull().any(axis=0).sum()\n",
    "            # print(node_idx, (node_lat, node_lon), num_nonnan_nodes)\n",
    "            if num_nonnan_nodes == 9:\n",
    "                replacement_values = slice_df.loc[:, f\"({node_lat}, {node_lon})\"]\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "            elif num_nonnan_nodes > 0:\n",
    "                replacement_values = np.nanmean(slice_df, axis = 1)\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                ds_regrided.loc[dict(lat = node_lat, lon = node_lon)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted['isNaN'].sum()}\")\n",
    "        print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "# for var_name in var_names:\n",
    "#     # Loop over catchments and find ones with issues\n",
    "#     issues = []\n",
    "#     for idx, row in tqdm.tqdm(indiawris_graph.iterrows()):\n",
    "#         huc, gauge_id = basin_name, row['River Point Name']\n",
    "#         nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "#         data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "#         if data.isnull().values.any():\n",
    "#             issues.append([huc, gauge_id])\n",
    "#     issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "#     print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "#     print(\"------\")\n",
    "\n",
    "#     # Fix the catchments with issues\n",
    "#     for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "#         print(issue_idx, huc, gauge_id)\n",
    "#         nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "#         data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "#         nodes_coords['isNaN'] = False\n",
    "#         nodes_coords['nonNaNneighbours'] = 0\n",
    "#         # Loop over nodes and find the nodes with issues\n",
    "#         for node_idx in nodes_coords.index:\n",
    "#             if data[str(node_idx)].isnull().values.any():\n",
    "#                 nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "#         print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "#         print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "# for var_name in var_names:\n",
    "#     # Loop over catchments and find ones with issues\n",
    "#     issues = []\n",
    "#     for idx, row in tqdm.tqdm(indiawris_graph.iterrows()):\n",
    "#         huc, gauge_id = basin_name, row['River Point Name']\n",
    "#         nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "#         data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "#         if data.isnull().values.any():\n",
    "#             issues.append([huc, gauge_id])\n",
    "#     issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "#     print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "#     print(\"------\")\n",
    "\n",
    "#     # Fix the catchments with issues\n",
    "#     for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "#         print(issue_idx, huc, gauge_id)\n",
    "#         nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "#         data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "#         nodes_coords['isNaN'] = False\n",
    "#         # Loop over nodes and find the nodes with issues\n",
    "#         for node_idx in nodes_coords.index:\n",
    "#             if data[str(node_idx)].isnull().values.any():\n",
    "#                 nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "#         print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "\n",
    "        \n",
    "#         for node_idx in tqdm.tqdm(nodes_coords[nodes_coords['isNaN']].index):\n",
    "#             nodes_coords['distances'] = None\n",
    "#             node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "#             for node_idx2 in nodes_coords[nodes_coords['isNaN'] == False].index:\n",
    "#                 if node_idx != node_idx2:\n",
    "#                     node_lat2, node_lon2 = float(round(nodes_coords.loc[node_idx2, 'lat'], 3)), float(round(nodes_coords.loc[node_idx2, 'lon'], 3))\n",
    "#                     distance = np.sqrt((node_lat - node_lat2)**2 + (node_lon - node_lon2)**2)\n",
    "#                     nodes_coords.loc[node_idx2, 'distances'] = distance\n",
    "#             min_distance = nodes_coords.loc[nodes_coords['distances'].idxmin(), 'distances']\n",
    "#             # Replace with mean of nodes having distance equal to min_distance\n",
    "#             replacement_nodes = nodes_coords[nodes_coords['distances'] == min_distance].index\n",
    "#             replacement_nodes = list(map(str, replacement_nodes))\n",
    "#             replacement_values = data.loc[:, replacement_nodes].mean(axis = 1)\n",
    "#             data.loc[:, str(node_idx)] = replacement_values\n",
    "#             nodes_coords.loc[node_idx, 'isNaN'] = False\n",
    "#         print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "#         print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "#         data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "#         print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solar Insolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solar_insolation(lat, lon, start_date, end_date):\n",
    "    # Constants\n",
    "    Sc = 1361  # Solar constant (W/m^2)\n",
    "    \n",
    "    # Convert dates to datetime objects\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Generate date range\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "    \n",
    "    # Function to calculate solar declination\n",
    "    def solar_declination(n):\n",
    "        return 23.45 * np.sin(np.radians((360 / 365) * (n - 81)))\n",
    "\n",
    "    # Function to calculate cos(theta_z) for solar zenith angle\n",
    "    def cos_theta_z(lat, decl, hour_angle):\n",
    "        lat_rad = np.radians(lat)\n",
    "        decl_rad = np.radians(decl)\n",
    "        return (np.sin(lat_rad) * np.sin(decl_rad) + \n",
    "                np.cos(lat_rad) * np.cos(decl_rad) * np.cos(np.radians(hour_angle)))\n",
    "    \n",
    "    # Function to calculate the hour angle\n",
    "    def hour_angle(lon, date):\n",
    "        # Assuming solar noon (local solar time = 12 hours)\n",
    "        return 0  # hour angle at solar noon\n",
    "    \n",
    "    # Calculate solar insolation for each day\n",
    "    insolation_values = []\n",
    "    for date in dates:\n",
    "        day_of_year = date.day_of_year\n",
    "        declination = solar_declination(day_of_year)\n",
    "        h = hour_angle(lon, date)\n",
    "        cos_zenith_angle = cos_theta_z(lat, declination, h)\n",
    "        \n",
    "        # Insolation formula\n",
    "        insolation = Sc * (1 + 0.033 * np.cos(np.radians(360 * day_of_year / 365))) * cos_zenith_angle\n",
    "        \n",
    "        # Make sure insolation is non-negative\n",
    "        insolation = max(insolation, 0)\n",
    "        insolation_values.append(insolation)\n",
    "    \n",
    "    # Create pandas Series\n",
    "    insolation_series = pd.Series(insolation_values, index=dates, name='Solar Insolation (kW/m²)')\n",
    "    insolation_series = insolation_series / 1000  # Convert to kW/m²\n",
    "    \n",
    "    return insolation_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2680.07it/s]\n"
     ]
    }
   ],
   "source": [
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = basin_name, row['River Point Name']\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = dates)\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        ds_window_loc = solar_insolation(lat, lon, '1980-01-01', '2020-12-31')\n",
    "        data.loc[:, node_idx] = ds_window_loc.values\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'solar_insolation.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)))\n",
    "\n",
    "# for idx, row in indiawris_graph.iterrows():\n",
    "#     process(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_time_encoding(start_date, end_date):\n",
    "    # (a) Create a date_range and remove leap days\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    dates = dates[~((dates.month == 2) & (dates.day == 29))]  # Remove February 29 (leap days)\n",
    "    \n",
    "    # (b) Create a dataframe with 'month', 'weekofyear', 'dayofyear' columns\n",
    "    df = pd.DataFrame(index=dates)\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    \n",
    "    # (c) Define lambda transformations for sine encoding\n",
    "    # For day of year (range 1-365), week of year (range 1-52), and month (range 1-12)\n",
    "    sine_transform = lambda x, max_val: np.sin(2 * np.pi * x / max_val)\n",
    "    \n",
    "    # (d) Apply sine transformation and add transformed columns\n",
    "    df['sine_month'] = df['month'].apply(sine_transform, max_val=12)\n",
    "    df['sine_weekofyear'] = df['weekofyear'].apply(sine_transform, max_val=52)\n",
    "    df['sine_dayofyear'] = df['dayofyear'].apply(sine_transform, max_val=365)\n",
    "    \n",
    "    # return df[['sine_month', 'sine_weekofyear', 'sine_dayofyear']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.08it/s]\n"
     ]
    }
   ],
   "source": [
    "df_encoded = sine_time_encoding('1980-01-01', '2020-12-31')\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = basin_name, row['River Point Name']\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    df_encoded.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'time_encodings.csv'))\n",
    "\n",
    "# with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    # _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)))\n",
    "\n",
    "for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)):\n",
    "    process(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terrain Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import rioxarray\n",
    "\n",
    "def coords_to_polygon(lon, lat, resolution):\n",
    "    half_res = resolution / 2\n",
    "    return Polygon([\n",
    "        (round(lon - half_res,3), round(lat - half_res,3)),\n",
    "        (round(lon - half_res,3), round(lat + half_res,3)),\n",
    "        (round(lon + half_res,3), round(lat + half_res,3)),\n",
    "        (round(lon + half_res,3), round(lat - half_res,3))\n",
    "    ])\n",
    "def tile_filename_to_coords(filename):\n",
    "    # format: n/s{dd}e/w{ddd}_elv.tif\n",
    "    # n/e: positive, s/w: negative\n",
    "    n_s, lat, e_w, lon = filename[0], int(filename[1:3]), filename[3], int(filename[4:7])\n",
    "    lat = lat if n_s == 'n' else -lat\n",
    "    lon = lon if e_w == 'e' else -lon\n",
    "    return (lon, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_percentage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_riserun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_degrees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_radians\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curvature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planform_curvature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile_curvature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:05<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "var_names = ['elv', 'slope_percentage', 'slope_riserun', 'slope_degrees', 'slope_radians', 'aspect', 'curvature', 'planform_curvature', 'profile_curvature', 'upa', 'wth']\n",
    "# valid_tiles = ['n30w150', 'n30w120', 'n30w090']\n",
    "\n",
    "issues = []\n",
    "for var_name in itertools.islice(var_names,0,None,1):\n",
    "    print(var_name)\n",
    "    tiles_paths = sorted(glob.glob(os.path.join(PATHS['MERIT-Hydro'], var_name, '**', '*.tif'), recursive=True))\n",
    "    # tiles_paths = [tile for tile in tiles_paths if os.path.basename(os.path.dirname(tile)).split('_')[-1] in valid_tiles]\n",
    "    tiles_filenames = [os.path.basename(tile) for tile in tiles_paths]\n",
    "    tiles_names = [tile.split('_')[0] for tile in tiles_filenames]\n",
    "    tiles_lower_left_corner = [tile_filename_to_coords(tile) for tile in tiles_filenames]\n",
    "    tiles_polygons = [Polygon([(lon, lat), (lon + 5, lat), (lon + 5, lat + 5), (lon, lat + 5)]) for lon, lat in tiles_lower_left_corner]\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = basin_name, row['River Point Name']\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = ['mean', 'std', '25%', '50%', '75%'])\n",
    "        cell_polygons = [coords_to_polygon(row['lon'], row['lat'], resolution) for _, row in nodes_coords.iterrows()]\n",
    "        catmt_polygon = cell_polygons[0]\n",
    "        for polygon in cell_polygons[1:]:\n",
    "            catmt_polygon = catmt_polygon.union(polygon)\n",
    "        intersected_tiles = []\n",
    "        for tile_polygon, tile_path in zip(tiles_polygons, tiles_paths):\n",
    "            if tile_polygon.intersects(catmt_polygon):\n",
    "                intersected_tiles.append(tile_path)\n",
    "        ds = rioxarray.open_rasterio(intersected_tiles[0])\n",
    "        for tile in intersected_tiles[1:]:\n",
    "            ds = ds.combine_first(rioxarray.open_rasterio(tile))\n",
    "        ds = ds.sel(band=1)\n",
    "        # Sort the x and y coordinates to be ascending\n",
    "        ds = ds.sortby('x', ascending=True)\n",
    "        ds = ds.sortby('y', ascending=True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            # ds_node = ds.rio.clip_box(lon - resolution/2, lat - resolution/2, lon + resolution/2, lat + resolution/2)\n",
    "            ds_node = ds.sel(x = slice(lon - resolution/2, lon + resolution/2), y = slice(lat - resolution/2, lat + resolution/2))\n",
    "            ds_node = ds_node.where(ds_node != ds.rio.nodata)\n",
    "            ds_node_values = ds_node.values.flatten()\n",
    "            mean = np.nanmean(ds_node_values)\n",
    "            std = np.nanstd(ds_node_values)\n",
    "            q25 = np.nanquantile(ds_node_values, 0.25)\n",
    "            q50 = np.nanquantile(ds_node_values, 0.50)\n",
    "            q75 = np.nanquantile(ds_node_values, 0.75)\n",
    "            data.loc['mean', node_idx] = mean\n",
    "            data.loc['std', node_idx] = std\n",
    "            data.loc['25%', node_idx] = q25\n",
    "            data.loc['50%', node_idx] = q50\n",
    "            data.loc['75%', node_idx] = q75\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'MERIT-Hydro'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'MERIT-Hydro', f\"{var_name}.csv\"))\n",
    "\n",
    "        ds.close()\n",
    "        del ds\n",
    "        gc.collect()\n",
    "\n",
    "    for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)):\n",
    "        try:\n",
    "            process(idx, row)\n",
    "        except Exception as e:\n",
    "            issues.append(f\"{var_name}-{row['huc_02']}-{row.name}\")\n",
    "            print(f\"Error: {var_name}-{row['huc_02']}-{row.name}. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_name</th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [var_name, huc_02, gauge_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df = [entry.split('-') for entry in issues]\n",
    "issues_df = pd.DataFrame(issues_df, columns = ['var_name', 'huc_02', 'gauge_id'])\n",
    "issues_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_name</th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [var_name, huc_02, gauge_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df[issues_df['var_name'] == 'elv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4017.53it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    # lon: -180 to 180; lat: -60 to 90\n",
    "    lon_transform = lambda x: np.sin(2 * np.pi * (x+180) / 360)\n",
    "    lat_transform = lambda x: (x - (-60))/(90 - (-60))\n",
    "\n",
    "    huc, gauge_id = basin_name, row['River Point Name']\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = ['lon_transformed', 'lat_transformed'])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        data.loc['lon_transformed', node_idx] = lon_transform(lon)\n",
    "        data.loc['lat_transformed', node_idx] = lat_transform(lat)\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'spatial_encodings.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uparea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2201.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uparea = xr.open_dataset(os.path.join(PATHS['gis_ldd'], 'CWatM_30min/upstream_area_km2.nc'))\n",
    "ds_varname = list(uparea.data_vars)[0]\n",
    "uparea = uparea[ds_varname]\n",
    "uparea = uparea.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "uparea.load()\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = basin_name, row['River Point Name']\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        data.loc[0, node_idx] = uparea.sel(lat = lat, lon = lon, method = 'nearest').values.item()\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'uparea.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)))\n",
    "\n",
    "uparea.close()\n",
    "del uparea\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndiaWRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4758.14it/s]\n"
     ]
    }
   ],
   "source": [
    "indiawris_flow = pd.read_csv(os.path.join(PATHS['IndiaWRIS'], 'indiawris_flow.csv'), index_col=0, parse_dates=True)\n",
    "indiawris_flow = indiawris_flow[indiawris_flow.columns.intersection(indiawris_graph['River Point Name'])]\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = basin_name, row['River Point Name']\n",
    "    uparea = row['snapped_uparea'] * 1e6\n",
    "\n",
    "    indiawris_gauge = indiawris_flow[[gauge_id]].copy()\n",
    "    indiawris_gauge.columns = ['Q_m3s']\n",
    "    indiawris_gauge['Q_mm'] = (indiawris_gauge['Q_m3s']*3600*24*1000) / uparea\n",
    "    \n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    indiawris_gauge.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'IndiaWRIS.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)))\n",
    "\n",
    "del indiawris_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloFAS Parameter Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanbnkf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 3318.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanflpn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2024.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4371.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanlength\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4275.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 3524.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4857.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanbw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4571.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracforest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4583.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracirrigated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 5155.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracrice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 5096.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracsealed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 5084.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracwater\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4038.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracother\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 4752.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"Catchment_morphology_and_river_network\" (14 surface fields)\n",
    "# - chanbnkf_Global_03min.nc (channel bankfull depth, m);\n",
    "# - chanflpn_Global_03min.nc (width of the floodplain, m);\n",
    "# - changrad_Global_03min.nc (channel longitudinal gradient, m/m);\n",
    "# - chanlength_Global_03min.nc (channel length within a pixel, m);\n",
    "# - chanman_Global_03min.nc (channel Manning's roughness coefficient, m^(1/3)s^(-1));\n",
    "# - chans_Global_03min.nc (channel side slope, m/m);\n",
    "# - chanbw_Global_03min.nc (channel bottom width, m):\n",
    "\n",
    "# \"Land_use\" (7 surface fields)\n",
    "# - fracforest_Global_03min.nc (fraction of forest for each grid-cell, -);\n",
    "# - fracirrigated_Global_03min.nc (fraction of irrigated crops [except rice] for each grid-cell, -);\n",
    "# - fracrice_Global_03min.nc (fraction of rice crops for each grid-cell, -);\n",
    "# - fracsealed_Global_03min.nc (fraction of urban area for each grid-cell, -);\n",
    "# - fracwater_Global_03min.nc (fraction of inland water for each grid-cell, -);\n",
    "# - fracother_Global_03min.nc (fraction of other land cover for each grid-cell, -);\n",
    "Parameter_Maps = os.path.join(PATHS['GloFAS'], 'LISFLOOD_Parameter_Maps')\n",
    "\n",
    "var_names = ['chanbnkf', 'chanflpn', 'changrad', 'chanlength', 'chanman', 'chans', 'chanbw']\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Catchments_morphology_and_river_network', f\"{var_name}_Global_03min.nc\"))['Band1']\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds.load()\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = basin_name, row['River Point Name']\n",
    "        nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            # ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            ds_window_loc = ds.sel(\n",
    "                lat = slice(lat + 0.5*resolution, lat - 0.5*resolution),\n",
    "                lon = slice(lon - 0.5*resolution, lon + 0.5*resolution)\n",
    "            ).mean()\n",
    "            data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "        os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()\n",
    "\n",
    "var_names = ['fracforest', 'fracirrigated', 'fracrice', 'fracsealed', 'fracwater', 'fracother']\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Land_use', f\"{var_name}_Global_03min.nc\"))['Band1']\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds.load()\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = basin_name, row['River Point Name']\n",
    "        nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            # ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            ds_window_loc = ds.sel(\n",
    "                lat = slice(lat + 0.5*resolution, lat - 0.5*resolution),\n",
    "                lon = slice(lon - 0.5*resolution, lon + 0.5*resolution)\n",
    "            ).mean()\n",
    "            data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "        os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(indiawris_graph.iterrows(), total=len(indiawris_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
