{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import networkx as nx\n",
    "\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import tqdm\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "cfg = configparser.ConfigParser()\n",
    "cfg.optionxform = str\n",
    "cfg.read('/data/sarth/rootdir/assets/global.ini')\n",
    "cfg = {s: dict(cfg.items(s)) for s in cfg.sections()}\n",
    "PATHS = cfg['PATHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME = '03min_GloFAS_CAMELS-US'\n",
    "SAVE_PATH = os.path.join(PATHS['devp_datasets'], DIRNAME)\n",
    "resolution = 0.05\n",
    "lon_360_180 = lambda x: (x + 180) % 360 - 180 # convert 0-360 to -180-180\n",
    "lon_180_360 = lambda x: x % 360 # convert -180-180 to 0-360\n",
    "region_bounds = {\n",
    "    'minx': -130,\n",
    "    'miny': 20,\n",
    "    'maxx': -65,\n",
    "    'maxy': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Watershed Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_lon</th>\n",
       "      <th>gauge_lat</th>\n",
       "      <th>area_geospa_fabric</th>\n",
       "      <th>snapped_lon</th>\n",
       "      <th>snapped_lat</th>\n",
       "      <th>snapped_uparea</th>\n",
       "      <th>snapped_iou</th>\n",
       "      <th>area_percent_difference</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_edges</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gauge_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>06452000</th>\n",
       "      <td>10</td>\n",
       "      <td>-99.55649</td>\n",
       "      <td>43.74833</td>\n",
       "      <td>25817.78</td>\n",
       "      <td>-99.525</td>\n",
       "      <td>43.775</td>\n",
       "      <td>26081.820000</td>\n",
       "      <td>0.942874</td>\n",
       "      <td>1.022710</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>1157.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13340000</th>\n",
       "      <td>17</td>\n",
       "      <td>-116.25750</td>\n",
       "      <td>46.47833</td>\n",
       "      <td>14270.76</td>\n",
       "      <td>-116.275</td>\n",
       "      <td>46.475</td>\n",
       "      <td>14113.031000</td>\n",
       "      <td>0.940266</td>\n",
       "      <td>1.105257</td>\n",
       "      <td>657.0</td>\n",
       "      <td>656.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06447000</th>\n",
       "      <td>10</td>\n",
       "      <td>-101.52487</td>\n",
       "      <td>43.75250</td>\n",
       "      <td>12869.46</td>\n",
       "      <td>-101.575</td>\n",
       "      <td>43.725</td>\n",
       "      <td>12933.430000</td>\n",
       "      <td>0.929852</td>\n",
       "      <td>0.497066</td>\n",
       "      <td>573.0</td>\n",
       "      <td>572.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06360500</th>\n",
       "      <td>10</td>\n",
       "      <td>-100.84292</td>\n",
       "      <td>45.25582</td>\n",
       "      <td>12601.47</td>\n",
       "      <td>-100.875</td>\n",
       "      <td>45.275</td>\n",
       "      <td>12741.733000</td>\n",
       "      <td>0.926271</td>\n",
       "      <td>1.113074</td>\n",
       "      <td>584.0</td>\n",
       "      <td>583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06354000</th>\n",
       "      <td>10</td>\n",
       "      <td>-100.93444</td>\n",
       "      <td>46.37611</td>\n",
       "      <td>10626.74</td>\n",
       "      <td>-100.925</td>\n",
       "      <td>46.375</td>\n",
       "      <td>10708.444000</td>\n",
       "      <td>0.919406</td>\n",
       "      <td>0.768854</td>\n",
       "      <td>500.0</td>\n",
       "      <td>499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14303200</th>\n",
       "      <td>17</td>\n",
       "      <td>-123.54650</td>\n",
       "      <td>45.32428</td>\n",
       "      <td>8.07</td>\n",
       "      <td>-123.525</td>\n",
       "      <td>45.325</td>\n",
       "      <td>21.783020</td>\n",
       "      <td>0.316890</td>\n",
       "      <td>169.925900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10336740</th>\n",
       "      <td>16</td>\n",
       "      <td>-119.93546</td>\n",
       "      <td>39.06658</td>\n",
       "      <td>7.94</td>\n",
       "      <td>-119.925</td>\n",
       "      <td>39.075</td>\n",
       "      <td>24.017084</td>\n",
       "      <td>0.186934</td>\n",
       "      <td>202.482150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01466500</th>\n",
       "      <td>02</td>\n",
       "      <td>-74.50528</td>\n",
       "      <td>39.88500</td>\n",
       "      <td>6.25</td>\n",
       "      <td>-74.525</td>\n",
       "      <td>39.875</td>\n",
       "      <td>23.746838</td>\n",
       "      <td>0.034301</td>\n",
       "      <td>279.949400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01594950</th>\n",
       "      <td>02</td>\n",
       "      <td>-79.39031</td>\n",
       "      <td>39.27669</td>\n",
       "      <td>6.10</td>\n",
       "      <td>-79.425</td>\n",
       "      <td>39.275</td>\n",
       "      <td>23.949966</td>\n",
       "      <td>0.206761</td>\n",
       "      <td>292.622400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06614800</th>\n",
       "      <td>10</td>\n",
       "      <td>-105.86501</td>\n",
       "      <td>40.49609</td>\n",
       "      <td>4.10</td>\n",
       "      <td>-105.875</td>\n",
       "      <td>40.475</td>\n",
       "      <td>23.541061</td>\n",
       "      <td>0.174398</td>\n",
       "      <td>474.172200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>671 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         huc_02  gauge_lon  gauge_lat  area_geospa_fabric  snapped_lon  \\\n",
       "gauge_id                                                                 \n",
       "06452000     10  -99.55649   43.74833            25817.78      -99.525   \n",
       "13340000     17 -116.25750   46.47833            14270.76     -116.275   \n",
       "06447000     10 -101.52487   43.75250            12869.46     -101.575   \n",
       "06360500     10 -100.84292   45.25582            12601.47     -100.875   \n",
       "06354000     10 -100.93444   46.37611            10626.74     -100.925   \n",
       "...         ...        ...        ...                 ...          ...   \n",
       "14303200     17 -123.54650   45.32428                8.07     -123.525   \n",
       "10336740     16 -119.93546   39.06658                7.94     -119.925   \n",
       "01466500     02  -74.50528   39.88500                6.25      -74.525   \n",
       "01594950     02  -79.39031   39.27669                6.10      -79.425   \n",
       "06614800     10 -105.86501   40.49609                4.10     -105.875   \n",
       "\n",
       "          snapped_lat  snapped_uparea  snapped_iou  area_percent_difference  \\\n",
       "gauge_id                                                                      \n",
       "06452000       43.775    26081.820000     0.942874                 1.022710   \n",
       "13340000       46.475    14113.031000     0.940266                 1.105257   \n",
       "06447000       43.725    12933.430000     0.929852                 0.497066   \n",
       "06360500       45.275    12741.733000     0.926271                 1.113074   \n",
       "06354000       46.375    10708.444000     0.919406                 0.768854   \n",
       "...               ...             ...          ...                      ...   \n",
       "14303200       45.325       21.783020     0.316890               169.925900   \n",
       "10336740       39.075       24.017084     0.186934               202.482150   \n",
       "01466500       39.875       23.746838     0.034301               279.949400   \n",
       "01594950       39.275       23.949966     0.206761               292.622400   \n",
       "06614800       40.475       23.541061     0.174398               474.172200   \n",
       "\n",
       "          num_nodes  num_edges  \n",
       "gauge_id                        \n",
       "06452000     1158.0     1157.0  \n",
       "13340000      657.0      656.0  \n",
       "06447000      573.0      572.0  \n",
       "06360500      584.0      583.0  \n",
       "06354000      500.0      499.0  \n",
       "...             ...        ...  \n",
       "14303200        1.0        0.0  \n",
       "10336740        1.0        0.0  \n",
       "01466500        1.0        0.0  \n",
       "01594950        1.0        0.0  \n",
       "06614800        1.0        0.0  \n",
       "\n",
       "[671 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_attributes_graph = pd.read_csv(os.path.join(SAVE_PATH, 'graph_attributes.csv'), index_col=0)\n",
    "camels_attributes_graph.index = camels_attributes_graph.index.map(lambda x: str(x).zfill(8))\n",
    "camels_attributes_graph['huc_02'] = camels_attributes_graph['huc_02'].map(lambda x: str(x).zfill(2))\n",
    "camels_attributes_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 11)\n",
      "(395, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "huc_02\n",
       "01    12\n",
       "02    39\n",
       "03    45\n",
       "04    16\n",
       "05    26\n",
       "06    10\n",
       "07    26\n",
       "08    10\n",
       "09     5\n",
       "10    52\n",
       "11    26\n",
       "12    28\n",
       "13     4\n",
       "14     5\n",
       "15    12\n",
       "16     6\n",
       "17    52\n",
       "18    21\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_graph = camels_attributes_graph.copy()\n",
    "camels_graph = camels_graph[camels_graph['area_percent_difference'] < 10]\n",
    "print(camels_graph.shape)\n",
    "camels_graph = camels_graph[camels_graph['num_nodes'] > 1]\n",
    "print(camels_graph.shape)\n",
    "# Print the number of graphs per 'huc_02' (sorted in values of huc_02)\n",
    "camels_graph.sort_values(ascending=True, by = 'huc_02').groupby('huc_02').size()\n",
    "# camels_graph['huc_02'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      395.000000\n",
       "mean      1158.826506\n",
       "std       2083.937516\n",
       "min         43.880000\n",
       "25%        270.385000\n",
       "50%        564.960000\n",
       "75%       1202.835000\n",
       "max      25817.780000\n",
       "Name: area_geospa_fabric, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_graph['area_geospa_fabric'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del camels_attributes_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Node Features as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(SAVE_PATH, \"graph_features\"), exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd = xr.open_dataset(os.path.join(PATHS['gis_ldd'], 'GloFAS_03min', 'ldd.nc'))\n",
    "ldd = ldd['ldd']\n",
    "ldd = ldd.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "\n",
    "lons = ldd['lon'].values\n",
    "lats = ldd['lat'].values\n",
    "\n",
    "ds_grid = xr.Dataset({\n",
    "    'lat': (['lat'], lats),\n",
    "    'lon': (['lon'], lons),\n",
    "})\n",
    "\n",
    "# Round the lat lon values to 3 decimal places in ds_grid\n",
    "ds_grid['lat'] = ds_grid['lat'].round(3)\n",
    "ds_grid['lon'] = ds_grid['lon'].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "precipitation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:04<00:00, 88.85it/s] \n"
     ]
    }
   ],
   "source": [
    "var_names = ['precipitation']\n",
    "# var_names = ['Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "\n",
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['precipitation']\n",
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_zarr(os.path.join(PATHS['GPM'], 'GPM_1998_2020.zarr'), consolidated=True)\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['miny'], region_bounds['maxy']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "\n",
    "    missing_date = np.datetime64('1999-09-03')\n",
    "    # Reindex the dataset to include the missing date (it will be filled with NaN)\n",
    "    new_times = np.sort(np.append(ds.time.values, missing_date))\n",
    "    ds = ds.reindex(time=new_times).chunk({'time': -1})\n",
    "    ds = ds.sortby('time')\n",
    "    # # Interpolate in time to fill NaN values using linear interpolation\n",
    "    # ds = ds.interpolate_na(dim='time', method='linear')\n",
    "    # # Chunk in a way that makes it faster\n",
    "    ds = ds.chunk({'time': 1, 'lat': -1, 'lon': -1})\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder', 'regridder_gpm_to_glofas_03min.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder', 'regridder_gpm_to_glofas_03min.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder', 'regridder_gpm_to_glofas_03min.nc'))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "\n",
    "    for start_year in range(1998, 2020+1, 5):\n",
    "        start_date = f\"{start_year}-01-01\"\n",
    "        end_date = f\"{min(start_year+4,2020)}-12-31\"\n",
    "        ds_window = ds_regrided.sel(time = slice(start_date, end_date)).copy()\n",
    "        start_time = time.time()\n",
    "        ds_window.load()\n",
    "        end_time = time.time()\n",
    "        print(start_date, end_date, f\"Time: {(end_time - start_time)/60:.2f} mins\")\n",
    "    \n",
    "        def process(idx, row):\n",
    "            huc, gauge_id = row['huc_02'], row.name\n",
    "            nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "            data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "            for node_idx, node_row in nodes_coords.iterrows():\n",
    "                lat, lon = node_row['lat'], node_row['lon']\n",
    "                ds_window_loc = ds_window.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "                # data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "                data.loc[start_date:end_date, str(node_idx)] = ds_window_loc.values\n",
    "            data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"))\n",
    "\n",
    "        with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "            _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "        # ds.close()\n",
    "        # del ds\n",
    "        # gc.collect()\n",
    "    ds_regrided.close()\n",
    "    del ds_regrided, ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:34<00:00, 11.30it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "\n",
    "    # Interpolate using a window of 15 days, centered\n",
    "    missing_date = np.datetime64('1999-09-03')\n",
    "    window = pd.Timedelta(days=1)\n",
    "    start_date = missing_date - window\n",
    "    end_date = missing_date + window\n",
    "    data_window = data.loc[start_date:end_date]\n",
    "    data.loc[missing_date] = data_window.mean(axis=0)\n",
    "\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [00:20, 18.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "var_names = ['precipitation']\n",
    "for var_name in var_names:\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        # Only consider from 1998 onwards\n",
    "        data = data.loc['1998-01-01':]\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        data = data.loc['1998-01-01':]\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERA5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "volumetric_soil_water_layer_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:04<00:00, 89.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:03<00:00, 110.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:03<00:00, 107.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:03<00:00, 109.64it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = [\n",
    "    # '2m_temperature', \n",
    "    # 'evaporation', \n",
    "    # 'snowfall', \n",
    "    # 'surface_net_solar_radiation', \n",
    "    # 'surface_net_thermal_radiation', \n",
    "    # 'surface_pressure', \n",
    "    # 'total_precipitation',\n",
    "    # '2m_dewpoint_temperature',\n",
    "    # '10m_u_component_of_wind',\n",
    "    # '10m_v_component_of_wind',\n",
    "    # 'forecast_albedo',\n",
    "    # 'potential_evaporation',\n",
    "    # 'runoff',\n",
    "    # 'snow_albedo',\n",
    "    # 'snow_depth',\n",
    "    # 'snowmelt',\n",
    "    # 'sub_surface_runoff',\n",
    "    # 'surface_runoff',\n",
    "    # 'total_column_water',\n",
    "    'volumetric_soil_water_layer_1',\n",
    "    'volumetric_soil_water_layer_2',\n",
    "    'volumetric_soil_water_layer_3',\n",
    "    'volumetric_soil_water_layer_4'\n",
    "]\n",
    "\n",
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_1\n",
      "Time: 7.0533 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:50<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_2\n",
      "Time: 5.1113 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:49<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_3\n",
      "Time: 6.0752 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [02:02<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "volumetric_soil_water_layer_4\n",
      "Time: 5.6948 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [02:17<00:00,  2.87it/s]\n"
     ]
    }
   ],
   "source": [
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['RawData'], 'ERA5', var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds['lon'] = [lon_360_180(lon) for lon in ds['lon'].values]\n",
    "    ds = ds.sortby('lon')\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    _, index = np.unique(ds['time'], return_index = True)\n",
    "    ds = ds.isel(time = index)\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_era5_to_glofas_03min.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder_era5_to_glofas_03min.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_era5_to_glofas_03min.nc'))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'Time: {((end_time - start_time) / 60):.4f} mins')\n",
    "    \n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds_regrided.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_soil_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:16<00:00, 23.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_high_vegetation_cover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:16<00:00, 24.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_low_vegetation_cover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:16<00:00, 24.66it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_type_of_high_vegetation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:16<00:00, 24.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_type_of_low_vegetation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:16<00:00, 24.60it/s] \n"
     ]
    }
   ],
   "source": [
    "var_names = [\n",
    "    'static_soil_type', \n",
    "    'static_high_vegetation_cover', \n",
    "    'static_low_vegetation_cover', \n",
    "    'static_type_of_high_vegetation', \n",
    "    'static_type_of_low_vegetation'\n",
    "    ]\n",
    "ds_filenames = [\n",
    "    'soil_type_static.nc',\n",
    "    'high_vegetation_cover_static.nc',\n",
    "    'low_vegetation_cover_static.nc',\n",
    "    'type_of_high_vegetation_static.nc',\n",
    "    'type_of_low_vegetation_static.nc'\n",
    "]\n",
    "\n",
    "for var_name, ds_filename in zip(var_names, ds_filenames):\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(PATHS['RawData'], 'ERA5', ds_filename))\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.isel(time = 0)\n",
    "    ds = ds.drop('time')\n",
    "    ds = ds.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
    "    ds['lon'] = [lon_360_180(lon) for lon in ds['lon'].values]\n",
    "    ds = ds.sortby('lon')\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[0, node_idx] = int(ds_window_loc.values)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'ERA5'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "        process(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HWSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_CLAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:05<00:00, 70.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_GRAVEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:05<00:00, 71.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_SAND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:05<00:00, 71.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_SILT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:05<00:00, 71.23it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_CLAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:05<00:00, 71.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_GRAVEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:05<00:00, 70.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_SAND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:05<00:00, 68.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_SILT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:05<00:00, 70.81it/s] \n"
     ]
    }
   ],
   "source": [
    "var_names = ['S_CLAY', 'S_GRAVEL', 'S_SAND', 'S_SILT', 'T_CLAY', 'T_GRAVEL', 'T_SAND', 'T_SILT']\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(PATHS['HWSD'], f'{var_name}.nc4'))\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['miny'], region_bounds['maxy']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds = ds / 100\n",
    "    ds.load()\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(\n",
    "                lat = slice(lat-resolution/2, lat+resolution/2),\n",
    "                lon = slice(lon-resolution/2, lon+resolution/2)\n",
    "            ).values.mean()\n",
    "            data.loc[0, node_idx] = ds_window_loc\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'HWSD'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'HWSD', f\"{var_name}.csv\"))\n",
    "\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "        process(idx, row)\n",
    "    \n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "Ep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:04<00:00, 84.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMroot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:03<00:00, 103.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMsurf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:03<00:00, 107.87it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "\n",
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep\n",
      "Time: 5.1678 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [02:11<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMroot\n",
      "Time: 2.8857 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [02:21<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMsurf\n",
      "Time: 2.7060 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [03:06<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_gleam_to_glofas_03min.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder_gleam_to_glofas_03min.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_gleam_to_glofas_03min.nc'))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'Time: {((end_time - start_time) / 60):.4f} mins')\n",
    "    \n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds_regrided.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep (Time: 3.2873 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [01:56,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 24\n",
      "------\n",
      "0 10 06191500\n",
      "Number of nodes with NaN values: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:05<00:00, 13.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 11\n",
      "0 10 06191500 True\n",
      "------\n",
      "1 04 04056500\n",
      "Number of nodes with NaN values: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 13.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "1 04 04056500 False\n",
      "------\n",
      "2 10 06043500\n",
      "Number of nodes with NaN values: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:02<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n",
      "2 10 06043500 True\n",
      "------\n",
      "3 04 04045500\n",
      "Number of nodes with NaN values: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:02<00:00, 13.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "3 04 04045500 False\n",
      "------\n",
      "4 10 06188000\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "4 10 06188000 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "5 18 11532500\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 13.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "5 18 11532500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "6 03 02196000\n",
      "Number of nodes with NaN values: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "6 03 02196000 False\n",
      "------\n",
      "7 08 08014500\n",
      "Number of nodes with NaN values: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "7 08 08014500 False\n",
      "------\n",
      "8 08 08013000\n",
      "Number of nodes with NaN values: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 13.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "8 08 08013000 False\n",
      "------\n",
      "9 17 12040500\n",
      "Number of nodes with NaN values: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 13.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "9 17 12040500 False\n",
      "------\n",
      "10 10 06919500\n",
      "Number of nodes with NaN values: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:02<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 2\n",
      "10 10 06919500 True\n",
      "------\n",
      "11 07 05495000\n",
      "Number of nodes with NaN values: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "11 07 05495000 False\n",
      "------\n",
      "12 17 12411000\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 13.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "12 17 12411000 False\n",
      "------\n",
      "13 03 02481510\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 13.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "13 03 02481510 False\n",
      "------\n",
      "14 07 05413500\n",
      "Number of nodes with NaN values: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 13.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n",
      "14 07 05413500 True\n",
      "------\n",
      "15 17 12041200\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "15 17 12041200 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "16 17 12048000\n",
      "Number of nodes with NaN values: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "16 17 12048000 False\n",
      "------\n",
      "17 12 08025500\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 13.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "17 12 08025500 False\n",
      "------\n",
      "18 07 05414000\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 13.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "18 07 05414000 False\n",
      "------\n",
      "19 08 07359610\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "19 08 07359610 False\n",
      "------\n",
      "20 18 11468500\n",
      "Number of nodes with NaN values: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 13.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n",
      "20 18 11468500 True\n",
      "------\n",
      "21 03 02481000\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "21 03 02481000 False\n",
      "------\n",
      "22 04 04015330\n",
      "Number of nodes with NaN values: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "22 04 04015330 False\n",
      "------\n",
      "23 04 04043050\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "23 04 04043050 False\n",
      "------\n",
      "SMroot (Time: 9.0868 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [01:24,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 25\n",
      "------\n",
      "0 10 06191500\n",
      "Number of nodes with NaN values: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:06<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 11\n",
      "0 10 06191500 True\n",
      "------\n",
      "1 04 04056500\n",
      "Number of nodes with NaN values: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "1 04 04056500 False\n",
      "------\n",
      "2 10 06043500\n",
      "Number of nodes with NaN values: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:03<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n",
      "2 10 06043500 True\n",
      "------\n",
      "3 04 04045500\n",
      "Number of nodes with NaN values: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:03<00:00, 11.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "3 04 04045500 False\n",
      "------\n",
      "4 10 06188000\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "4 10 06188000 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "5 18 11532500\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "5 18 11532500 False\n",
      "------\n",
      "6 03 02196000\n",
      "Number of nodes with NaN values: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "6 03 02196000 False\n",
      "------\n",
      "7 08 08014500\n",
      "Number of nodes with NaN values: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 10.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "7 08 08014500 False\n",
      "------\n",
      "8 08 08013000\n",
      "Number of nodes with NaN values: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "8 08 08013000 False\n",
      "------\n",
      "9 17 12040500\n",
      "Number of nodes with NaN values: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "9 17 12040500 False\n",
      "------\n",
      "10 10 06919500\n",
      "Number of nodes with NaN values: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:03<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 2\n",
      "10 10 06919500 True\n",
      "------\n",
      "11 07 05495000\n",
      "Number of nodes with NaN values: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "11 07 05495000 False\n",
      "------\n",
      "12 17 12411000\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "12 17 12411000 False\n",
      "------\n",
      "13 03 02481510\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "13 03 02481510 False\n",
      "------\n",
      "14 07 05413500\n",
      "Number of nodes with NaN values: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n",
      "14 07 05413500 True\n",
      "------\n",
      "15 17 12041200\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "15 17 12041200 False\n",
      "------\n",
      "16 17 12048000\n",
      "Number of nodes with NaN values: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "16 17 12048000 False\n",
      "------\n",
      "17 18 11148900\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "17 18 11148900 False\n",
      "------\n",
      "18 12 08025500\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "18 12 08025500 False\n",
      "------\n",
      "19 07 05414000\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "19 07 05414000 False\n",
      "------\n",
      "20 08 07359610\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "20 08 07359610 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "21 18 11468500\n",
      "Number of nodes with NaN values: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n",
      "21 18 11468500 True\n",
      "------\n",
      "22 03 02481000\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "22 03 02481000 False\n",
      "------\n",
      "23 04 04015330\n",
      "Number of nodes with NaN values: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "23 04 04015330 False\n",
      "------\n",
      "24 04 04043050\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "24 04 04043050 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "SMsurf (Time: 5.5462 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [01:12,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 25\n",
      "------\n",
      "0 10 06191500\n",
      "Number of nodes with NaN values: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:07<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 11\n",
      "0 10 06191500 True\n",
      "------\n",
      "1 04 04056500\n",
      "Number of nodes with NaN values: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "1 04 04056500 False\n",
      "------\n",
      "2 10 06043500\n",
      "Number of nodes with NaN values: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:04<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n",
      "2 10 06043500 True\n",
      "------\n",
      "3 04 04045500\n",
      "Number of nodes with NaN values: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:04<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "3 04 04045500 False\n",
      "------\n",
      "4 10 06188000\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "4 10 06188000 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "5 18 11532500\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 12.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "5 18 11532500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "6 03 02196000\n",
      "Number of nodes with NaN values: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00, 12.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "6 03 02196000 False\n",
      "------\n",
      "7 08 08014500\n",
      "Number of nodes with NaN values: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "7 08 08014500 False\n",
      "------\n",
      "8 08 08013000\n",
      "Number of nodes with NaN values: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "8 08 08013000 False\n",
      "------\n",
      "9 17 12040500\n",
      "Number of nodes with NaN values: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 12.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "9 17 12040500 False\n",
      "------\n",
      "10 10 06919500\n",
      "Number of nodes with NaN values: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:03<00:00, 12.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 2\n",
      "10 10 06919500 True\n",
      "------\n",
      "11 07 05495000\n",
      "Number of nodes with NaN values: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "11 07 05495000 False\n",
      "------\n",
      "12 17 12411000\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "12 17 12411000 False\n",
      "------\n",
      "13 03 02481510\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "13 03 02481510 False\n",
      "------\n",
      "14 07 05413500\n",
      "Number of nodes with NaN values: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 12.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n",
      "14 07 05413500 True\n",
      "------\n",
      "15 17 12041200\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "15 17 12041200 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "16 17 12048000\n",
      "Number of nodes with NaN values: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 12.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "16 17 12048000 False\n",
      "------\n",
      "17 18 11148900\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "17 18 11148900 False\n",
      "------\n",
      "18 12 08025500\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "18 12 08025500 False\n",
      "------\n",
      "19 07 05414000\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 10.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "19 07 05414000 False\n",
      "------\n",
      "20 08 07359610\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 12.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "20 08 07359610 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "21 18 11468500\n",
      "Number of nodes with NaN values: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 12.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n",
      "21 18 11468500 True\n",
      "------\n",
      "22 03 02481000\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 12.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "22 03 02481000 False\n",
      "------\n",
      "23 04 04015330\n",
      "Number of nodes with NaN values: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 12.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "23 04 04015330 False\n",
      "------\n",
      "24 04 04043050\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 12.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "24 04 04043050 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "for var_name in var_names:\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_gleam_to_glofas_03min.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder_gleam_to_glofas_03min.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_gleam_to_glofas_03min.nc'))\n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'{var_name} (Time: {((end_time - start_time) / 60):.4f} mins)')\n",
    "\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "                node_lat = float(round(nodes_coords.loc[node_idx, 'lat'], 3))\n",
    "                node_lon = float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "                multiplier = 1.5\n",
    "                ds_slice = ds_regrided.sel(\n",
    "                    lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                    lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                    )\n",
    "                slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "                slice_df['lat'] = slice_df['lat'].round(3)\n",
    "                slice_df['lon'] = slice_df['lon'].round(3)\n",
    "                slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "                slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "                num_nan_nodes = slice_df.isnull().any(axis=0).sum()\n",
    "                num_nonnan_nodes = len(slice_df.columns) - num_nan_nodes\n",
    "                nodes_coords.loc[node_idx, 'nonNaNneighbours'] = num_nonnan_nodes\n",
    "        nodes_coords_sorted = nodes_coords.sort_values(by = 'nonNaNneighbours', ascending = False)\n",
    "        nodes_coords_sorted = nodes_coords_sorted[nodes_coords_sorted['isNaN']]\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted.shape[0]}\")\n",
    "        \n",
    "        for node_idx in tqdm.tqdm(nodes_coords_sorted.index):\n",
    "            node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "            multiplier = 1.5\n",
    "            ds_slice = ds_regrided.sel(\n",
    "                lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                )\n",
    "            slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "            slice_df['lat'] = slice_df['lat'].round(3)\n",
    "            slice_df['lon'] = slice_df['lon'].round(3)\n",
    "            slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "            slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "            slice_df.columns = list(map(str, slice_df.columns))\n",
    "            num_nonnan_nodes = len(slice_df.columns) - slice_df.isnull().any(axis=0).sum()\n",
    "            # print(node_idx, (node_lat, node_lon), num_nonnan_nodes)\n",
    "            if num_nonnan_nodes == 9:\n",
    "                replacement_values = slice_df.loc[:, f\"({node_lat}, {node_lon})\"]\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "            elif num_nonnan_nodes > 0:\n",
    "                replacement_values = np.nanmean(slice_df, axis = 1)\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                ds_regrided.loc[dict(lat = node_lat, lon = node_lon)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted['isNaN'].sum()}\")\n",
    "        print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [00:57,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 5\n",
      "------\n",
      "0 10 06191500\n",
      "Number of nodes with NaN values: 11\n",
      "------\n",
      "1 10 06043500\n",
      "Number of nodes with NaN values: 1\n",
      "------\n",
      "2 10 06919500\n",
      "Number of nodes with NaN values: 2\n",
      "------\n",
      "3 07 05413500\n",
      "Number of nodes with NaN values: 1\n",
      "------\n",
      "4 18 11468500\n",
      "Number of nodes with NaN values: 1\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [01:23,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 5\n",
      "------\n",
      "0 10 06191500\n",
      "Number of nodes with NaN values: 11\n",
      "------\n",
      "1 10 06043500\n",
      "Number of nodes with NaN values: 1\n",
      "------\n",
      "2 10 06919500\n",
      "Number of nodes with NaN values: 2\n",
      "------\n",
      "3 07 05413500\n",
      "Number of nodes with NaN values: 1\n",
      "------\n",
      "4 18 11468500\n",
      "Number of nodes with NaN values: 1\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [00:56,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 5\n",
      "------\n",
      "0 10 06191500\n",
      "Number of nodes with NaN values: 11\n",
      "------\n",
      "1 10 06043500\n",
      "Number of nodes with NaN values: 1\n",
      "------\n",
      "2 10 06919500\n",
      "Number of nodes with NaN values: 2\n",
      "------\n",
      "3 07 05413500\n",
      "Number of nodes with NaN values: 1\n",
      "------\n",
      "4 18 11468500\n",
      "Number of nodes with NaN values: 1\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "for var_name in var_names:\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [00:42,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 5\n",
      "------\n",
      "0 10 06191500\n",
      "Number of nodes with NaN values: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 10 06191500 False\n",
      "------\n",
      "1 10 06043500\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 53.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "1 10 06043500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "2 10 06919500\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 107.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "2 10 06919500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "3 07 05413500\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 111.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "3 07 05413500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "4 18 11468500\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 194.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "4 18 11468500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [00:53,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 5\n",
      "------\n",
      "0 10 06191500\n",
      "Number of nodes with NaN values: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 17.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 10 06191500 False\n",
      "------\n",
      "1 10 06043500\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 45.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "1 10 06043500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "2 10 06919500\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 86.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "2 10 06919500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "3 07 05413500\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 109.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "3 07 05413500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "4 18 11468500\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 102.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "4 18 11468500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [00:54,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 5\n",
      "------\n",
      "0 10 06191500\n",
      "Number of nodes with NaN values: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 17.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 10 06191500 False\n",
      "------\n",
      "1 10 06043500\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 37.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "1 10 06043500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "2 10 06919500\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 68.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "2 10 06919500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "3 07 05413500\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 107.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "3 07 05413500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "4 18 11468500\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 152.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "4 18 11468500 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "for var_name in var_names:\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "\n",
    "        \n",
    "        for node_idx in tqdm.tqdm(nodes_coords[nodes_coords['isNaN']].index):\n",
    "            nodes_coords['distances'] = None\n",
    "            node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "            for node_idx2 in nodes_coords[nodes_coords['isNaN'] == False].index:\n",
    "                if node_idx != node_idx2:\n",
    "                    node_lat2, node_lon2 = float(round(nodes_coords.loc[node_idx2, 'lat'], 3)), float(round(nodes_coords.loc[node_idx2, 'lon'], 3))\n",
    "                    distance = np.sqrt((node_lat - node_lat2)**2 + (node_lon - node_lon2)**2)\n",
    "                    nodes_coords.loc[node_idx2, 'distances'] = distance\n",
    "            min_distance = nodes_coords.loc[nodes_coords['distances'].idxmin(), 'distances']\n",
    "            # Replace with mean of nodes having distance equal to min_distance\n",
    "            replacement_nodes = nodes_coords[nodes_coords['distances'] == min_distance].index\n",
    "            replacement_nodes = list(map(str, replacement_nodes))\n",
    "            replacement_values = data.loc[:, replacement_nodes].mean(axis = 1)\n",
    "            data.loc[:, str(node_idx)] = replacement_values\n",
    "            nodes_coords.loc[node_idx, 'isNaN'] = False\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "        print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLEAM4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_names = ['Ep', 'SMrz', 'SMs']#, 'Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "var_names = ['Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "\n",
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], 'GLEAM4.2a', var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder', 'regridder_gleam4_to_glofas_03min.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder', 'regridder_gleam4_to_glofas_03min.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder', 'regridder_gleam4_to_glofas_03min.nc'))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'Time: {((end_time - start_time) / 60):.4f} mins')\n",
    "    \n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds_regrided.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_names = ['Ep', 'SMrz', 'SMs']#, 'Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "var_names = ['Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "for var_name in var_names:\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], 'GLEAM4.2a', var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder', 'regridder_gleam4_to_glofas_03min.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder', 'regridder_gleam4_to_glofas_03min.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder', 'regridder_gleam4_to_glofas_03min.nc'))\n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'{var_name} (Time: {((end_time - start_time) / 60):.4f} mins)')\n",
    "\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "                node_lat = float(round(nodes_coords.loc[node_idx, 'lat'], 3))\n",
    "                node_lon = float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "                multiplier = 1.5\n",
    "                ds_slice = ds_regrided.sel(\n",
    "                    lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                    lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                    )\n",
    "                slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "                slice_df['lat'] = slice_df['lat'].round(3)\n",
    "                slice_df['lon'] = slice_df['lon'].round(3)\n",
    "                slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "                slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "                num_nan_nodes = slice_df.isnull().any(axis=0).sum()\n",
    "                num_nonnan_nodes = len(slice_df.columns) - num_nan_nodes\n",
    "                nodes_coords.loc[node_idx, 'nonNaNneighbours'] = num_nonnan_nodes\n",
    "        nodes_coords_sorted = nodes_coords.sort_values(by = 'nonNaNneighbours', ascending = False)\n",
    "        nodes_coords_sorted = nodes_coords_sorted[nodes_coords_sorted['isNaN']]\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted.shape[0]}\")\n",
    "        \n",
    "        for node_idx in tqdm.tqdm(nodes_coords_sorted.index):\n",
    "            node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "            multiplier = 1.5\n",
    "            ds_slice = ds_regrided.sel(\n",
    "                lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                )\n",
    "            slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "            slice_df['lat'] = slice_df['lat'].round(3)\n",
    "            slice_df['lon'] = slice_df['lon'].round(3)\n",
    "            slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "            slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "            slice_df.columns = list(map(str, slice_df.columns))\n",
    "            num_nonnan_nodes = len(slice_df.columns) - slice_df.isnull().any(axis=0).sum()\n",
    "            # print(node_idx, (node_lat, node_lon), num_nonnan_nodes)\n",
    "            if num_nonnan_nodes == 9:\n",
    "                replacement_values = slice_df.loc[:, f\"({node_lat}, {node_lon})\"]\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "            elif num_nonnan_nodes > 0:\n",
    "                replacement_values = np.nanmean(slice_df, axis = 1)\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                ds_regrided.loc[dict(lat = node_lat, lon = node_lon)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted['isNaN'].sum()}\")\n",
    "        print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"))\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_names = ['Ep', 'SMrz', 'SMs']#, 'Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "var_names = ['Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "for var_name in var_names:\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_names = ['Ep', 'SMrz', 'SMs']#, 'Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "var_names = ['Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "for var_name in var_names:\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "\n",
    "        \n",
    "        for node_idx in tqdm.tqdm(nodes_coords[nodes_coords['isNaN']].index):\n",
    "            nodes_coords['distances'] = None\n",
    "            node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "            for node_idx2 in nodes_coords[nodes_coords['isNaN'] == False].index:\n",
    "                if node_idx != node_idx2:\n",
    "                    node_lat2, node_lon2 = float(round(nodes_coords.loc[node_idx2, 'lat'], 3)), float(round(nodes_coords.loc[node_idx2, 'lon'], 3))\n",
    "                    distance = np.sqrt((node_lat - node_lat2)**2 + (node_lon - node_lon2)**2)\n",
    "                    nodes_coords.loc[node_idx2, 'distances'] = distance\n",
    "            min_distance = nodes_coords.loc[nodes_coords['distances'].idxmin(), 'distances']\n",
    "            # Replace with mean of nodes having distance equal to min_distance\n",
    "            replacement_nodes = nodes_coords[nodes_coords['distances'] == min_distance].index\n",
    "            replacement_nodes = list(map(str, replacement_nodes))\n",
    "            replacement_values = data.loc[:, replacement_nodes].mean(axis = 1)\n",
    "            data.loc[:, str(node_idx)] = replacement_values\n",
    "            nodes_coords.loc[node_idx, 'isNaN'] = False\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "        print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"))\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solar Insolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solar_insolation(lat, lon, start_date, end_date):\n",
    "    # Constants\n",
    "    Sc = 1361  # Solar constant (W/m^2)\n",
    "    \n",
    "    # Convert dates to datetime objects\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Generate date range\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "    \n",
    "    # Function to calculate solar declination\n",
    "    def solar_declination(n):\n",
    "        return 23.45 * np.sin(np.radians((360 / 365) * (n - 81)))\n",
    "\n",
    "    # Function to calculate cos(theta_z) for solar zenith angle\n",
    "    def cos_theta_z(lat, decl, hour_angle):\n",
    "        lat_rad = np.radians(lat)\n",
    "        decl_rad = np.radians(decl)\n",
    "        return (np.sin(lat_rad) * np.sin(decl_rad) + \n",
    "                np.cos(lat_rad) * np.cos(decl_rad) * np.cos(np.radians(hour_angle)))\n",
    "    \n",
    "    # Function to calculate the hour angle\n",
    "    def hour_angle(lon, date):\n",
    "        # Assuming solar noon (local solar time = 12 hours)\n",
    "        return 0  # hour angle at solar noon\n",
    "    \n",
    "    # Calculate solar insolation for each day\n",
    "    insolation_values = []\n",
    "    for date in dates:\n",
    "        day_of_year = date.day_of_year\n",
    "        declination = solar_declination(day_of_year)\n",
    "        h = hour_angle(lon, date)\n",
    "        cos_zenith_angle = cos_theta_z(lat, declination, h)\n",
    "        \n",
    "        # Insolation formula\n",
    "        insolation = Sc * (1 + 0.033 * np.cos(np.radians(360 * day_of_year / 365))) * cos_zenith_angle\n",
    "        \n",
    "        # Make sure insolation is non-negative\n",
    "        insolation = max(insolation, 0)\n",
    "        insolation_values.append(insolation)\n",
    "    \n",
    "    # Create pandas Series\n",
    "    insolation_series = pd.Series(insolation_values, index=dates, name='Solar Insolation (kW/m²)')\n",
    "    insolation_series = insolation_series / 1000  # Convert to kW/m²\n",
    "    \n",
    "    return insolation_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [07:07<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = dates)\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        ds_window_loc = solar_insolation(lat, lon, '1980-01-01', '2020-12-31')\n",
    "        data.loc[:, node_idx] = ds_window_loc.values\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'solar_insolation.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_time_encoding(start_date, end_date):\n",
    "    # (a) Create a date_range and remove leap days\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    dates = dates[~((dates.month == 2) & (dates.day == 29))]  # Remove February 29 (leap days)\n",
    "    \n",
    "    # (b) Create a dataframe with 'month', 'weekofyear', 'dayofyear' columns\n",
    "    df = pd.DataFrame(index=dates)\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    \n",
    "    # (c) Define lambda transformations for sine encoding\n",
    "    # For day of year (range 1-365), week of year (range 1-52), and month (range 1-12)\n",
    "    sine_transform = lambda x, max_val: np.sin(2 * np.pi * x / max_val)\n",
    "    \n",
    "    # (d) Apply sine transformation and add transformed columns\n",
    "    df['sine_month'] = df['month'].apply(sine_transform, max_val=12)\n",
    "    df['sine_weekofyear'] = df['weekofyear'].apply(sine_transform, max_val=52)\n",
    "    df['sine_dayofyear'] = df['dayofyear'].apply(sine_transform, max_val=365)\n",
    "    \n",
    "    # return df[['sine_month', 'sine_weekofyear', 'sine_dayofyear']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:39<00:00, 10.11it/s]\n"
     ]
    }
   ],
   "source": [
    "df_encoded = sine_time_encoding('1980-01-01', '2020-12-31')\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    df_encoded.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'time_encodings.csv'))\n",
    "\n",
    "# with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    # _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "    process(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daymet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "prcp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:11<00:00, 35.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:03<00:00, 102.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:03<00:00, 111.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:04<00:00, 89.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:03<00:00, 107.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:03<00:00, 104.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dayl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:03<00:00, 107.02it/s]\n"
     ]
    }
   ],
   "source": [
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "var_names = ['prcp', 'srad', 'swe', 'tmax', 'tmin', 'vp', 'dayl']\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'Daymet'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'Daymet', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates = ['1980-12-31', '1984-12-31', '1988-12-31', '1992-12-31',\n",
    "               '1996-12-31', '2000-12-31', '2004-12-31', '2008-12-31',\n",
    "               '2012-12-31', '2016-12-31', '2020-12-31']\n",
    "missing_dates = [date + 'T12:00:00' for date in missing_dates]\n",
    "missing_dates = np.array(missing_dates, dtype = 'datetime64')\n",
    "ds_missing_dates = xr.DataArray(\n",
    "    np.nan*np.zeros((len(missing_dates), len(lats), len(lons))),\n",
    "    coords = [missing_dates, lats, lons],\n",
    "    dims = ['time', 'lat', 'lon']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prcp\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 7.07 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:23<00:00, 17.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 7.37 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:26<00:00, 15.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 7.38 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:28<00:00, 14.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 7.27 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:33<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 6.86 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:33<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 7.17 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:38<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 7.16 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:41<00:00,  9.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 7.34 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:42<00:00,  9.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 1.63 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:37<00:00, 10.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srad\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 7.54 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:30<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 7.98 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:39<00:00, 10.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 7.92 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:44<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 7.79 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:51<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 7.42 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:58<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 7.82 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:07<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 7.89 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:16<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 8.13 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:23<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 1.76 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:17<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swe\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 7.12 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:25<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 7.53 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:26<00:00, 14.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 7.42 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:28<00:00, 13.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 7.41 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:33<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 7.11 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:33<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 7.29 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:36<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 7.46 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:39<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 7.56 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:41<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 1.65 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:35<00:00, 11.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmax\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 7.09 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:32<00:00, 12.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 7.63 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:36<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 7.48 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:46<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 7.39 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:52<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 7.04 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:01<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 7.44 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:08<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 7.42 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:18<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 7.84 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:23<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 1.76 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:19<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmin\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 7.37 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:31<00:00, 12.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 8.02 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:36<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 7.65 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:44<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 7.37 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:52<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 7.11 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:00<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 7.45 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:09<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 7.62 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:17<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 7.85 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:25<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 1.96 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:19<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vp\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 9.54 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:33<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 9.55 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:37<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 8.80 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:45<00:00,  8.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 8.49 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:52<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 8.10 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:00<00:00,  6.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 8.93 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:07<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 8.86 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:14<00:00,  5.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 9.05 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:25<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 1.84 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:18<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dayl\n",
      "timesteps: 14965\n",
      "1980-01-01 1984-12-31 Time: 6.93 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:31<00:00, 12.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985-01-01 1989-12-31 Time: 7.44 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:34<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1990-01-01 1994-12-31 Time: 7.50 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:38<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995-01-01 1999-12-31 Time: 7.47 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:45<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01 2004-12-31 Time: 7.10 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:50<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005-01-01 2009-12-31 Time: 7.45 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:56<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2014-12-31 Time: 7.85 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:02<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-01 2019-12-31 Time: 7.87 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:07<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-01 2020-12-31 Time: 1.70 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:04<00:00,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['prcp', 'srad', 'swe', 'tmax', 'tmin', 'vp', 'dayl']\n",
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['Daymet'], var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds = ds[var_name]\n",
    "    ds = ds.rename({'x': 'lon', 'y': 'lat'})\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_daymet_to_glofas_03min.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder_daymet_to_glofas_03min.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_daymet_to_glofas_03min.nc'))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    # ds_regrided['time'] = ds_regrided['time'].dt.floor('D')\n",
    "    # ds_regrided['time'] = [np.datetime64(str(date).split('T')[0]) for date in ds_regrided['time'].values]\n",
    "\n",
    "    # Concatenate missing dates\n",
    "    ds_regrided = xr.concat([ds_regrided, ds_missing_dates], dim = 'time')\n",
    "    ds_regrided = ds_regrided.sortby('time')\n",
    "    ds.close()\n",
    "    # Print length of time\n",
    "    print(f\"timesteps: {len(ds_regrided['time'])}\")\n",
    "\n",
    "    for start_year in range(1980, 2020+1, 5):\n",
    "        start_date = f\"{start_year}-01-01\"\n",
    "        end_date = f\"{min(start_year+4,2020)}-12-31\"\n",
    "        ds_window = ds_regrided.sel(time = slice(start_date, end_date)).copy()\n",
    "        start_time = time.time()\n",
    "        ds_window.load()\n",
    "        end_time = time.time()\n",
    "        print(start_date, end_date, f\"Time: {(end_time - start_time)/60:.2f} mins\")\n",
    "    \n",
    "        def process(idx, row):\n",
    "            huc, gauge_id = row['huc_02'], row.name\n",
    "            nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "            data = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'Daymet', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "            for node_idx, node_row in nodes_coords.iterrows():\n",
    "                lat, lon = node_row['lat'], node_row['lon']\n",
    "                ds_window_loc = ds_window.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "                data.loc[start_date:end_date, str(node_idx)] = ds_window_loc.values\n",
    "            data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'Daymet', f\"{var_name}.csv\"))\n",
    "            return None\n",
    "        \n",
    "        with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "            _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "        ds_window.close()\n",
    "        del ds_window\n",
    "        gc.collect()\n",
    "\n",
    "    ds_regrided.close()\n",
    "    del ds, ds_regrided\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "395it [59:52,  9.10s/it]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['prcp', 'srad', 'swe', 'tmax', 'tmin', 'vp', 'dayl']\n",
    "for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    for var_name in var_names:\n",
    "        data = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'Daymet', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        # Fill the NaN values with a window of 15 days centered around the missing value\n",
    "        for col in data.columns:\n",
    "            data[col] = data[col].fillna(data[col].rolling(15, min_periods = 1, center = True).mean())\n",
    "        data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'Daymet', f\"{var_name}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terrain Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import rioxarray\n",
    "\n",
    "def coords_to_polygon(lon, lat, resolution):\n",
    "    half_res = resolution / 2\n",
    "    return Polygon([\n",
    "        (round(lon - half_res,3), round(lat - half_res,3)),\n",
    "        (round(lon - half_res,3), round(lat + half_res,3)),\n",
    "        (round(lon + half_res,3), round(lat + half_res,3)),\n",
    "        (round(lon + half_res,3), round(lat - half_res,3))\n",
    "    ])\n",
    "def tile_filename_to_coords(filename):\n",
    "    # format: n/s{dd}e/w{ddd}_elv.tif\n",
    "    # n/e: positive, s/w: negative\n",
    "    n_s, lat, e_w, lon = filename[0], int(filename[1:3]), filename[3], int(filename[4:7])\n",
    "    lat = lat if n_s == 'n' else -lat\n",
    "    lon = lon if e_w == 'e' else -lon\n",
    "    return (lon, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_percentage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [04:08<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_riserun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [04:16<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_degrees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [04:07<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope_radians\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [04:05<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [04:08<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curvature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [04:15<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "planform_curvature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [04:13<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile_curvature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [04:41<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [05:00<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [04:04<00:00,  1.62it/s]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "var_names = ['elv', 'slope_percentage', 'slope_riserun', 'slope_degrees', 'slope_radians', 'aspect', 'curvature', 'planform_curvature', 'profile_curvature', 'upa', 'wth']\n",
    "# valid_tiles = ['n30w150', 'n30w120', 'n30w090']\n",
    "\n",
    "issues = []\n",
    "for var_name in itertools.islice(var_names,1,None,1):\n",
    "    print(var_name)\n",
    "    tiles_paths = sorted(glob.glob(os.path.join(PATHS['MERIT-Hydro'], var_name, '**', '*.tif'), recursive=True))\n",
    "    # tiles_paths = [tile for tile in tiles_paths if os.path.basename(os.path.dirname(tile)).split('_')[-1] in valid_tiles]\n",
    "    tiles_filenames = [os.path.basename(tile) for tile in tiles_paths]\n",
    "    tiles_names = [tile.split('_')[0] for tile in tiles_filenames]\n",
    "    tiles_lower_left_corner = [tile_filename_to_coords(tile) for tile in tiles_filenames]\n",
    "    tiles_polygons = [Polygon([(lon, lat), (lon + 5, lat), (lon + 5, lat + 5), (lon, lat + 5)]) for lon, lat in tiles_lower_left_corner]\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = ['mean', 'std', '25%', '50%', '75%'])\n",
    "        cell_polygons = [coords_to_polygon(row['lon'], row['lat'], resolution) for _, row in nodes_coords.iterrows()]\n",
    "        catmt_polygon = cell_polygons[0]\n",
    "        for polygon in cell_polygons[1:]:\n",
    "            catmt_polygon = catmt_polygon.union(polygon)\n",
    "        intersected_tiles = []\n",
    "        for tile_polygon, tile_path in zip(tiles_polygons, tiles_paths):\n",
    "            if tile_polygon.intersects(catmt_polygon):\n",
    "                intersected_tiles.append(tile_path)\n",
    "        ds = rioxarray.open_rasterio(intersected_tiles[0])\n",
    "        for tile in intersected_tiles[1:]:\n",
    "            ds = ds.combine_first(rioxarray.open_rasterio(tile))\n",
    "        ds = ds.sel(band=1)\n",
    "        # Sort the x and y coordinates to be ascending\n",
    "        ds = ds.sortby('x', ascending=True)\n",
    "        ds = ds.sortby('y', ascending=True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            # ds_node = ds.rio.clip_box(lon - resolution/2, lat - resolution/2, lon + resolution/2, lat + resolution/2)\n",
    "            ds_node = ds.sel(x = slice(lon - resolution/2, lon + resolution/2), y = slice(lat - resolution/2, lat + resolution/2))\n",
    "            ds_node = ds_node.where(ds_node != ds.rio.nodata)\n",
    "            ds_node_values = ds_node.values.flatten()\n",
    "            mean = np.nanmean(ds_node_values)\n",
    "            std = np.nanstd(ds_node_values)\n",
    "            q25 = np.nanquantile(ds_node_values, 0.25)\n",
    "            q50 = np.nanquantile(ds_node_values, 0.50)\n",
    "            q75 = np.nanquantile(ds_node_values, 0.75)\n",
    "            data.loc['mean', node_idx] = mean\n",
    "            data.loc['std', node_idx] = std\n",
    "            data.loc['25%', node_idx] = q25\n",
    "            data.loc['50%', node_idx] = q50\n",
    "            data.loc['75%', node_idx] = q75\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'MERIT-Hydro'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'MERIT-Hydro', f\"{var_name}.csv\"))\n",
    "\n",
    "        ds.close()\n",
    "        del ds\n",
    "        gc.collect()\n",
    "\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "        try:\n",
    "            process(idx, row)\n",
    "        except Exception as e:\n",
    "            issues.append(f\"{var_name}-{row['huc_02']}-{row.name}\")\n",
    "            print(f\"Error: {var_name}-{row['huc_02']}-{row.name}. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_name</th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [var_name, huc_02, gauge_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df = [entry.split('-') for entry in issues]\n",
    "issues_df = pd.DataFrame(issues_df, columns = ['var_name', 'huc_02', 'gauge_id'])\n",
    "issues_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_name</th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [var_name, huc_02, gauge_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df[issues_df['var_name'] == 'elv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 343.82it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    # lon: -180 to 180; lat: -60 to 90\n",
    "    lon_transform = lambda x: np.sin(2 * np.pi * (x+180) / 360)\n",
    "    lat_transform = lambda x: (x - (-60))/(90 - (-60))\n",
    "\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = ['lon_transformed', 'lat_transformed'])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        data.loc['lon_transformed', node_idx] = lon_transform(lon)\n",
    "        data.loc['lat_transformed', node_idx] = lat_transform(lat)\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'spatial_encodings.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uparea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:02<00:00, 192.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uparea = xr.open_dataset(os.path.join(PATHS['gis_ldd'], 'GloFAS_03min/upstream_area_km2.nc'))\n",
    "ds_varname = list(uparea.data_vars)[0]\n",
    "uparea = uparea[ds_varname]\n",
    "uparea = uparea.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "uparea.load()\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        data.loc[0, node_idx] = uparea.sel(lat = lat, lon = lon, method = 'nearest').values.item()\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'uparea.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "uparea.close()\n",
    "del uparea\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:03<00:00, 105.21it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/395 [00:19<00:08, 44.93it/s]"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    uparea = row['area_geospa_fabric']\n",
    "\n",
    "    usgs_filepath = os.path.join(PATHS['USGS'], 'CAMELS-US', huc, 'csv', f'{gauge_id}.csv')\n",
    "    usgs_data = pd.read_csv(usgs_filepath, index_col = 0, parse_dates = True)\n",
    "    usgs_data.columns = ['Q_ft3s']\n",
    "    usgs_data['Q_mm'] = ((usgs_data['Q_ft3s'] / (3.28084**3)) / (uparea * 1e6)) * (3600*24*1000)\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    usgs_data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'USGS.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloFAS Discharge in mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [01:03<00:00,  6.22it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "\n",
    "    glofas_filepath = os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'GloFAS', 'discharge.csv')\n",
    "    glofas_data = pd.read_csv(glofas_filepath, index_col = 0, parse_dates = True)\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    uparea = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'uparea.csv'), index_col = 0)\n",
    "    glofas_Q_mm = glofas_data.copy()\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        uparea_node = uparea.loc[0, str(node_idx)] * 1e6\n",
    "        glofas_Q_mm[str(node_idx)] = (glofas_data[str(node_idx)] / uparea_node) * (3600*24*1000)\n",
    "    glofas_Q_mm.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'GloFAS', 'discharge_mm.csv'), index = True)\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloFAS Parameter Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanbnkf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 224.04it/s]\n",
      "  4%|▍         | 15/395 [01:55<48:46,  7.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanflpn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 259.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 258.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanlength\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 243.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 269.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 255.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanbw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 259.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracforest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 277.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracirrigated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 274.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracrice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 273.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracsealed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 279.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracwater\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 281.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracother\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:01<00:00, 242.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"Catchment_morphology_and_river_network\" (14 surface fields)\n",
    "# - chanbnkf_Global_03min.nc (channel bankfull depth, m);\n",
    "# - chanflpn_Global_03min.nc (width of the floodplain, m);\n",
    "# - changrad_Global_03min.nc (channel longitudinal gradient, m/m);\n",
    "# - chanlength_Global_03min.nc (channel length within a pixel, m);\n",
    "# - chanman_Global_03min.nc (channel Manning's roughness coefficient, m^(1/3)s^(-1));\n",
    "# - chans_Global_03min.nc (channel side slope, m/m);\n",
    "# - chanbw_Global_03min.nc (channel bottom width, m):\n",
    "\n",
    "# \"Land_use\" (7 surface fields)\n",
    "# - fracforest_Global_03min.nc (fraction of forest for each grid-cell, -);\n",
    "# - fracirrigated_Global_03min.nc (fraction of irrigated crops [except rice] for each grid-cell, -);\n",
    "# - fracrice_Global_03min.nc (fraction of rice crops for each grid-cell, -);\n",
    "# - fracsealed_Global_03min.nc (fraction of urban area for each grid-cell, -);\n",
    "# - fracwater_Global_03min.nc (fraction of inland water for each grid-cell, -);\n",
    "# - fracother_Global_03min.nc (fraction of other land cover for each grid-cell, -);\n",
    "Parameter_Maps = os.path.join(PATHS['GloFAS'], 'LISFLOOD_Parameter_Maps')\n",
    "\n",
    "var_names = ['chanbnkf', 'chanflpn', 'changrad', 'chanlength', 'chanman', 'chans', 'chanbw']\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Catchments_morphology_and_river_network', f\"{var_name}_Global_03min.nc\"))['Band1']\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds.load()\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "        os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()\n",
    "\n",
    "var_names = ['fracforest', 'fracirrigated', 'fracrice', 'fracsealed', 'fracwater', 'fracother']\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Land_use', f\"{var_name}_Global_03min.nc\"))['Band1']\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds.load()\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "        os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 395/395 [00:02<00:00, 174.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parameter_Maps = os.path.join(PATHS['GloFAS'], 'LISFLOOD_Parameter_Maps')\n",
    "ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Main', 'pixarea_Global_03min.nc'))['Band1'] / 1e6\n",
    "ds = ds.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "ds.load()\n",
    "var_name = 'cellarea_km2'\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "        data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "ds.close()\n",
    "del ds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
