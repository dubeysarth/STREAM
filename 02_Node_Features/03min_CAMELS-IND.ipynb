{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import networkx as nx\n",
    "\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import itertools\n",
    "import tqdm\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "cfg = configparser.ConfigParser()\n",
    "cfg.optionxform = str\n",
    "cfg.read('/data/sarth/rootdir/assets/global.ini')\n",
    "cfg = {s: dict(cfg.items(s)) for s in cfg.sections()}\n",
    "PATHS = cfg['PATHS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRNAME = '03min_GloFAS_CAMELS-IND'\n",
    "SAVE_PATH = os.path.join(PATHS['devp_datasets'], DIRNAME)\n",
    "resolution = 0.05\n",
    "lon_360_180 = lambda x: (x + 180) % 360 - 180 # convert 0-360 to -180-180\n",
    "lon_180_360 = lambda x: x % 360 # convert -180-180 to 0-360\n",
    "region_bounds = {\n",
    "    'minx': 66,\n",
    "    'miny': 5,\n",
    "    'maxx': 100,\n",
    "    'maxy': 30\n",
    "}\n",
    "# minx, miny, maxx, maxy = 66, 5, 100, 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Watershed Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>huc_02</th>\n",
       "      <th>ghi_lon</th>\n",
       "      <th>ghi_lat</th>\n",
       "      <th>ghi_area</th>\n",
       "      <th>cwc_lon</th>\n",
       "      <th>cwc_lat</th>\n",
       "      <th>cwc_area</th>\n",
       "      <th>cwc_site_name</th>\n",
       "      <th>ghi_stn_id</th>\n",
       "      <th>river_basin</th>\n",
       "      <th>cwc_river</th>\n",
       "      <th>flow_availability</th>\n",
       "      <th>snapped_lon</th>\n",
       "      <th>snapped_lat</th>\n",
       "      <th>snapped_uparea</th>\n",
       "      <th>snapped_iou</th>\n",
       "      <th>area_percent_difference</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_edges</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gauge_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14015</th>\n",
       "      <td>14</td>\n",
       "      <td>73.11033</td>\n",
       "      <td>18.73540</td>\n",
       "      <td>125.7</td>\n",
       "      <td>73.1108</td>\n",
       "      <td>18.7367</td>\n",
       "      <td>125.0</td>\n",
       "      <td>Pen</td>\n",
       "      <td>wfrn_penxx</td>\n",
       "      <td>WFRN</td>\n",
       "      <td>Bhogeswari</td>\n",
       "      <td>31.36</td>\n",
       "      <td>73.125</td>\n",
       "      <td>18.725</td>\n",
       "      <td>116.744995</td>\n",
       "      <td>0.650870</td>\n",
       "      <td>7.124106</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14009</th>\n",
       "      <td>14</td>\n",
       "      <td>73.28122</td>\n",
       "      <td>18.23122</td>\n",
       "      <td>292.3</td>\n",
       "      <td>73.2833</td>\n",
       "      <td>18.2317</td>\n",
       "      <td>259.0</td>\n",
       "      <td>Mangaon (Seasonal)</td>\n",
       "      <td>wfrn_manga</td>\n",
       "      <td>WFRN</td>\n",
       "      <td>Savitri/Kal</td>\n",
       "      <td>67.40</td>\n",
       "      <td>73.325</td>\n",
       "      <td>18.225</td>\n",
       "      <td>380.302100</td>\n",
       "      <td>0.520864</td>\n",
       "      <td>30.106777</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15006</th>\n",
       "      <td>15</td>\n",
       "      <td>74.88124</td>\n",
       "      <td>13.51876</td>\n",
       "      <td>299.6</td>\n",
       "      <td>74.8800</td>\n",
       "      <td>13.5214</td>\n",
       "      <td>253.0</td>\n",
       "      <td>Avershe</td>\n",
       "      <td>wfrs_avers</td>\n",
       "      <td>WFRS</td>\n",
       "      <td>Seetha</td>\n",
       "      <td>39.00</td>\n",
       "      <td>74.925</td>\n",
       "      <td>13.475</td>\n",
       "      <td>329.444850</td>\n",
       "      <td>0.607418</td>\n",
       "      <td>9.961565</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05025</th>\n",
       "      <td>05</td>\n",
       "      <td>78.05617</td>\n",
       "      <td>11.93959</td>\n",
       "      <td>356.0</td>\n",
       "      <td>78.0572</td>\n",
       "      <td>11.9383</td>\n",
       "      <td>362.0</td>\n",
       "      <td>Thoppur</td>\n",
       "      <td>cauv_thopp</td>\n",
       "      <td>Cauvery</td>\n",
       "      <td>Cauvery/Thoppaiyar</td>\n",
       "      <td>31.52</td>\n",
       "      <td>78.125</td>\n",
       "      <td>11.975</td>\n",
       "      <td>331.356480</td>\n",
       "      <td>0.634196</td>\n",
       "      <td>6.922338</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15032</th>\n",
       "      <td>15</td>\n",
       "      <td>74.98123</td>\n",
       "      <td>13.29791</td>\n",
       "      <td>356.9</td>\n",
       "      <td>74.9806</td>\n",
       "      <td>13.2942</td>\n",
       "      <td>327.0</td>\n",
       "      <td>Yennehole</td>\n",
       "      <td>wfrs_yenne</td>\n",
       "      <td>WFRS</td>\n",
       "      <td>Swarna</td>\n",
       "      <td>70.67</td>\n",
       "      <td>74.975</td>\n",
       "      <td>13.275</td>\n",
       "      <td>329.717100</td>\n",
       "      <td>0.755086</td>\n",
       "      <td>7.616389</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04062</th>\n",
       "      <td>04</td>\n",
       "      <td>80.06876</td>\n",
       "      <td>16.79375</td>\n",
       "      <td>240055.4</td>\n",
       "      <td>80.0692</td>\n",
       "      <td>16.7942</td>\n",
       "      <td>235544.0</td>\n",
       "      <td>Wadenapally</td>\n",
       "      <td>kris_waden</td>\n",
       "      <td>Krishna</td>\n",
       "      <td>Krishna</td>\n",
       "      <td>95.43</td>\n",
       "      <td>80.075</td>\n",
       "      <td>16.775</td>\n",
       "      <td>240186.480000</td>\n",
       "      <td>0.981840</td>\n",
       "      <td>0.054603</td>\n",
       "      <td>8131.0</td>\n",
       "      <td>8130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04060</th>\n",
       "      <td>04</td>\n",
       "      <td>80.61874</td>\n",
       "      <td>16.49796</td>\n",
       "      <td>257260.0</td>\n",
       "      <td>80.6250</td>\n",
       "      <td>16.5011</td>\n",
       "      <td>251360.0</td>\n",
       "      <td>Vijayawada</td>\n",
       "      <td>kris_vijay</td>\n",
       "      <td>Krishna</td>\n",
       "      <td>Krishna</td>\n",
       "      <td>84.90</td>\n",
       "      <td>80.625</td>\n",
       "      <td>16.475</td>\n",
       "      <td>256368.720000</td>\n",
       "      <td>0.978610</td>\n",
       "      <td>0.346452</td>\n",
       "      <td>8681.0</td>\n",
       "      <td>8680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03073</th>\n",
       "      <td>03</td>\n",
       "      <td>80.39375</td>\n",
       "      <td>18.58542</td>\n",
       "      <td>267340.4</td>\n",
       "      <td>80.3958</td>\n",
       "      <td>18.5872</td>\n",
       "      <td>268200.0</td>\n",
       "      <td>Perur</td>\n",
       "      <td>goda_perur</td>\n",
       "      <td>Godavari</td>\n",
       "      <td>Godavari</td>\n",
       "      <td>94.32</td>\n",
       "      <td>80.375</td>\n",
       "      <td>18.625</td>\n",
       "      <td>267313.380000</td>\n",
       "      <td>0.981013</td>\n",
       "      <td>0.010111</td>\n",
       "      <td>9218.0</td>\n",
       "      <td>9217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03043</th>\n",
       "      <td>03</td>\n",
       "      <td>81.38540</td>\n",
       "      <td>17.48542</td>\n",
       "      <td>304628.7</td>\n",
       "      <td>81.3875</td>\n",
       "      <td>17.4875</td>\n",
       "      <td>305460.0</td>\n",
       "      <td>Koida</td>\n",
       "      <td>goda_koida</td>\n",
       "      <td>Godavari</td>\n",
       "      <td>Godavari</td>\n",
       "      <td>64.48</td>\n",
       "      <td>81.375</td>\n",
       "      <td>17.475</td>\n",
       "      <td>304718.620000</td>\n",
       "      <td>0.983208</td>\n",
       "      <td>0.029524</td>\n",
       "      <td>10496.0</td>\n",
       "      <td>10495.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03075</th>\n",
       "      <td>03</td>\n",
       "      <td>81.66043</td>\n",
       "      <td>17.25215</td>\n",
       "      <td>306683.6</td>\n",
       "      <td>81.6564</td>\n",
       "      <td>17.2519</td>\n",
       "      <td>307800.0</td>\n",
       "      <td>Polavaram</td>\n",
       "      <td>goda_polav</td>\n",
       "      <td>Godavari</td>\n",
       "      <td>Godavari</td>\n",
       "      <td>94.03</td>\n",
       "      <td>81.675</td>\n",
       "      <td>17.275</td>\n",
       "      <td>306892.750000</td>\n",
       "      <td>0.983199</td>\n",
       "      <td>0.068199</td>\n",
       "      <td>10570.0</td>\n",
       "      <td>10569.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         huc_02   ghi_lon   ghi_lat  ghi_area  cwc_lon  cwc_lat  cwc_area  \\\n",
       "gauge_id                                                                    \n",
       "14015        14  73.11033  18.73540     125.7  73.1108  18.7367     125.0   \n",
       "14009        14  73.28122  18.23122     292.3  73.2833  18.2317     259.0   \n",
       "15006        15  74.88124  13.51876     299.6  74.8800  13.5214     253.0   \n",
       "05025        05  78.05617  11.93959     356.0  78.0572  11.9383     362.0   \n",
       "15032        15  74.98123  13.29791     356.9  74.9806  13.2942     327.0   \n",
       "...         ...       ...       ...       ...      ...      ...       ...   \n",
       "04062        04  80.06876  16.79375  240055.4  80.0692  16.7942  235544.0   \n",
       "04060        04  80.61874  16.49796  257260.0  80.6250  16.5011  251360.0   \n",
       "03073        03  80.39375  18.58542  267340.4  80.3958  18.5872  268200.0   \n",
       "03043        03  81.38540  17.48542  304628.7  81.3875  17.4875  305460.0   \n",
       "03075        03  81.66043  17.25215  306683.6  81.6564  17.2519  307800.0   \n",
       "\n",
       "               cwc_site_name  ghi_stn_id river_basin           cwc_river  \\\n",
       "gauge_id                                                                   \n",
       "14015                    Pen  wfrn_penxx        WFRN          Bhogeswari   \n",
       "14009     Mangaon (Seasonal)  wfrn_manga        WFRN         Savitri/Kal   \n",
       "15006                Avershe  wfrs_avers        WFRS              Seetha   \n",
       "05025                Thoppur  cauv_thopp     Cauvery  Cauvery/Thoppaiyar   \n",
       "15032              Yennehole  wfrs_yenne        WFRS              Swarna   \n",
       "...                      ...         ...         ...                 ...   \n",
       "04062            Wadenapally  kris_waden     Krishna             Krishna   \n",
       "04060             Vijayawada  kris_vijay     Krishna             Krishna   \n",
       "03073                  Perur  goda_perur    Godavari            Godavari   \n",
       "03043                  Koida  goda_koida    Godavari            Godavari   \n",
       "03075              Polavaram  goda_polav    Godavari            Godavari   \n",
       "\n",
       "          flow_availability  snapped_lon  snapped_lat  snapped_uparea  \\\n",
       "gauge_id                                                                \n",
       "14015                 31.36       73.125       18.725      116.744995   \n",
       "14009                 67.40       73.325       18.225      380.302100   \n",
       "15006                 39.00       74.925       13.475      329.444850   \n",
       "05025                 31.52       78.125       11.975      331.356480   \n",
       "15032                 70.67       74.975       13.275      329.717100   \n",
       "...                     ...          ...          ...             ...   \n",
       "04062                 95.43       80.075       16.775   240186.480000   \n",
       "04060                 84.90       80.625       16.475   256368.720000   \n",
       "03073                 94.32       80.375       18.625   267313.380000   \n",
       "03043                 64.48       81.375       17.475   304718.620000   \n",
       "03075                 94.03       81.675       17.275   306892.750000   \n",
       "\n",
       "          snapped_iou  area_percent_difference  num_nodes  num_edges  \n",
       "gauge_id                                                              \n",
       "14015        0.650870                 7.124106        4.0        3.0  \n",
       "14009        0.520864                30.106777       13.0       12.0  \n",
       "15006        0.607418                 9.961565       11.0       10.0  \n",
       "05025        0.634196                 6.922338       11.0       10.0  \n",
       "15032        0.755086                 7.616389       11.0       10.0  \n",
       "...               ...                      ...        ...        ...  \n",
       "04062        0.981840                 0.054603     8131.0     8130.0  \n",
       "04060        0.978610                 0.346452     8681.0     8680.0  \n",
       "03073        0.981013                 0.010111     9218.0     9217.0  \n",
       "03043        0.983208                 0.029524    10496.0    10495.0  \n",
       "03075        0.983199                 0.068199    10570.0    10569.0  \n",
       "\n",
       "[242 rows x 19 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_attributes_graph = pd.read_csv(os.path.join(SAVE_PATH, 'graph_attributes.csv'), index_col=0)\n",
    "camels_attributes_graph.index = camels_attributes_graph.index.map(lambda x: str(x).zfill(5))\n",
    "camels_attributes_graph['huc_02'] = camels_attributes_graph['huc_02'].map(lambda x: str(x).zfill(2))\n",
    "camels_attributes_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 19)\n",
      "(200, 19)\n",
      "(191, 19)\n",
      "(191, 19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "huc_02\n",
       "03    37\n",
       "04    31\n",
       "05    16\n",
       "06     4\n",
       "07     7\n",
       "08    16\n",
       "09     5\n",
       "10     5\n",
       "11     5\n",
       "12    13\n",
       "13     6\n",
       "14     8\n",
       "15    21\n",
       "16     5\n",
       "17    12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_graph = camels_attributes_graph.copy()\n",
    "print(camels_graph.shape)\n",
    "camels_graph = camels_graph[camels_graph['ghi_area'] <= 30000]\n",
    "print(camels_graph.shape)\n",
    "camels_graph = camels_graph[camels_graph['area_percent_difference'] < 10]\n",
    "print(camels_graph.shape)\n",
    "camels_graph = camels_graph[camels_graph['num_nodes'] > 1]\n",
    "print(camels_graph.shape)\n",
    "# Print the number of graphs per 'huc_02' (sorted in values of huc_02)\n",
    "camels_graph.sort_values(ascending=True, by = 'huc_02').groupby('huc_02').size()\n",
    "# camels_graph['huc_02'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      191.000000\n",
       "mean      5765.108901\n",
       "std       5827.288252\n",
       "min        125.700000\n",
       "25%       1757.100000\n",
       "50%       3442.600000\n",
       "75%       8072.400000\n",
       "max      29822.500000\n",
       "Name: ghi_area, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_graph['ghi_area'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     191.000000\n",
       "mean      197.267016\n",
       "std       200.706979\n",
       "min         4.000000\n",
       "25%        60.000000\n",
       "50%       117.000000\n",
       "75%       270.500000\n",
       "max      1034.000000\n",
       "Name: num_nodes, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "camels_graph['num_nodes'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del camels_attributes_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Node Features as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.join(SAVE_PATH, \"graph_features\"), exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldd = xr.open_dataset(os.path.join(PATHS['gis_ldd'], 'GloFAS_03min', 'ldd.nc'))\n",
    "ldd = ldd['ldd']\n",
    "ldd = ldd.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "\n",
    "lons = ldd['lon'].values\n",
    "lats = ldd['lat'].values\n",
    "\n",
    "ds_grid = xr.Dataset({\n",
    "    'lat': (['lat'], lats),\n",
    "    'lon': (['lon'], lons),\n",
    "})\n",
    "\n",
    "# Round the lat lon values to 3 decimal places in ds_grid\n",
    "ds_grid['lat'] = ds_grid['lat'].round(3)\n",
    "ds_grid['lon'] = ds_grid['lon'].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ERA5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_names = [\n",
    "#     '2m_temperature', \n",
    "#     'evaporation', \n",
    "#     'snowfall', \n",
    "#     'surface_net_solar_radiation', \n",
    "#     'surface_net_thermal_radiation', \n",
    "#     'surface_pressure', \n",
    "#     'total_precipitation',\n",
    "#     '2m_dewpoint_temperature',\n",
    "#     '10m_u_component_of_wind',\n",
    "#     '10m_v_component_of_wind',\n",
    "#     'forecast_albedo',\n",
    "#     'potential_evaporation',\n",
    "#     'runoff',\n",
    "#     'snow_albedo',\n",
    "#     'snow_depth',\n",
    "#     'snowmelt',\n",
    "#     'sub_surface_runoff',\n",
    "#     'surface_runoff',\n",
    "#     'total_column_water',\n",
    "#     'volumetric_soil_water_layer_1',\n",
    "#     'volumetric_soil_water_layer_2',\n",
    "#     'volumetric_soil_water_layer_3',\n",
    "#     'volumetric_soil_water_layer_4'\n",
    "# ]\n",
    "\n",
    "# dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "# dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "# print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "# def process(idx, row, var_name):\n",
    "#     huc, gauge_id = row['huc_02'], row.name\n",
    "#     nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "#     data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "#     os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "#     os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5'), exist_ok = True)\n",
    "#     data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "# for var_name in var_names:\n",
    "#     print(var_name)\n",
    "#     with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "#         _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "#     print(var_name)\n",
    "#     ds = xr.open_mfdataset(os.path.join(PATHS['RawData'], 'ERA5', var_name, f\"*.nc\"), combine='by_coords')\n",
    "#     ds_var_name = list(ds.data_vars)[0]\n",
    "#     ds = ds[ds_var_name]\n",
    "#     ds = ds.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
    "#     ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "#     ds['lon'] = [lon_360_180(lon) for lon in ds['lon'].values]\n",
    "#     ds = ds.sortby('lon')\n",
    "#     ds = ds.sel(\n",
    "#         lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "#         lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "#     )\n",
    "#     _, index = np.unique(ds['time'], return_index = True)\n",
    "#     ds = ds.isel(time = index)\n",
    "\n",
    "#     if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_era5_to_glofas_03min_IND.nc')):\n",
    "#         regridder = xe.Regridder(\n",
    "#             ds, \n",
    "#             ds_grid, \n",
    "#             'bilinear', \n",
    "#             reuse_weights=True, \n",
    "#             filename = os.path.join(PATHS['Assets'], 'regridder_era5_to_glofas_03min_IND.nc')\n",
    "#         )\n",
    "#     else:\n",
    "#         regridder = xe.Regridder(\n",
    "#             ds, \n",
    "#             ds_grid, \n",
    "#             'bilinear', \n",
    "#             reuse_weights=False\n",
    "#         )\n",
    "#         regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_era5_to_glofas_03min_IND.nc'))\n",
    "    \n",
    "#     ds_regrided = regridder(ds)\n",
    "#     ds.close()\n",
    "#     start_time = time.time()\n",
    "#     ds_regrided.load()\n",
    "#     end_time = time.time()\n",
    "#     print(f'Time: {((end_time - start_time) / 60):.4f} mins')\n",
    "    \n",
    "#     def process(idx, row):\n",
    "#         huc, gauge_id = row['huc_02'], row.name\n",
    "#         nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "#         data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "#         for node_idx, node_row in nodes_coords.iterrows():\n",
    "#             lat, lon = node_row['lat'], node_row['lon']\n",
    "#             ds_window_loc = ds_regrided.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "#             data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "#         data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "#     with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "#         _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "#     ds.close()\n",
    "#     del ds\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_soil_type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:32<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_high_vegetation_cover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:30<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_low_vegetation_cover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:30<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_type_of_high_vegetation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:30<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_type_of_low_vegetation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:30<00:00,  6.28it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = [\n",
    "    'static_soil_type', \n",
    "    'static_high_vegetation_cover', \n",
    "    'static_low_vegetation_cover', \n",
    "    'static_type_of_high_vegetation', \n",
    "    'static_type_of_low_vegetation'\n",
    "    ]\n",
    "ds_filenames = [\n",
    "    'soil_type_static.nc',\n",
    "    'high_vegetation_cover_static.nc',\n",
    "    'low_vegetation_cover_static.nc',\n",
    "    'type_of_high_vegetation_static.nc',\n",
    "    'type_of_low_vegetation_static.nc'\n",
    "]\n",
    "\n",
    "for var_name, ds_filename in zip(var_names, ds_filenames):\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(PATHS['RawData'], 'ERA5', ds_filename))\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.isel(time = 0)\n",
    "    ds = ds.drop('time')\n",
    "    ds = ds.rename({'longitude': 'lon', 'latitude': 'lat'})\n",
    "    ds['lon'] = [lon_360_180(lon) for lon in ds['lon'].values]\n",
    "    ds = ds.sortby('lon')\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[0, node_idx] = int(ds_window_loc.values)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'ERA5'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'ERA5', f\"{var_name}.csv\"))\n",
    "\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "        process(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HWSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_CLAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:10<00:00, 18.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_GRAVEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:09<00:00, 19.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_SAND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:09<00:00, 19.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_SILT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:09<00:00, 19.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_CLAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:09<00:00, 19.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_GRAVEL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:10<00:00, 19.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_SAND\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:09<00:00, 19.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_SILT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:09<00:00, 19.53it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['S_CLAY', 'S_GRAVEL', 'S_SAND', 'S_SILT', 'T_CLAY', 'T_GRAVEL', 'T_SAND', 'T_SILT']\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(PATHS['HWSD'], f'{var_name}.nc4'))\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['miny'], region_bounds['maxy']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds = ds / 100\n",
    "    ds.load()\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(\n",
    "                lat = slice(lat-resolution/2, lat+resolution/2),\n",
    "                lon = slice(lon-resolution/2, lon+resolution/2)\n",
    "            ).values.mean()\n",
    "            data.loc[0, node_idx] = ds_window_loc\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'HWSD'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'HWSD', f\"{var_name}.csv\"))\n",
    "\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "        process(idx, row)\n",
    "    \n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "Ep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:02<00:00, 85.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMroot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:02<00:00, 86.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMsurf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:02<00:00, 90.00it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "\n",
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep\n",
      "Time: 1.6640 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [02:03<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMroot\n",
      "Time: 1.2847 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [02:09<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMsurf\n",
      "Time: 1.3772 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [02:16<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_gleam_to_glofas_03min_IND.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder_gleam_to_glofas_03min_IND.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_gleam_to_glofas_03min_IND.nc'))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'Time: {((end_time - start_time) / 60):.4f} mins')\n",
    "    \n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds_regrided.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep (Time: 1.6138 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [02:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 6\n",
      "------\n",
      "0 15 15014\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00,  9.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 15 15014 False\n",
      "------\n",
      "1 15 15023\n",
      "Number of nodes with NaN values: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "1 15 15023 False\n",
      "------\n",
      "2 05 05013\n",
      "Number of nodes with NaN values: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "2 05 05013 False\n",
      "------\n",
      "3 15 15026\n",
      "Number of nodes with NaN values: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "3 15 15026 False\n",
      "------\n",
      "4 05 05028\n",
      "Number of nodes with NaN values: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "4 05 05028 False\n",
      "------\n",
      "5 11 11010\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "5 11 11010 False\n",
      "------\n",
      "SMroot (Time: 1.5371 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:29,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 6\n",
      "------\n",
      "0 15 15014\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 15 15014 False\n",
      "------\n",
      "1 15 15023\n",
      "Number of nodes with NaN values: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "1 15 15023 False\n",
      "------\n",
      "2 05 05013\n",
      "Number of nodes with NaN values: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 11.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "2 05 05013 False\n",
      "------\n",
      "3 15 15026\n",
      "Number of nodes with NaN values: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 10.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "3 15 15026 False\n",
      "------\n",
      "4 05 05028\n",
      "Number of nodes with NaN values: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 10.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "4 05 05028 False\n",
      "------\n",
      "5 11 11010\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "5 11 11010 False\n",
      "------\n",
      "SMsurf (Time: 1.2104 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:51,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 6\n",
      "------\n",
      "0 15 15014\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 15 15014 False\n",
      "------\n",
      "1 15 15023\n",
      "Number of nodes with NaN values: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 10.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "1 15 15023 False\n",
      "------\n",
      "2 05 05013\n",
      "Number of nodes with NaN values: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "2 05 05013 False\n",
      "------\n",
      "3 15 15026\n",
      "Number of nodes with NaN values: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "3 15 15026 False\n",
      "------\n",
      "4 05 05028\n",
      "Number of nodes with NaN values: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "4 05 05028 False\n",
      "------\n",
      "5 11 11010\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "5 11 11010 False\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "for var_name in var_names:\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder_gleam_to_glofas_03min_IND.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder_gleam_to_glofas_03min_IND.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder_gleam_to_glofas_03min_IND.nc'))\n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'{var_name} (Time: {((end_time - start_time) / 60):.4f} mins)')\n",
    "\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "                node_lat = float(round(nodes_coords.loc[node_idx, 'lat'], 3))\n",
    "                node_lon = float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "                multiplier = 1.5\n",
    "                ds_slice = ds_regrided.sel(\n",
    "                    lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                    lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                    )\n",
    "                slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "                slice_df['lat'] = slice_df['lat'].round(3)\n",
    "                slice_df['lon'] = slice_df['lon'].round(3)\n",
    "                slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "                slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "                num_nan_nodes = slice_df.isnull().any(axis=0).sum()\n",
    "                num_nonnan_nodes = len(slice_df.columns) - num_nan_nodes\n",
    "                nodes_coords.loc[node_idx, 'nonNaNneighbours'] = num_nonnan_nodes\n",
    "        nodes_coords_sorted = nodes_coords.sort_values(by = 'nonNaNneighbours', ascending = False)\n",
    "        nodes_coords_sorted = nodes_coords_sorted[nodes_coords_sorted['isNaN']]\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted.shape[0]}\")\n",
    "        \n",
    "        for node_idx in tqdm.tqdm(nodes_coords_sorted.index):\n",
    "            node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "            multiplier = 1.5\n",
    "            ds_slice = ds_regrided.sel(\n",
    "                lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                )\n",
    "            slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "            slice_df['lat'] = slice_df['lat'].round(3)\n",
    "            slice_df['lon'] = slice_df['lon'].round(3)\n",
    "            slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "            slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "            slice_df.columns = list(map(str, slice_df.columns))\n",
    "            num_nonnan_nodes = len(slice_df.columns) - slice_df.isnull().any(axis=0).sum()\n",
    "            # print(node_idx, (node_lat, node_lon), num_nonnan_nodes)\n",
    "            if num_nonnan_nodes == 9:\n",
    "                replacement_values = slice_df.loc[:, f\"({node_lat}, {node_lon})\"]\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "            elif num_nonnan_nodes > 0:\n",
    "                replacement_values = np.nanmean(slice_df, axis = 1)\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                ds_regrided.loc[dict(lat = node_lat, lon = node_lon)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted['isNaN'].sum()}\")\n",
    "        print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "for var_name in var_names:\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['Ep', 'SMroot', 'SMsurf']\n",
    "for var_name in var_names:\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "\n",
    "        \n",
    "        for node_idx in tqdm.tqdm(nodes_coords[nodes_coords['isNaN']].index):\n",
    "            nodes_coords['distances'] = None\n",
    "            node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "            for node_idx2 in nodes_coords[nodes_coords['isNaN'] == False].index:\n",
    "                if node_idx != node_idx2:\n",
    "                    node_lat2, node_lon2 = float(round(nodes_coords.loc[node_idx2, 'lat'], 3)), float(round(nodes_coords.loc[node_idx2, 'lon'], 3))\n",
    "                    distance = np.sqrt((node_lat - node_lat2)**2 + (node_lon - node_lon2)**2)\n",
    "                    nodes_coords.loc[node_idx2, 'distances'] = distance\n",
    "            min_distance = nodes_coords.loc[nodes_coords['distances'].idxmin(), 'distances']\n",
    "            # Replace with mean of nodes having distance equal to min_distance\n",
    "            replacement_nodes = nodes_coords[nodes_coords['distances'] == min_distance].index\n",
    "            replacement_nodes = list(map(str, replacement_nodes))\n",
    "            replacement_values = data.loc[:, replacement_nodes].mean(axis = 1)\n",
    "            data.loc[:, str(node_idx)] = replacement_values\n",
    "            nodes_coords.loc[node_idx, 'isNaN'] = False\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "        print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM', f\"{var_name}.csv\"))\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLEAM4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "Ep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:04<00:00, 44.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMrz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:02<00:00, 91.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:02<00:00, 90.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:05<00:00, 35.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ei\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:02<00:00, 90.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:02<00:00, 89.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:02<00:00, 92.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:02<00:00, 91.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:02<00:00, 91.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:02<00:00, 92.03it/s] \n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMrz', 'SMs', 'Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "# var_names = ['Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "\n",
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep\n",
      "Time: 1.8767 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [01:46<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMrz\n",
      "Time: 1.2634 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [01:49<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMs\n",
      "Time: 1.2806 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [01:48<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eb\n",
      "Time: 1.3667 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [01:47<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ei\n",
      "Time: 1.2192 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [01:18<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es\n",
      "Time: 0.7418 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:34<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Et\n",
      "Time: 1.3326 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [01:49<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ew\n",
      "Time: 1.3358 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [01:51<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "Time: 1.3433 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [01:44<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "Time: 1.3044 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [01:46<00:00,  1.79it/s]\n"
     ]
    }
   ],
   "source": [
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], 'GLEAM4.2a', var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder', 'regridder_gleam4_to_glofas_03min_IND.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder', 'regridder_gleam4_to_glofas_03min_IND.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder', 'regridder_gleam4_to_glofas_03min_IND.nc'))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'Time: {((end_time - start_time) / 60):.4f} mins')\n",
    "    \n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds_regrided.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep (Time: 1.2970 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:08,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n",
      "SMrz (Time: 1.2745 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:09,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 10\n",
      "------\n",
      "0 15 15006\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 15 15006 False\n",
      "------\n",
      "1 15 15014\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "1 15 15014 False\n",
      "------\n",
      "2 14 14005\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "2 14 14005 False\n",
      "------\n",
      "3 15 15023\n",
      "Number of nodes with NaN values: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "3 15 15023 False\n",
      "------\n",
      "4 05 05013\n",
      "Number of nodes with NaN values: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "4 05 05013 False\n",
      "------\n",
      "5 15 15026\n",
      "Number of nodes with NaN values: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n",
      "5 15 15026 True\n",
      "------\n",
      "6 14 14003\n",
      "Number of nodes with NaN values: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "6 14 14003 False\n",
      "------\n",
      "7 05 05028\n",
      "Number of nodes with NaN values: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 11.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "7 05 05028 False\n",
      "------\n",
      "8 16 16014\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "8 16 16014 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "9 11 11010\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "9 11 11010 False\n",
      "------\n",
      "SMs (Time: 1.2240 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:07,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 10\n",
      "------\n",
      "0 15 15006\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 15 15006 False\n",
      "------\n",
      "1 15 15014\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "1 15 15014 False\n",
      "------\n",
      "2 14 14005\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "2 14 14005 False\n",
      "------\n",
      "3 15 15023\n",
      "Number of nodes with NaN values: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "3 15 15023 False\n",
      "------\n",
      "4 05 05013\n",
      "Number of nodes with NaN values: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 11.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "4 05 05013 False\n",
      "------\n",
      "5 15 15026\n",
      "Number of nodes with NaN values: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 1\n",
      "5 15 15026 True\n",
      "------\n",
      "6 14 14003\n",
      "Number of nodes with NaN values: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "6 14 14003 False\n",
      "------\n",
      "7 05 05028\n",
      "Number of nodes with NaN values: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 11.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "7 05 05028 False\n",
      "------\n",
      "8 16 16014\n",
      "Number of nodes with NaN values: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 11.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "8 16 16014 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "9 11 11010\n",
      "Number of nodes with NaN values: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 11.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "9 11 11010 False\n",
      "------\n",
      "Eb (Time: 1.3143 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:11,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n",
      "Ei (Time: 1.1882 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [00:55,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n",
      "Es (Time: 0.7126 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [00:23,  8.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n",
      "Et (Time: 1.3268 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:12,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n",
      "Ew (Time: 1.3153 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:14,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n",
      "S (Time: 1.3380 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:11,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n",
      "H (Time: 1.3112 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:10,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMrz', 'SMs', 'Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "# var_names = ['Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "for var_name in var_names:\n",
    "    ds = xr.open_mfdataset(os.path.join(PATHS['GLEAM'], 'GLEAM4.2a', var_name, f\"*.nc\"), combine='by_coords')\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder', 'regridder_gleam4_to_glofas_03min_IND.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder', 'regridder_gleam4_to_glofas_03min_IND.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder', 'regridder_gleam4_to_glofas_03min_IND.nc'))\n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "    start_time = time.time()\n",
    "    ds_regrided.load()\n",
    "    end_time = time.time()\n",
    "    print(f'{var_name} (Time: {((end_time - start_time) / 60):.4f} mins)')\n",
    "\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "                node_lat = float(round(nodes_coords.loc[node_idx, 'lat'], 3))\n",
    "                node_lon = float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "                multiplier = 1.5\n",
    "                ds_slice = ds_regrided.sel(\n",
    "                    lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                    lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                    )\n",
    "                slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "                slice_df['lat'] = slice_df['lat'].round(3)\n",
    "                slice_df['lon'] = slice_df['lon'].round(3)\n",
    "                slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "                slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "                num_nan_nodes = slice_df.isnull().any(axis=0).sum()\n",
    "                num_nonnan_nodes = len(slice_df.columns) - num_nan_nodes\n",
    "                nodes_coords.loc[node_idx, 'nonNaNneighbours'] = num_nonnan_nodes\n",
    "        nodes_coords_sorted = nodes_coords.sort_values(by = 'nonNaNneighbours', ascending = False)\n",
    "        nodes_coords_sorted = nodes_coords_sorted[nodes_coords_sorted['isNaN']]\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted.shape[0]}\")\n",
    "        \n",
    "        for node_idx in tqdm.tqdm(nodes_coords_sorted.index):\n",
    "            node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "            multiplier = 1.5\n",
    "            ds_slice = ds_regrided.sel(\n",
    "                lat = slice(node_lat+multiplier*resolution, node_lat-multiplier*resolution), \n",
    "                lon = slice(node_lon-multiplier*resolution, node_lon+multiplier*resolution)\n",
    "                )\n",
    "            slice_df = ds_slice.to_dataframe(name = var_name).reset_index()\n",
    "            slice_df['lat'] = slice_df['lat'].round(3)\n",
    "            slice_df['lon'] = slice_df['lon'].round(3)\n",
    "            slice_df['location'] = list(zip(slice_df['lat'], slice_df['lon']))\n",
    "            slice_df = slice_df.pivot(index='time', columns='location', values=var_name)\n",
    "            slice_df.columns = list(map(str, slice_df.columns))\n",
    "            num_nonnan_nodes = len(slice_df.columns) - slice_df.isnull().any(axis=0).sum()\n",
    "            # print(node_idx, (node_lat, node_lon), num_nonnan_nodes)\n",
    "            if num_nonnan_nodes == 9:\n",
    "                replacement_values = slice_df.loc[:, f\"({node_lat}, {node_lon})\"]\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "            elif num_nonnan_nodes > 0:\n",
    "                replacement_values = np.nanmean(slice_df, axis = 1)\n",
    "                data.loc[:, str(node_idx)] = replacement_values\n",
    "                ds_regrided.loc[dict(lat = node_lat, lon = node_lon)] = replacement_values\n",
    "                nodes_coords_sorted.loc[node_idx, 'isNaN'] = False\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords_sorted['isNaN'].sum()}\")\n",
    "        print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"))\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['Ep', 'SMrz', 'SMs', 'Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "# var_names = ['Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "for var_name in var_names:\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:02,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:03,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 1\n",
      "------\n",
      "0 15 15026\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 87.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 15 15026 False\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:04,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 1\n",
      "------\n",
      "0 15 15026\n",
      "Number of nodes with NaN values: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 89.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes with NaN values: 0\n",
      "0 15 15026 False\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:04,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [00:45,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [00:21,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:06,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:05,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [01:02,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "var_names = ['Ep', 'SMrz', 'SMs', 'Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "# var_names = ['Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "for var_name in var_names:\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        nodes_coords['isNaN'] = False\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "\n",
    "        \n",
    "        for node_idx in tqdm.tqdm(nodes_coords[nodes_coords['isNaN']].index):\n",
    "            nodes_coords['distances'] = None\n",
    "            node_lat, node_lon = float(round(nodes_coords.loc[node_idx, 'lat'], 3)), float(round(nodes_coords.loc[node_idx, 'lon'], 3))\n",
    "            for node_idx2 in nodes_coords[nodes_coords['isNaN'] == False].index:\n",
    "                if node_idx != node_idx2:\n",
    "                    node_lat2, node_lon2 = float(round(nodes_coords.loc[node_idx2, 'lat'], 3)), float(round(nodes_coords.loc[node_idx2, 'lon'], 3))\n",
    "                    distance = np.sqrt((node_lat - node_lat2)**2 + (node_lon - node_lon2)**2)\n",
    "                    nodes_coords.loc[node_idx2, 'distances'] = distance\n",
    "            min_distance = nodes_coords.loc[nodes_coords['distances'].idxmin(), 'distances']\n",
    "            # Replace with mean of nodes having distance equal to min_distance\n",
    "            replacement_nodes = nodes_coords[nodes_coords['distances'] == min_distance].index\n",
    "            replacement_nodes = list(map(str, replacement_nodes))\n",
    "            replacement_values = data.loc[:, replacement_nodes].mean(axis = 1)\n",
    "            data.loc[:, str(node_idx)] = replacement_values\n",
    "            nodes_coords.loc[node_idx, 'isNaN'] = False\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "        print(issue_idx, huc, gauge_id, data.isnull().values.any())\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GLEAM4', f\"{var_name}.csv\"))\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dates: 14965\n",
      "precipitation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:06<00:00, 28.18it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['precipitation']\n",
    "# var_names = ['Eb', 'Ei', 'Es', 'Et', 'Ew', 'S', 'H']\n",
    "\n",
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "print(f\"Number of dates: {len(dates)}\")\n",
    "\n",
    "def process(idx, row, var_name):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(index = dates, columns = nodes_coords.index)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic'), exist_ok = True)\n",
    "    os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"))\n",
    "\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row, var_name) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precipitation\n",
      "1998-01-01 2002-12-31 Time: 8.52 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:24<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003-01-01 2007-12-31 Time: 3.47 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:27<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008-01-01 2012-12-31 Time: 3.44 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:33<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-01-01 2017-12-31 Time: 3.27 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:39<00:00,  4.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-01 2020-12-31 Time: 2.76 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:42<00:00,  4.54it/s]\n"
     ]
    }
   ],
   "source": [
    "var_names = ['precipitation']\n",
    "for var_name in itertools.islice(var_names, 0, None, 1):\n",
    "    print(var_name)\n",
    "    ds = xr.open_zarr(os.path.join(PATHS['GPM'], 'GPM_1998_2020.zarr'), consolidated=True)\n",
    "    ds_var_name = list(ds.data_vars)[0]\n",
    "    ds = ds[ds_var_name]\n",
    "    ds = ds.sel(time=~((ds['time.month'] == 2) & (ds['time.day'] == 29)))\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['miny'], region_bounds['maxy']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "\n",
    "    missing_date = np.datetime64('1999-09-03')\n",
    "    # Reindex the dataset to include the missing date (it will be filled with NaN)\n",
    "    new_times = np.sort(np.append(ds.time.values, missing_date))\n",
    "    ds = ds.reindex(time=new_times).chunk({'time': -1})\n",
    "    ds = ds.sortby('time')\n",
    "    # # Interpolate in time to fill NaN values using linear interpolation\n",
    "    # ds = ds.interpolate_na(dim='time', method='linear')\n",
    "    # # Chunk in a way that makes it faster\n",
    "    ds = ds.chunk({'time': 1, 'lat': -1, 'lon': -1})\n",
    "\n",
    "    if os.path.exists(os.path.join(PATHS['Assets'], 'regridder', 'regridder_gpm_to_glofas_03min_IND.nc')):\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=True, \n",
    "            filename = os.path.join(PATHS['Assets'], 'regridder', 'regridder_gpm_to_glofas_03min_IND.nc')\n",
    "        )\n",
    "    else:\n",
    "        regridder = xe.Regridder(\n",
    "            ds, \n",
    "            ds_grid, \n",
    "            'bilinear', \n",
    "            reuse_weights=False\n",
    "        )\n",
    "        regridder.to_netcdf(os.path.join(PATHS['Assets'], 'regridder', 'regridder_gpm_to_glofas_03min_IND.nc'))\n",
    "    \n",
    "    ds_regrided = regridder(ds)\n",
    "    ds.close()\n",
    "\n",
    "    for start_year in range(1998, 2020+1, 5):\n",
    "        start_date = f\"{start_year}-01-01\"\n",
    "        end_date = f\"{min(start_year+4,2020)}-12-31\"\n",
    "        ds_window = ds_regrided.sel(time = slice(start_date, end_date)).copy()\n",
    "        start_time = time.time()\n",
    "        ds_window.load()\n",
    "        end_time = time.time()\n",
    "        print(start_date, end_date, f\"Time: {(end_time - start_time)/60:.2f} mins\")\n",
    "    \n",
    "        def process(idx, row):\n",
    "            huc, gauge_id = row['huc_02'], row.name\n",
    "            nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "            data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "            for node_idx, node_row in nodes_coords.iterrows():\n",
    "                lat, lon = node_row['lat'], node_row['lon']\n",
    "                ds_window_loc = ds_window.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "                # data.loc[:, str(node_idx)] = ds_window_loc.values\n",
    "                data.loc[start_date:end_date, str(node_idx)] = ds_window_loc.values\n",
    "            data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"))\n",
    "\n",
    "        with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "            _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "        # ds.close()\n",
    "        # del ds\n",
    "        # gc.collect()\n",
    "    ds_regrided.close()\n",
    "    del ds_regrided, ds\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:38<00:00,  5.01it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "\n",
    "    # Interpolate using a window of 15 days, centered\n",
    "    missing_date = np.datetime64('1999-09-03')\n",
    "    window = pd.Timedelta(days=1)\n",
    "    start_date = missing_date - window\n",
    "    end_date = missing_date + window\n",
    "    data_window = data.loc[start_date:end_date]\n",
    "    data.loc[missing_date] = data_window.mean(axis=0)\n",
    "\n",
    "    data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191it [00:32,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of catchments with issues: 0\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "var_names = ['precipitation']\n",
    "for var_name in var_names:\n",
    "    # Loop over catchments and find ones with issues\n",
    "    issues = []\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows()):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        # Only consider from 1998 onwards\n",
    "        data = data.loc['1998-01-01':]\n",
    "        if data.isnull().values.any():\n",
    "            issues.append([huc, gauge_id])\n",
    "    issues = pd.DataFrame(issues, columns = ['huc_02', 'gauge_id'])\n",
    "    print(f\"Number of catchments with issues: {issues.shape[0]}\")\n",
    "    print(\"------\")\n",
    "\n",
    "    # Fix the catchments with issues\n",
    "    for issue_idx, (huc, gauge_id) in enumerate(issues.values):\n",
    "        print(issue_idx, huc, gauge_id)\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.read_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'dynamic', 'GPM', f\"{var_name}.csv\"), index_col = 0, parse_dates = True)\n",
    "        data = data.loc['1998-01-01':]\n",
    "        nodes_coords['isNaN'] = False\n",
    "        nodes_coords['nonNaNneighbours'] = 0\n",
    "        # Loop over nodes and find the nodes with issues\n",
    "        for node_idx in nodes_coords.index:\n",
    "            if data[str(node_idx)].isnull().values.any():\n",
    "                nodes_coords.loc[node_idx, 'isNaN'] = True\n",
    "        print(f\"Number of nodes with NaN values: {nodes_coords['isNaN'].sum()}\")\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solar Insolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solar_insolation(lat, lon, start_date, end_date):\n",
    "    # Constants\n",
    "    Sc = 1361  # Solar constant (W/m^2)\n",
    "    \n",
    "    # Convert dates to datetime objects\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    \n",
    "    # Generate date range\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "    \n",
    "    # Function to calculate solar declination\n",
    "    def solar_declination(n):\n",
    "        return 23.45 * np.sin(np.radians((360 / 365) * (n - 81)))\n",
    "\n",
    "    # Function to calculate cos(theta_z) for solar zenith angle\n",
    "    def cos_theta_z(lat, decl, hour_angle):\n",
    "        lat_rad = np.radians(lat)\n",
    "        decl_rad = np.radians(decl)\n",
    "        return (np.sin(lat_rad) * np.sin(decl_rad) + \n",
    "                np.cos(lat_rad) * np.cos(decl_rad) * np.cos(np.radians(hour_angle)))\n",
    "    \n",
    "    # Function to calculate the hour angle\n",
    "    def hour_angle(lon, date):\n",
    "        # Assuming solar noon (local solar time = 12 hours)\n",
    "        return 0  # hour angle at solar noon\n",
    "    \n",
    "    # Calculate solar insolation for each day\n",
    "    insolation_values = []\n",
    "    for date in dates:\n",
    "        day_of_year = date.day_of_year\n",
    "        declination = solar_declination(day_of_year)\n",
    "        h = hour_angle(lon, date)\n",
    "        cos_zenith_angle = cos_theta_z(lat, declination, h)\n",
    "        \n",
    "        # Insolation formula\n",
    "        insolation = Sc * (1 + 0.033 * np.cos(np.radians(360 * day_of_year / 365))) * cos_zenith_angle\n",
    "        \n",
    "        # Make sure insolation is non-negative\n",
    "        insolation = max(insolation, 0)\n",
    "        insolation_values.append(insolation)\n",
    "    \n",
    "    # Create pandas Series\n",
    "    insolation_series = pd.Series(insolation_values, index=dates, name='Solar Insolation (kW/m²)')\n",
    "    insolation_series = insolation_series / 1000  # Convert to kW/m²\n",
    "    \n",
    "    return insolation_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [09:17<00:00,  2.92s/it]\n"
     ]
    }
   ],
   "source": [
    "dates = pd.date_range('1980-01-01', '2020-12-31', freq='D')\n",
    "dates = dates[~((dates.month == 2) & (dates.day == 29))]\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = dates)\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        ds_window_loc = solar_insolation(lat, lon, '1980-01-01', '2020-12-31')\n",
    "        data.loc[:, node_idx] = ds_window_loc.values\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'solar_insolation.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sine_time_encoding(start_date, end_date):\n",
    "    # (a) Create a date_range and remove leap days\n",
    "    dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "    dates = dates[~((dates.month == 2) & (dates.day == 29))]  # Remove February 29 (leap days)\n",
    "    \n",
    "    # (b) Create a dataframe with 'month', 'weekofyear', 'dayofyear' columns\n",
    "    df = pd.DataFrame(index=dates)\n",
    "    df['month'] = df.index.month\n",
    "    df['weekofyear'] = df.index.isocalendar().week\n",
    "    df['dayofyear'] = df.index.dayofyear\n",
    "    \n",
    "    # (c) Define lambda transformations for sine encoding\n",
    "    # For day of year (range 1-365), week of year (range 1-52), and month (range 1-12)\n",
    "    sine_transform = lambda x, max_val: np.sin(2 * np.pi * x / max_val)\n",
    "    \n",
    "    # (d) Apply sine transformation and add transformed columns\n",
    "    df['sine_month'] = df['month'].apply(sine_transform, max_val=12)\n",
    "    df['sine_weekofyear'] = df['weekofyear'].apply(sine_transform, max_val=52)\n",
    "    df['sine_dayofyear'] = df['dayofyear'].apply(sine_transform, max_val=365)\n",
    "    \n",
    "    # return df[['sine_month', 'sine_weekofyear', 'sine_dayofyear']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:18<00:00, 10.18it/s]\n"
     ]
    }
   ],
   "source": [
    "df_encoded = sine_time_encoding('1980-01-01', '2020-12-31')\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    df_encoded.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'time_encodings.csv'))\n",
    "\n",
    "# with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    # _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "    process(idx, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terrain Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import rioxarray\n",
    "\n",
    "def coords_to_polygon(lon, lat, resolution):\n",
    "    half_res = resolution / 2\n",
    "    return Polygon([\n",
    "        (round(lon - half_res,3), round(lat - half_res,3)),\n",
    "        (round(lon - half_res,3), round(lat + half_res,3)),\n",
    "        (round(lon + half_res,3), round(lat + half_res,3)),\n",
    "        (round(lon + half_res,3), round(lat - half_res,3))\n",
    "    ])\n",
    "def tile_filename_to_coords(filename):\n",
    "    # format: n/s{dd}e/w{ddd}_elv.tif\n",
    "    # n/e: positive, s/w: negative\n",
    "    n_s, lat, e_w, lon = filename[0], int(filename[1:3]), filename[3], int(filename[4:7])\n",
    "    lat = lat if n_s == 'n' else -lat\n",
    "    lon = lon if e_w == 'e' else -lon\n",
    "    return (lon, lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [04:01<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "var_names = ['elv', 'slope_percentage', 'slope_riserun', 'slope_degrees', 'slope_radians', 'aspect', 'curvature', 'planform_curvature', 'profile_curvature', 'upa', 'wth']\n",
    "# valid_tiles = ['n30w150', 'n30w120', 'n30w090']\n",
    "\n",
    "issues = []\n",
    "for var_name in itertools.islice(var_names,0,None,1):\n",
    "    print(var_name)\n",
    "    tiles_paths = sorted(glob.glob(os.path.join(PATHS['MERIT-Hydro'], var_name, '**', '*.tif'), recursive=True))\n",
    "    # tiles_paths = [tile for tile in tiles_paths if os.path.basename(os.path.dirname(tile)).split('_')[-1] in valid_tiles]\n",
    "    tiles_filenames = [os.path.basename(tile) for tile in tiles_paths]\n",
    "    tiles_names = [tile.split('_')[0] for tile in tiles_filenames]\n",
    "    tiles_lower_left_corner = [tile_filename_to_coords(tile) for tile in tiles_filenames]\n",
    "    tiles_polygons = [Polygon([(lon, lat), (lon + 5, lat), (lon + 5, lat + 5), (lon, lat + 5)]) for lon, lat in tiles_lower_left_corner]\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(SAVE_PATH, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = ['mean', 'std', '25%', '50%', '75%'])\n",
    "        cell_polygons = [coords_to_polygon(row['lon'], row['lat'], resolution) for _, row in nodes_coords.iterrows()]\n",
    "        catmt_polygon = cell_polygons[0]\n",
    "        for polygon in cell_polygons[1:]:\n",
    "            catmt_polygon = catmt_polygon.union(polygon)\n",
    "        intersected_tiles = []\n",
    "        for tile_polygon, tile_path in zip(tiles_polygons, tiles_paths):\n",
    "            if tile_polygon.intersects(catmt_polygon):\n",
    "                intersected_tiles.append(tile_path)\n",
    "        ds = rioxarray.open_rasterio(intersected_tiles[0])\n",
    "        for tile in intersected_tiles[1:]:\n",
    "            ds = ds.combine_first(rioxarray.open_rasterio(tile))\n",
    "        ds = ds.sel(band=1)\n",
    "        # Sort the x and y coordinates to be ascending\n",
    "        ds = ds.sortby('x', ascending=True)\n",
    "        ds = ds.sortby('y', ascending=True)\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            # ds_node = ds.rio.clip_box(lon - resolution/2, lat - resolution/2, lon + resolution/2, lat + resolution/2)\n",
    "            ds_node = ds.sel(x = slice(lon - resolution/2, lon + resolution/2), y = slice(lat - resolution/2, lat + resolution/2))\n",
    "            ds_node = ds_node.where(ds_node != ds.rio.nodata)\n",
    "            ds_node_values = ds_node.values.flatten()\n",
    "            mean = np.nanmean(ds_node_values)\n",
    "            std = np.nanstd(ds_node_values)\n",
    "            q25 = np.nanquantile(ds_node_values, 0.25)\n",
    "            q50 = np.nanquantile(ds_node_values, 0.50)\n",
    "            q75 = np.nanquantile(ds_node_values, 0.75)\n",
    "            data.loc['mean', node_idx] = mean\n",
    "            data.loc['std', node_idx] = std\n",
    "            data.loc['25%', node_idx] = q25\n",
    "            data.loc['50%', node_idx] = q50\n",
    "            data.loc['75%', node_idx] = q75\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static'), exist_ok = True)\n",
    "        os.makedirs(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'MERIT-Hydro'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(SAVE_PATH, \"graph_features\", huc, gauge_id, 'static', 'MERIT-Hydro', f\"{var_name}.csv\"))\n",
    "\n",
    "        ds.close()\n",
    "        del ds\n",
    "        gc.collect()\n",
    "\n",
    "    for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)):\n",
    "        try:\n",
    "            process(idx, row)\n",
    "        except Exception as e:\n",
    "            issues.append(f\"{var_name}-{row['huc_02']}-{row.name}\")\n",
    "            print(f\"Error: {var_name}-{row['huc_02']}-{row.name}. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_name</th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [var_name, huc_02, gauge_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df = [entry.split('-') for entry in issues]\n",
    "issues_df = pd.DataFrame(issues_df, columns = ['var_name', 'huc_02', 'gauge_id'])\n",
    "issues_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_name</th>\n",
       "      <th>huc_02</th>\n",
       "      <th>gauge_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [var_name, huc_02, gauge_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues_df[issues_df['var_name'] == 'elv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 299.81it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    # lon: -180 to 180; lat: -60 to 90\n",
    "    lon_transform = lambda x: np.sin(2 * np.pi * (x+180) / 360)\n",
    "    lat_transform = lambda x: (x - (-60))/(90 - (-60))\n",
    "\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = ['lon_transformed', 'lat_transformed'])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        data.loc['lon_transformed', node_idx] = lon_transform(lon)\n",
    "        data.loc['lat_transformed', node_idx] = lat_transform(lat)\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'spatial_encodings.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uparea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:01<00:00, 143.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uparea = xr.open_dataset(os.path.join(PATHS['gis_ldd'], 'GloFAS_03min/upstream_area_km2.nc'))\n",
    "ds_varname = list(uparea.data_vars)[0]\n",
    "uparea = uparea[ds_varname]\n",
    "uparea = uparea.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "uparea.load()\n",
    "\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        data.loc[0, node_idx] = uparea.sel(lat = lat, lon = lon, method = 'nearest').values.item()\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'uparea.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "uparea.close()\n",
    "del uparea\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndiaWRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:06<00:00, 28.11it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    uparea = row['ghi_area']\n",
    "\n",
    "    streamflow = pd.read_csv(os.path.join(PATHS['CAMELS'], 'CAMELS-IND', 'CAMELS_IND_Catchments_Streamflow_Sufficient', 'streamflow_timeseries', 'streamflow_observed.csv'))\n",
    "    streamflow['date'] = pd.to_datetime(streamflow[['year', 'month', 'day']])\n",
    "    streamflow = streamflow.set_index('date')\n",
    "    streamflow = streamflow.drop(columns=['year', 'month', 'day'])\n",
    "    streamflow = streamflow[~((streamflow.index.month == 2) & (streamflow.index.day == 29))]\n",
    "    streamflow.columns = streamflow.columns.astype(str)\n",
    "    streamflow.columns = streamflow.columns.str.zfill(5)\n",
    "    streamflow = streamflow[[gauge_id]]\n",
    "    streamflow.columns = ['Q_m3s']\n",
    "    streamflow['Q_mm'] = (streamflow['Q_m3s'] / (uparea * 1e6)) * (3600*24*1000)\n",
    "\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id), exist_ok = True)\n",
    "    streamflow.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, f'IndiaWRIS.csv'))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloFAS Parameter Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanbnkf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:01<00:00, 146.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanflpn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 360.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changrad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 356.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanlength\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 353.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 359.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 200.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chanbw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 361.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracforest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 359.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracirrigated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 354.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracrice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 349.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracsealed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 351.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracwater\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 350.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fracother\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 357.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"Catchment_morphology_and_river_network\" (14 surface fields)\n",
    "# - chanbnkf_Global_03min.nc (channel bankfull depth, m);\n",
    "# - chanflpn_Global_03min.nc (width of the floodplain, m);\n",
    "# - changrad_Global_03min.nc (channel longitudinal gradient, m/m);\n",
    "# - chanlength_Global_03min.nc (channel length within a pixel, m);\n",
    "# - chanman_Global_03min.nc (channel Manning's roughness coefficient, m^(1/3)s^(-1));\n",
    "# - chans_Global_03min.nc (channel side slope, m/m);\n",
    "# - chanbw_Global_03min.nc (channel bottom width, m):\n",
    "\n",
    "# \"Land_use\" (7 surface fields)\n",
    "# - fracforest_Global_03min.nc (fraction of forest for each grid-cell, -);\n",
    "# - fracirrigated_Global_03min.nc (fraction of irrigated crops [except rice] for each grid-cell, -);\n",
    "# - fracrice_Global_03min.nc (fraction of rice crops for each grid-cell, -);\n",
    "# - fracsealed_Global_03min.nc (fraction of urban area for each grid-cell, -);\n",
    "# - fracwater_Global_03min.nc (fraction of inland water for each grid-cell, -);\n",
    "# - fracother_Global_03min.nc (fraction of other land cover for each grid-cell, -);\n",
    "Parameter_Maps = os.path.join(PATHS['GloFAS'], 'LISFLOOD_Parameter_Maps')\n",
    "\n",
    "var_names = ['chanbnkf', 'chanflpn', 'changrad', 'chanlength', 'chanman', 'chans', 'chanbw']\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Catchments_morphology_and_river_network', f\"{var_name}_Global_03min.nc\"))['Band1']\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds.load()\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "        os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "    ds.close()\n",
    "    del ds\n",
    "    gc.collect()\n",
    "\n",
    "var_names = ['fracforest', 'fracirrigated', 'fracrice', 'fracsealed', 'fracwater', 'fracother']\n",
    "for var_name in var_names:\n",
    "    print(var_name)\n",
    "    ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Land_use', f\"{var_name}_Global_03min.nc\"))['Band1']\n",
    "    ds = ds.sel(\n",
    "        lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "        lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    "    )\n",
    "    ds.load()\n",
    "\n",
    "    def process(idx, row):\n",
    "        huc, gauge_id = row['huc_02'], row.name\n",
    "        nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "        data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "        for node_idx, node_row in nodes_coords.iterrows():\n",
    "            lat, lon = node_row['lat'], node_row['lon']\n",
    "            ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "            data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "        os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "        data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "    with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "        _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [00:00<00:00, 336.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Parameter_Maps = os.path.join(PATHS['GloFAS'], 'LISFLOOD_Parameter_Maps')\n",
    "ds = xr.open_dataset(os.path.join(Parameter_Maps, 'Main', 'pixarea_Global_03min.nc'))['Band1'] / 1e6\n",
    "ds = ds.sel(\n",
    "    lat = slice(region_bounds['maxy'], region_bounds['miny']), \n",
    "    lon = slice(region_bounds['minx'], region_bounds['maxx'])\n",
    ")\n",
    "ds.load()\n",
    "var_name = 'cellarea_km2'\n",
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    data = pd.DataFrame(columns = nodes_coords.index, index = [0])\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        lat, lon = node_row['lat'], node_row['lon']\n",
    "        ds_window_loc = ds.sel(lat = lat, lon = lon, method = 'nearest')\n",
    "        data.loc[0, node_idx] = ds_window_loc.values.item()\n",
    "    os.makedirs(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS'), exist_ok = True)\n",
    "    data.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"{var_name}.csv\"))\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))\n",
    "\n",
    "ds.close()\n",
    "del ds\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloFAS Discharge in mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [01:22<00:00,  2.31it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "\n",
    "    glofas_filepath = os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'GloFAS', 'discharge.csv')\n",
    "    glofas_data = pd.read_csv(glofas_filepath, index_col = 0, parse_dates = True)\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    uparea = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'uparea.csv'), index_col = 0)\n",
    "    glofas_Q_mm = glofas_data.copy()\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        uparea_node = uparea.loc[0, str(node_idx)] * 1e6\n",
    "        glofas_Q_mm[str(node_idx)] = (glofas_data[str(node_idx)] / uparea_node) * (3600*24*1000)\n",
    "    glofas_Q_mm.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'GloFAS', 'discharge_mm.csv'), index = True)\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloFAS Runoff in m3s and mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 191/191 [03:11<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "def process(idx, row):\n",
    "    huc, gauge_id = row['huc_02'], row.name\n",
    "\n",
    "    glofas_filepath = os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'GloFAS', 'runoff_water_equivalent.csv')\n",
    "    glofas_data = pd.read_csv(glofas_filepath, index_col = 0, parse_dates = True)\n",
    "    nodes_coords = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_files', huc, gauge_id, 'nodes_coords.csv'), index_col = 0)\n",
    "    cellarea = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'static', 'GloFAS', f\"cellarea_km2.csv\"), index_col = 0)\n",
    "    uparea = pd.read_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'uparea.csv'), index_col = 0)\n",
    "    glofas_Q_m3s = glofas_data.copy()\n",
    "    glofas_Q_mm = glofas_data.copy()\n",
    "    for node_idx, node_row in nodes_coords.iterrows():\n",
    "        cellarea_node = cellarea.loc[0, str(node_idx)]\n",
    "        uparea_node = cellarea.loc[0, str(node_idx)]\n",
    "        glofas_Q_m3s[str(node_idx)] = (glofas_data[str(node_idx)] * cellarea_node) / 86.4\n",
    "        glofas_Q_mm[str(node_idx)] = (glofas_data[str(node_idx)] * cellarea_node) / uparea_node\n",
    "    glofas_Q_m3s.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'GloFAS', 'runoff_water_equivalent_m3s.csv'), index = True)\n",
    "    glofas_Q_mm.to_csv(os.path.join(PATHS['devp_datasets'], DIRNAME, 'graph_features', huc, gauge_id, 'dynamic', 'GloFAS', 'runoff_water_equivalent_mm.csv'), index = True)\n",
    "\n",
    "with Parallel(n_jobs = 8, verbose = 0) as parallel:\n",
    "    _ = parallel(delayed(process)(idx, row) for idx, row in tqdm.tqdm(camels_graph.iterrows(), total=len(camels_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
